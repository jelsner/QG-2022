# Lesson 21

## Today {-} 

- Predictive uncertainty
- Random forests
- Cross validation
- Classification trees

## Regression tree example {-}

Example: Let's return to the Palmer penguins data frame. You are interested in using machine learning (regression tree) to predict the body mass of male Gentoo penguins using the penguin's bill and flipper dimensions. 

First you filter the data frame keeping only the male Gentoo penguins.
```{r}
Gentoo.df <- palmerpenguins::penguins |>
  dplyr::filter(species == "Gentoo", 
                sex == "male")
```

To train (fit) the model you use the `tree()` function from the {tree} package. The response variable is `body_mass_g` and the explanatory variables are `bill_length_mm`, `bill_depth_mm`, and `flipper_length_mm`.

The model formula, which is the first argument in the function, has the same format as the model formula for fitting a linear regression model.
```{r}
library(tree)

model <- tree(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm,
              data = Gentoo.df)
model
```

A _tree diagram_ helps make sense of this output. Here we use the generic `plot()` method and add text with the `text()` function.
```{r}
plot(model)
text(model)
```

The most important variable in explaining body mass of the male Gentoo penguins is bill length. It is the variable named on the top of the tree diagram. For penguins with a bill length less than 47.45 mm, the next most important variable is bill depth (recall if the condition is true, then go left). For those penguins if bill depth is less than 15.7 mm then the average body mass is 5131 grams and if the bill depth is greater or equal to 15.7 mm then the average body mass is larger at 5410 g. Again, if the condition is tree, then go left.

For male Gentoo penguins with a bill length at least 47.45 mm, the next most important variable is flipper length. For these larger-bill length penguins if flipper length is at least 228.5 mm then the average body mass is 5755 g. For these larger-bill length penguins if flipper length is shorter than 228.5 mm, then split on flipper length again.

As we move down the tree the branches get shorter (length of the vertical lines get shorter) indicating the splits are _less_ important in reducing the residual sum of squares (RSS). As such we might want to stop splitting the variables at some point.

The interpretation of a tree regression is easier to understand compared to the interpretation of a linear regression because the splits have the same units as the explanatory variables. In a linear regression the coefficients have units of response variable divided by the units of the explanatory variable.

Machine learning algorithms are often used solely for predictions. Given the branching structure of the tree, the predicted values are easy to anticipate.

Prediction amounts to following the rules defined by the final model. We start with the value of the variable `bill_length_mm` and work our way down until we hit a terminal node (a branch that ends in a value rather than another split). The value is the _average of the response variable_ conditional on the splits.

For example, if `bill_length_mm` is less than 47.45 mm and `bill_depth_mm` is less than 15.7 mm, then the mass of the penguin is predicted to be 5131 grams. The value of 5131 is the average body mass for all male Gentoo penguins with bill length less than 47.45 and bill depth less than 15.7.

We can check this as follows.
```{r}
Gentoo.df %>%
  filter(bill_length_mm < 47.45 & bill_depth_mm < 15.7) %>%
  summarize(mean(body_mass_g))
```

We again use the `predict()` method to make predictions. The first argument is the model object and the next is the data frame containing the variables.
```{r}
predict(model, 
        newdata = data.frame(bill_length_mm = 45,
                             bill_depth_mm = 15,
                             flipper_length_mm = 220))

predict(model, 
        newdata = data.frame(bill_length_mm = 45,
                             bill_depth_mm = 16,
                             flipper_length_mm = 220))
```

_Note_: We need to specify values for all the explanatory variables even if the variables are not involved with any of the splits.

The standard error on the predicted value is more difficult to work out. We can use the standard error on the subset mean for that terminal node, but this error ignores the uncertainty surrounding what variable to split and at what value to make the split.

Because of this machine learning algorithms are not typically used for inference because is no straightforward way to get an uncertainty estimate about the predicted value for penguin body mass.

Can the model be simplified? Simplifying regression trees involves a trade-off between minimizing the bias and maximizing the variance explained. If the tree has many explanatory variables with many decisions then it will explain more variance, but it is biased toward the particular data set. On the other hand, if the tree is too simple it will not explain much of the variance. If the regression tree is too complex, predictions on new data will be poor.

Pruning the tree solves for this trade-off. The `prune.tree()` function gives a sequence of smaller trees from the largest tree by removing the least important splits.

The function returns the tree size (number of terminal nodes), the total deviance for each tree, and the increase in deviance by going from a larger tree to a smaller tree.
```{r}
prune.tree(model)
```

Here we see the largest tree (8 terminal nodes) results in the smallest deviance (`$dev`) (3452632). By pruning one branch the deviance increases (3536660) but only by a relatively small amount (84028 which is an increase of about 2.4%). Similarly by pruning a second branch the deviance increases, this time a bit more. 

It is helpful to plot the deviance as a function of tree size.
```{r}
plot(prune.tree(model))
```

Here the plot is opposite of the table. Deviance declines with increasing complexity. We see that the amount of decline (vertical line between the steps) generally diminishes with each node. We can see that at a tree size of 6 branches the vertical lines are quite small.

_Code challenge:_ Recreate this plot using the grammar of graphs. Hint: save the prune tree object and then create a data frame from the object lists. Then use `scale_x_reverse()`.

Suppose we want a tree with six terminal nodes.
```{r}
model2 <- prune.tree(model, 
                     best = 6)
plot(model2)
text(model2)
```

## Predictive uncertainty {-}

Let's look at another example. Predicting Atlantic hurricane counts using a regression tree.

Import the annual hurricane data and subset on years since 1950. We create a data frame containing only the basin-wide hurricane counts and Southern Oscillation Index (El Nino) and SST as the two explanatory variables. The SOI has units of standard deviation and the SST has units of degrees C.
```{r}
df <- read.table("http://myweb.fsu.edu/jelsner/temp/data/AnnualData.txt", 
                 header = TRUE) %>%
  filter(Year >= 1950) %>%
  select(Year = Year, H = B.1, SOI = soi, SST = sst)
head(df)
```

Then fit a regression tree to these data with the annual count of hurricanes as the response variable and `SOI` and `SST` as the explanatory variables.
```{r}
model <- tree(H ~ SOI + SST, 
              data = df)
plot(model)
text(model)
```

The predicted number of hurricanes when `SOI` is -2 s.d. and when `SST` is .2C is found using the `predict()` method. 
```{r}
predict(model, 
        newdata = data.frame(SOI = -2, SST = .2))
```

So, on average, we can expect between 7 and 8 hurricanes in the Atlantic in years with the SOI is -2 s.d. and the SST is .2C above normal.

The predicted value depends on the tree, which in turn depends exactly on what values of SOI and SST are used to "grow" it. 

To see this, we grow a new tree for hurricane count, but this time leaving the values of SOI and SST from the last year in the data frame out. 

Recall from earlier in the semester that since there are 61 years, leaving the last year out amounts to sub-setting the data frame as `df[-61, ]`.

Then we fit a new tree model (`model2`) and make a prediction for the same values of `SOI` and `SST` (`SOI` = -2 s.d. and `SST` = .2C).
```{r}
model2 <- tree(H ~ SOI + SST, 
               data = df[-61, ])
predict(model2, 
        newdata = data.frame(SOI = -2, SST = .2))
```

The predicted value is quite a bit different even though we left out only one year of data. 

Predictive uncertainty occurs with any model, but the level of uncertainty is much larger with machine learning algorithms because they have many parameters. Each branch in a regression tree is a parameter and so is every split so with our two explanatory variables there are at least 10 parameters.

For example, compare the predicted hurricane rates from a linear regression model with and without the last year.
```{r}
predict(lm(H ~ SOI + SST, data = df),
        newdata = data.frame(SOI = -2, SST = .2))
predict(lm(H ~ SOI + SST, data = df[-61, ]),
        newdata = data.frame(SOI = -2, SST = .2))
```

The difference in predicted values is very small with a linear regression model. On the order of .2% `r (6.23 - 6.22)/6.23 * 100` compared with 22% (`(7.35 - 5.71)/7.35 * 100`) for the machine learning algorithm

## Random forest algorithm {-}

A random forest algorithm solves the problem of large predictive uncertainty by fitting many regression trees (a 'forest'). The algorithm takes many samples (e.g., 100) from the set of all observations and then using these samples fits many regression trees. Given a set of new values for the explanatory variables, each regression tree provides a predicted value. The average over all predicted values is taken as the random forest prediction.

The `randomForest()` function in the {randomForest} package provides a random forest algorithm. 

Here we apply a random forest algorithm to the hurricane data frame. We use the word _algorithm_ because there is no single model. The number of trees is set at 500 (this is the default number). This should not be set too small to ensure all observations get predicted at least a few times.
```{r}
library(randomForest)

rfa <- randomForest(H ~ SOI + SST,
                    data = df,
                    ntree = 500)
```

Then to make a prediction we again use the `predict()` method. 
```{r}
predict(rfa, 
        newdata = data.frame(SOI = -2, SST = .2))
```

The predicted value is less sensitive to removing single observations. To see this we apply the random forest algorithm with the last observation removed.
```{r}
rfa <- randomForest(H ~ SOI + SST, 
                    data = df[-61, ],
                    ntree = 500)
predict(rfa, 
        newdata = data.frame(SOI = -2, SST = .2))
```

In summary: Regression trees tend to produce predicted values that are unstable, which means a small change in the data used to grow the tree results in a large difference in the predicted value. 

The _in-sample error_ (the error made when using the model to predict the data that were used to fit the model) might be small, but the _out-of-sample error_ (the error made when using the model to predict new data) can be large. 

A random forest algorithm solves the problem of unstable predictions by growing many trees.

## Cross validation {-}

Why use a machine learning algorithm? They are useful when we have a large amount of data. They can find features in the data that are not accessible with linear regression.

But how do we compare the predictive skill of a random forest algorithm against the predictive skill of a linear regression model?

We use a method called _cross validation_. Cross validation helps us decide what model to use across the universe of all possible models.

Cross validation is a method that removes noise specific to each observation and estimates how well the model/algorithm finds useful prediction rules when this coincident information is unavailable.

Key: When comparing different types of models/algorithms we can't use AIC to help us decide.

For example, to compare a random forest algorithm with a _Poisson_ regression model for predicting hurricane counts we arrange a cross validation as follows:
```{r}
n <- length(df$H)
rfx <- numeric(n)
prx <- numeric(n)
for(i in 1:n){
  rfa <- randomForest(H ~ SOI + SST, data = df[-i, ])
  prm <- glm(H ~ SOI + SST, data = df[-i, ], 
             family = "poisson")
  new <- df[i, ]
  rfx[i] <- predict(rfa, newdata = new)
  prx[i] <- predict(prm, newdata = new, 
                    type = "response")
}
```

The out-of-sample averaged prediction error squared (mean squared prediction error) is computed by typing
```{r}
mean((df$H - prx)^2)
mean((df$H - rfx)^2)
```

Results show that the Poisson regression model performs slightly better than the random forest model in this case although the difference is not large.  

The correlation between the actual and predicted value is
```{r}
cor(df$H, prx)
cor(df$H, rfx)
```

We can see the influence of the two explanatory variables on hurricanes using the following code chunk.
```{r}
newdat <- expand.grid(SST = seq(-.5, .7, .01), 
                      SOI = seq(-5, 4, .1))
z1 <- predict(rfa, newdata = newdat)
prm <- glm(H ~ SOI + SST, data = df, family = "poisson")
z2 <- predict(prm, newdata = newdat, type = "response")
newdat$Hrf <- z1
newdat$Hpr <- z2

library(ggplot2)

p1 <- ggplot(newdat, aes(x = SST, y = SOI, fill = Hrf)) + 
        geom_tile() +
        scale_fill_viridis_c(limits = c(0, 12)) +
        labs(fill = "Rate") +
        xlab("Atlantic Sea Surface Temperature (°C)") + 
        ylab("Southern Oscillation Index (s.d.)") +
        ggtitle(subtitle = "Random forest algorithm",
                label = "Annual Atlantic hurricane rate") +
        theme_minimal()
p2 <- ggplot(newdat, aes(x = SST, y = SOI, fill = Hpr)) + 
        geom_tile() +
        scale_fill_viridis_c(limits = c(0, 12)) +
        labs(fill = "Rate") +
        xlab("Atlantic Sea Surface Temperature (°C)") + ylab("Southern Oscillation Index (s.d.)") +
        ggtitle(subtitle = "Poisson regression model",
                label = "Annual Atlantic hurricane rate") +
        theme_minimal()

library(patchwork)
p1 + p2
```

The figure shows the influence of SST and SOI on hurricane counts using the random forest algorithm and Poisson regression model. Hurricane counts increase with SST and SOI but for high values of SOI the influence of SST is stronger.  

Similarly for high values of SST the influence of the SOI is more pronounced. The random forest algorithm is able to capture smaller features at various thresholds, but at the expense of interpreting some noise as signal as seen by the relative high count with SOI values near -3 s.d. and SST values near -0.1C.

## Classification trees {-}

When the response variable is categorical (e.g., species) then a regression tree is called a _classification tree_.  A classification tree is a set of if-then rules that results in a probability for each category. 

For instance, if a patient's resting heart rate exceeds 120 bpm, then predict a high chance that the patient will suffer a heart attack within the next 5 years.

If instability is high and wind shear is strong, then predict a high chance of damaging tornadoes.

Consider once again the `penguins` data frame from the {palmerpenguins} package.
```{r}
head(penguins)
```

With `species` as the categorical response variable (there are three different penguin species) and the other columns as explanatory variables we can specify the model formula as `species ~ .`. The period after the tilde (`~`) indicating all other variables in the data frame.
```{r}
model <- tree(species ~ .,
              data = penguins)
plot(model)
text(model)
```

From the plot we see that flipper length split at 206.5 mm is the most important variable in distinguishing the species. The next two branches are bill length split at 43.45 mm and island.

Since island is a categorical explanatory variable there is a `:a` next to the name `island`. This indicates that the spit is on the first island in alphabetical order. The island names are
```{r}
unique(penguins$island)
```

Biscoe Island is the first in alphabetical order so the split says that if the island is Biscoe (yes, means go left) then predict the species is Gentoo and if the island is not Biscoe (either Dream or Torgersen) then predict the species to be Chinstrap.

After these three splits resulting in four branches, the branches become very small (the vertical lines are short) indicating that these additional splits do not reduce the RSS by much at all.

So we prune the tree by setting `best = 4`. That is, we ask the model to give us a tree with the four most important branches.
```{r}
model2 <- prune.tree(model, 
                     best = 4)
plot(model2)
text(model2)
```

We examine the model quantitatively by the misclassification error rate. This is given in the last line of the output from the `summary()` method.
```{r}
summary(model2)
```

We see that the model misclassifies the species in 12 of the 333 penguins resulting in a misclassification error rate of 3.6%.

Note that the full model has a misclassification error rate of only 1.8% (6 out of the 333 penguins). But this requires twice as many nodes so the model has greater bias.

Suppose we are able to observe a penguin on Biscoe Island with a flipper length of 200 mm, a bill length of 44 mm, a bill depth 18 mm, and a body mass of 4000 g. What is the likely species?

We use the `predict()` method with the `newdata =` argument specified as a data frame. We must also include by name all the variables in the model specification. This includes `island` and `sex` as factors along with `year`, `flipper_length_mm`, `bill_length_mm`, `bill_depth_mm`, and `body_mass_g` as integers or numeric values. Note that since the final tree only includes the variables `island`, `flipper_length_mm`, and `bill_length_mm` it does not matter what values we use for the other variables.
```{r}
predict(model2,
        newdata = data.frame(island = factor("Biscoe"),
                             sex = factor("male"),
                             year = 2020,
                             flipper_length_mm = 200,
                             bill_length_mm = 44,
                             bill_depth_mm = 18,
                             body_mass_g = 4000))
```

The result tells us that there is a 92% chance that it is a Chinstrap penguin, a 6% chance that it is an Adelie penguin and a 2% chance that it is a Gentoo penguin.

The `ctree()` function in {party} package can be used to model binary, nominal, ordinal and numeric variables. 

The {rpart} packages contains additional functions for modeling our data with decision trees. See also: http://rstatistics.net/decision-trees-with-r/