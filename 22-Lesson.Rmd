# Tuesday November 29, 2022 {.unnumbered}

- Estimating spatial autocorrelation in model residuals
- Choosing a spatial regression model
- Fitting and interpreting spatial regression models

## Estimating spatial autocorrelation in model residuals {.unnumbered}

Previous focus has been on estimating the amount of spatial autocorrelation in a variable aggregated at some areal-unit level. You saw examples of crime rates and police expenditures aggregated at the county level. This is important for hot-spot analysis or spatial sampling.

However, if you want a model for a response variable as a function of explanatory variables across a set of areal units then focus shifts to estimating the spatial autocorrelation in the model residuals.

The model of first choice is a linear regression, but when the residuals from a linear regression model exhibit significant spatial autocorrelation then you should consider a _spatial_ regression model.

So the workflow starts by fitting a linear regression model where you regress the response variable onto the explanatory variables. You then check for spatial autocorrelation in the residuals.

Consider again the crime data at the tract level in the city of Columbus, Ohio.

```{r}
CC.sf <- sf::st_read(dsn = here::here("data", "columbus"),
                     layer = "columbus")
```

`CRIME` is the response variable and `INC` and `HOVAL` are the explanatory variables. The data are aggregated to census tracts. How well do these two explanatory variables statistically explain the amount of crime at the tract level?

An answer to this question is obtained by regressing crime onto income and housing values. Here you use the `lm()` function and save the results to the object `model.lm`.

Set the formula assigning it to the object `f`, then use the formula as the first argument in the `lm()` function. Summarize the results with the `summary()` method.

```{r}
f <- CRIME ~ INC + HOVAL

model.lm <- lm(f, 
                data = CC.sf)

model.lm |>
  summary()
```

The output shows that the linear regression model statistically explains 55% of the variation in crime.(multiple R-squared). 

Looking at the coefficients (values under the `Estimate` column), you see that _higher_ incomes are associated with _lower_ values of crime (negative coefficient) and _higher_ housing values are associated with _lower_ crime. The marginal effect of income on crime is -1.6 and the marginal effect of housing value on crime is -.27.

The extractor function `residuals()` is used to get the model residuals as a vector object.

```{r}
( res <- model.lm |>
   residuals() )
```

There are 49 residuals, one for each tract. The residuals are the difference between the observed crime rates and the predicted crime rates (observed - predicted). A residual that has a value greater than 0 indicates that the model _under_ predicts the observed crime rate in that tract and a residual that has a value less than 0 indicates that the model _over_ predicts the observed crime rate.

Inferences made about the relationships between the response and explanatory variables are valid under the assumption that the residuals can be adequately described by a normal distribution. You check this with the `sm::sm.density()` function with the first argument the vector of residuals (`res`) and the argument `model = "Normal"`.

```{r}
res |>
  sm::sm.density(model = "Normal")
```

The density curve of the residuals (black line) fits within the blue ribbon that defines a normal distribution, so there is no evidence against normally distributed residuals.

Next create a map of the model residuals. Do the residuals show any pattern of clustering? 

_Key point_: The values in the vector of residuals `res` are arranged in the same order as the rows in the simple feature data frame.

So you create a new column in the simple feature data frame using the `$` syntax calling the new column `res`. Now the residuals are aligned with the corresponding simple feature geometry defining the tract boundary.

```{r}
CC.sf$res <- res

CC.sf |>
  head()
```
Now you make two maps using the functions from the {tmap} package, one showing the crime rates and the other showing the residuals from the linear model of crime.

```{r}
tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = c("CRIME", "res"), title = "") +
  tmap::tm_borders(col = "gray70") +
  tmap::tm_layout(title = c("Crime rate", 
                            "Linear model residuals"))
```

As you saw last time, crime rates tend to cluster. Residuals from a model of crime also show some clustering. Tracts with negative residuals are noted across the southwestern and southern part of the city and a group of tracts with positive residuals toward the center.

Clustering in the residuals appears to be less than with the crime values themselves. That is, after accounting for regional factors related to crime, the autocorrelation is reduced.

To determine the amount of spatial autocorrelation in crime use the `spdep::moran.test()` function after specifying the neighborhood topology and the weights. Here you use the default queen contiguity and row standardized weights.

```{r}
nbs <- CC.sf |>
  spdep::poly2nb()
wts <- nbs |>
  spdep::nb2listw()

CC.sf$CRIME |>
  spdep::moran.test(listw = wts)
```

To determine the amount of autocorrelation in the model residuals use the `spdep::lm.morantest()` function, passing the regression model object and the weights object to it.

```{r}
model.lm |>
  spdep::lm.morantest(listw = wts)
```

Moran's I on the model residuals is .22. This compares with the value of .5 on the value of crime alone. This is what you might have expected given the maps of crime and the residuals.

Part of the autocorrelation in the crime rates is statistically 'absorbed' by the explanatory factors.

The $p$-value on I of .002, thus you reject the null hypothesis of no spatial autocorrelation in the model residuals and conclude that a spatial regression model would be an improvement over the non-spatial model. The $z$-value (as the basis for the $p$-value) takes into account the fact that these are residuals from a model so the variance is adjusted accordingly.

Given significant spatial autocorrelation in the model residuals, the next step is to choose the type of spatial regression model.

## Choosing a spatial regression model {.unnumbered}

Linear regression models fit to spatial data can lead to improper inference because observations are not independent. This might lead to poor policy decisions. If the residuals from a linear model are strongly correlated the model is not specified properly.

You can try to improve the model by adding variables. If that's not possible (no additional data, or no clue as to what variable to include), you can try a spatial regression model. Spatial regression models are widely used in econometrics and epidemiology.

The equation for a regression model in vector notation is

$$
y = X \beta + \varepsilon
$$

where $y$ is a $n$ by 1 vector of response variable values, $X$ is a $n$ by $p+1$ matrix containing the explanatory variables and augmented by a column of ones for the intercept term, $\beta$ is a $p+1$ $\times$ 1 vector of model coefficients and $\varepsilon$ is a $n$ by 1 vector of residuals (iid: independent and identically distributed).

A couple options exist if the elements of the vector $\varepsilon$ are correlated. One is to include a spatial lag term so the model becomes

$$
y = \rho W y + X \beta + \varepsilon
$$

where $Wy$ is the weighted average of the neighborhood response values with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient. This is called a spatial autoregressive (SAR) model.

_Note_: $Wy$ is the spatial lag variable you compute with the `spdep::lag.listw()` function and $\rho$ is Moran's I. Thus the model is also called a spatial lag model (SLM).

Justification for the spatial lag model comes from domain specific but is motivated by a 'diffusion' process. The response variable $y_i$ is influenced by the explanatory variables at location $i$ and by explanatory variables at locations $j$.

$\rho Wy$ is called the _spatial signal_ term and $\beta X$ is called the _trend_ term.

Another option is to include a spatial error term so the model becomes

$$
y = X\beta + \lambda W \epsilon + u
$$

where $\lambda$ is the autoregression coefficient, $W\epsilon$ is the spatial error term representing the weighted average of the neighborhood residuals, and $u$ are the overall residuals assumed to be iid. This is called a spatial error model (SEM).

Here the lag term is computed using the residuals rather the response variable.

Application of the spatial error model is motivated by the omitted (unobserved) variable bias. Examples of unobservable (latent) variables include local culture, social capital, neighborhood readiness. 

Importantly you would expect the latent variable to be spatially correlated (e.g., culture will be similar across neighborhoods). Another motivation for considering a spatial error model is heterogeneity. 

In the absence of domain-specific knowledge of the process that might be responsible for the spatially autocorrelated residuals, you can run statistical tests on the linear model.

The tests are performed with the `spdep::lm.LMtests()` function. The `LM` stands for 'Lagrange multiplier' indicating that the technique simultaneously determines the coefficients on the explanatory variables AND the coefficient on the spatial lag variable.

The test type is specified as a character string. The tests should be considered in a sequence starting with the standard versions and moving to the 'robust' versions if the choice is ambiguous based on the standard versions.

To perform LM tests you specify the model object, the weights matrix, and the two model types using the `test =` argument. The model types are specified as character strings `"LMerr"` and `"LMlag"` for the spatial error and lag models, respectively.

```{r}
model.lm |>
  spdep::lm.LMtests(listw = wts, 
                    test = c("LMerr", "LMlag"))
```

The output shows that both the spatial error and spatial lag models are significant ($p$-value < .15). Ideally one model is significant and the other is not, and you choose the model that is significant.

Since both are significant, you test again. This time you use the robust forms of the statistics with character strings `"RLMerr"` and `"RLMlag"` in the `test =` argument.

```{r}
model.lm |>
  spdep::lm.LMtests(listw = wts, 
                    test = c("RLMerr", "RLMlag"))
```

Here the error model has a large $p$-value and the lag model has a $p$-value that is less than .15 so you choose the lag model for your spatial regression.

A decision tree (from Luc Anselin) shows the sequence of tests for making a choice about which type of spatial model to use [Decision Tree](http://myweb.fsu.edu/jelsner/temp/SpatialRegressionDecisionTree.png)

If both tests show significant models, then you should fit both models and check which one gives the lowest information criteria (AIC). Another options is to include both a spatial lag term and a spatial error term into a single model.

## Fitting and interpreting spatial regression models {-}

Recall that the marginal effect of income on crime is -1.6 and the marginal effect of housing value on crime is -.27.

```{r}
model.lm
```

A nice way to visualize the relative significance of the explanatory variables is to make a plot. Here you use the `broom::tidy()` method and then `ggplot()` as follows.

```{r}
if(!require(broom)) install.packages(pkgs = "broom", repos = "http://cran.us.r-project.org")
library(broom)

( d <- broom::tidy(model.lm, 
                   conf.int = TRUE) )

library(ggplot2)

ggplot(d[-1,], aes(x = estimate,  # do not plot the intercept term
                   y = term, 
                   xmin = conf.low, 
                   xmax = conf.high, 
                   height = 0)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh()
```

The two explanatory variables are plotted on the vertical axis. The marginal effect estimate is shown as a point and the confidence interval around the estimate is shown as a horizontal error bar. The default confidence level is 95% (`conf.level = .95`). The effects are statistically significant as the confidence intervals do not intersect the zero line (dashed-dotted vertical line).

But you've shown above that the model residuals have significant spatial autocorrelation so reporting the marginal effects with this regression model is incorrect.

Instead, you fit a spatially-lagged Y model using the `lagsarlm()` function from the {spatialreg} package. The model is 

$$
y = \rho W y + X \beta + \varepsilon
$$ 

where $Wy$ is the weighted average of the neighborhood response values (spatial lag variable) with $W$ the spatial weights matrix, and $\rho$ is the autoregression coefficient.

The `spatialreg::lagsarlm()` function first determines a value for $\rho$ ( with the internal `optimize()` function) and then the $\beta$'s are obtained using generalized least squares (GLS). The model formula `f` is the same as what you used to fit the linear regression model above. You save the model object as `model.slym`.

```{r}
if(!require(spatialreg)) install.packages(pkgs = "spatialreg", repos = "http://cran.us.r-project.org")

model.slym <- spatialreg::lagsarlm(formula = f, 
                                   data = CC.sf, 
                                   listw = wts)

model.slym |>
  summary()
```

The first batch of output concerns the model residuals and the coefficients on the explanatory variables. Again, the model residuals are the observed crime rates minus the predicted crime rates.

The coefficients on income and housing have the same sign (negative) and they remain statistically significant (-1.05 for income and -.27 for housing value). But you can't interpret these coefficients as marginal effects.

The next set of output is about the coefficient of spatial autocorrelation ($\rho$). The value is .423 and a likelihood ratio test gives a value of 9.41 which translates to a $p$-value of .002. The null hypothesis is the autocorrelation is zero, so you confidently reject it. This is consistent with the significant Moran's I value that you found in the linear model residuals.

Two other tests are performed on the value of $\rho$ including a z-test (t-test) using the asymptotic standard error. Both tests confirm that the lag term should be included in the model for crime.

In spatial models that contain a lagged response term, the coefficients are not marginal effects. The spatial lag model allows for 'spillover'. That is a change in an explanatory variable anywhere in the study domain will affect the value of the response variable *everywhere*. Spillover occurs even when the neighborhood weights matrix represents local contiguity. The spillover makes interpreting the coefficients more complicated.

With a spatially-lagged Y model a change in the value of an explanatory variable results in both *direct* and *indirect* effects on the response variable. For example, the direct effect gives the impact a change in income has on crime averaged over all tracts. It takes into account the effects that occur from a change in the $i$th tract's income on crime across neighboring tracts.

The indirect effect gives the impact of a change in income has on crime averaged over all *other* tracts. The indirect effect represent spillovers. The influences on the dependent variable $y$ in a region rendered by change in $x$ in some *other* region. For example, if all tracts $i \ne j$ (i not equal to j) increase their income, what will be the impact on crime in region $i$?

The total effect (TE) is the sum of the direct and indirect effects. It measures the total cumulative impact on crime arising from one tract $j$ increasing its income over all other tracts (on average). It is given by 

$$
\hbox{TE} = \left(\frac{\beta_k}{1-\rho^2}\right)\left(1 + \rho\right)
$$ 

where $\beta_k$ is the effect of variable $k$ and $\rho$ is the spatial autocorrelation coefficient. With $\rho = 0$ TE is $\beta_k$.

Here $\beta_{INC}$ is -1.0487 and $\rho$ is .4233, so the total effect is

```{r}
( TE_INC <- -1.0487 / (1 - .4233^2) * (1 + .4233) )
```

The direct, indirect, and total effects are shown using the `spatialreg::impacts()` function.

```{r}
model.slym |> 
  spatialreg::impacts(listw = wts)
```

The direct effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in that region.

The indirect effects are the changes in the response variable of a particular region arising from a one unit increase in an explanatory variable in another region. For example, due to spatial autocorrelation, a one-unit change in the income variable in region 1 affects the crime rate in regions 2 and 3.

The next set of output concerns the overall model fit. It includes the log likelihood value and the AIC (Akaike Information Criterion). The AIC value for the linear model is included. Here it is clear that the spatial lag model is an improvement (smaller AIC) over the aspatial model.

The larger the likelihood, the better the model and two times the difference in log likelihoods from two competing models divided by the number of observations gives a scale for how much improvement.

```{r}
x <- 2 * (logLik(model.slym) - logLik(model.lm))/49
x[1]
```

Improvement table

| Likelihood difference | Qualitative improvement |
|-----------------------|-------------------------|
| 1                     | huge                    |
| .1                    | large                   |
| .01                   | good                    |
| .001                  | okay                    |

The final bit of output is a Lagrange multiplier test for remaining autocorrelation. The null hypothesis is there is no remaining autocorrelation since we have a lag term in the model. The result is a high $p$-value so you are satisfied that the lag term takes care of the autocorrelation.

Compare the spatial lag model to a spatial error model. Here you use the `spatialreg::errorsarlm()` function.

```{r}
model.sem <- spatialreg::errorsarlm(formula = f, 
                                    data = CC.sf, 
                                    listw = wts)
summary(model.sem)
```

You find the coefficient of spatial autocorrelation ($\lambda$) is significant, but the log likelihood value from the model is smaller (-183.7) and the AIC value is larger (377.5) compared with corresponding values from the lag model. This is consistent with the Lagrange multiplier (LM) tests indicating the spatial lag model is more appropriate.

Also you can compare the log likelihoods from the two spatial regression models that you fit.

```{r}
x <- 2 * (logLik(model.slym) - logLik(model.sem))/49
x[1]
```

With a value of .04 you conclude that there is good improvement of the lag model over the error model. Again, this is consistent with your decision above to use the lag model.

With the spatial error model the coefficients can be interpreted as marginal effects like with the linear model.

If there are large differences (e.g., different signs) between the coefficient estimate from SEM and the linear model, this suggests that neither model is yielding parameters estimates matching the underlying parameters of the data generating process.

You test whether there is a significant difference in coefficient estimates with the Hausman test under the hypothesis of no difference.

```{r}
model.sem |>
  spatialreg::Hausman.test()
```

The $p$-value gives inconclusive evidence that the coefficients are different and that maybe the SEM is not the right way to proceed with these data.

The `predict()` method implements the `predict.sarlm()` function to calculate predictions from the spatial regression model. The prediction on a spatial lag Y model is decomposed into a "trend" term (explanatory variable effect) and a "signal" term (spatial smoother). The predicted fit is the sum of the trend and the signal terms when using the spatial lag model.

You make predictions with the `predict()` method under the assumption that the mean response is known. You examine the structure of the corresponding predict object.

```{r}
( predictedValues <- predict(model.slym) )
```

The predicted values are in the column labeled `fit`. The predicted values are a sum of the trend term ($X\beta$) and the signal term ($\rho W y$). The signal term is called the spatial smoother.

As a first-order check to see if things are what you think they should be, compare the first five predicted values with the corresponding observed values.

```{r}
predictedValues[1:5]
CC.sf$CRIME[1:5]
```

Some predicted values are lower than the corresponding observed values and some are higher.

The predicted values along with the values for the trend and signal are appended to the simple features data frame using the `$` notation.

```{r}
CC.sf$fit <- as.numeric(predictedValues)
CC.sf$trend <- attr(predictedValues, "trend")
CC.sf$signal <- attr(predictedValues, "signal")
```

You plot the observed versus the predicted as a scatter plot with a y = x line and a best-fit regression line.

```{r}
ggplot(data = CC.sf,
       mapping = aes(x = CRIME, y = fit)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE, color = "red") +
  geom_abline() +
  scale_x_continuous(limits = c(0, 70)) +
  scale_y_continuous(limits = c(0, 70)) +
  xlab("Observed Crime") +
  ylab("Predicted Crime")
```

The components of the predictions are mapped and placed on the same plot.

```{r}
( g1 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = fit)) +
    scale_fill_viridis_c() +
    ggtitle("Predicted Crime") )

( g2 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = trend)) +
    scale_fill_viridis_c() +
    ggtitle("Trend (Explanatory Variables)") )

( g3 <- ggplot() +
    geom_sf(data = CC.sf, aes(fill = signal)) +
    scale_fill_viridis_c() +
    ggtitle("Signal") )

library(patchwork)
g1 + g2 + g3
```

The trend term and the spatial smoother have similar ranges indicating nearly equal contributions to the predictions. The largest difference between the two terms occurs in the city's east side.

A map of the difference makes this clear.

```{r}
CC.sf <- CC.sf |>
  dplyr::mutate(CovMinusSmooth = trend - signal)

tmap::tm_shape(CC.sf) +
  tmap::tm_fill(col = "CovMinusSmooth")
```

How many tracts have a smaller residual with the lag model versus the linear model?

```{r}
CC.sf |>
  dplyr::mutate(residualsL = CRIME - fit,
                lagWins = abs(residuals(model.lm)) > abs(residualsL),
                CovMinusSmooth = trend - signal) |>
  sf::st_drop_geometry() |>
  dplyr::summarize(N = sum(lagWins))
```

In 32 out of the 49 tracts the residuals from the spatial model are smaller than the residuals from the linear model.

Another spatial regression option is to modify the linear model to include spatially-lagged explanatory variables. This is called the spatially-lagged X model. 

$$
y = X \beta + XW \theta + \varepsilon
$$

In this case the weights matrix is (post) multiplied by the matrix of X variables where $W$ is again the weights matrix and $\theta$ is a vector of coefficients for each lagged explanatory variable.

Here you fit the spatially-lagged X model using the `spatialreg::lmSLX()` function and save the model object as `model.slxm`.

```{r}
( model.slxm <- spatialreg::lmSLX(formula = f, 
                                  data = CC.sf, 
                                  listw = wts) )
```

With this model, beside the direct marginal effects of income and housing value on crime, you also have the spatially-lagged indirect effects.

The total effect of income on crime is the sum of the direct effect and indirect effect. And again, using the `spatialreg::impacts()` function you see this.

```{r}
model.slxm |>
  spatialreg::impacts(listw = wts)
```

You get the impact measures and their standard errors, z-values and $p$-values with the `summary()` method applied to the output of the `impacts()` function.

```{r}
summary(spatialreg::impacts(model.slxm, listw = wts))
```

Results show that income has a significant direct *and* indirect effect on crime rates, but housing values only show a significant direct effect and not a significant indirect effect.

Again you visualize the relative significance of the effects.

```{r}
model.slxm |>
  broom::tidy(conf.int = TRUE) |>
  dplyr::slice(-1) |>
ggplot(aes(x = estimate,
                   y = term, 
                   xmin = conf.low, 
                   xmax = conf.high, 
                   height = 0)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, lty = 4) +
  geom_errorbarh()
```

Compare R squared values between the linear model and the spatially-lagged X model.

```{r}
summary(model.lm)$r.squared
summary(model.slxm)$r.squared
```

The spatially lagged model has an R squared value that is higher than the R squared value from the linear regression.

Another way to find the correct spatial model is to consider both the spatial Durbin error model and the spatial Durbin model. The spatial Durban error model (SDEM) is a spatial error model with a spatially-lagged X term added. 

More information:

-   <https://youtu.be/b3HtV2Mhmvk> Video explaining the types of spatial regression models and how to implement them in R

-   <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2420725> What Regional Scientists Need to Know About Spatial Econometrics