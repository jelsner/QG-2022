[["tuesday-october-19-2022.html", "Tuesday, October 19, 2022 Today Example: Average income vs percent college graduates at the state level Outliers Summarizing Example: A regression model for trend Simpson’s paradox", " Tuesday, October 19, 2022 Today More examples Outliers Simpson’s paradox Linear regression provides an adequate model for our data under the following four assumptions. Linearity: Average values of Y in ordered intervals of X should be a straight-line function of X. Each interval creates a ‘sub-population’ of Y values. Constant variance: Sub-populations of Y should all have about the same standard deviation. Normality: Values from each sub-population can be described with a normal distribution. Independence: Each observation is independent from the other observations. Key idea: A model can be statistically significant, but not adequate. Example: Average income vs percent college graduates at the state level The file Inc.txt on my website contains average annual household income vs percentage of college graduates by state. Fit a linear regression model to these data and check the model assumption. Does the linear model appear to be adequate? Inc.df &lt;- read.table(&quot;http://myweb.fsu.edu/jelsner/temp/data/Inc.txt&quot;, header = TRUE) head(Inc.df) ## State College Income ## 1 AL 20.4 20487 ## 2 AK 28.1 26171 ## 3 AZ 24.6 21947 ## 4 AR 18.4 19479 ## 5 CA 27.5 26808 ## 6 CO 34.6 27573 What is the type and strength of the relationship between percent college graduates and income? These are answered by a scatter plot and correlation, respectively. cor(Inc.df$College, Inc.df$Income) ## [1] 0.7773291 library(ggplot2) ggplot(data = Inc.df, mapping = aes(x = College, y = Income)) + geom_point() + geom_smooth(method = lm) + xlab(&quot;College Graduates (%)&quot;) + ylab(&quot;Average Income ($)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The plot shows that there is a linear relationship between percent college graduates and income. The relationship is positive indicating that states with higher percentage of college graduates also have higher incomes on average. The relationship is quite strong (the correlation value is .78), with few, if any, of the states deviating from the linear pattern. When the data have a spatial component it’s a good idea to make a map. Here you create a choropleth map using functions from the {tmap} package. library(tmap) library(USAboundaries) library(sf) ## Linking to GEOS 3.9.1, GDAL 3.2.3, PROJ 7.2.1; sf_use_s2() is TRUE library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union states.sf &lt;- us_states() Inc.df &lt;- Inc.df |&gt; rename(stusps = State) states.sf &lt;- left_join(states.sf, Inc.df, by = &quot;stusps&quot;) |&gt; filter(!stusps %in% c(&quot;AK&quot;, &quot;HI&quot;, &quot;PR&quot;)) tm_shape(states.sf) + tm_polygons(col = c(&quot;Income&quot;, &quot;College&quot;)) + tm_legend(legend.position = c(&quot;left&quot;, &quot;bottom&quot;)) Regress annual household income on percent of college graduates. model1 &lt;- lm(Income ~ College, data = Inc.df) summary(model1) ## ## Call: ## lm(formula = Income ~ College, data = Inc.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4136.0 -1344.6 74.9 1275.6 5121.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10459.85 1614.70 6.478 4.27e-08 *** ## College 545.27 63.04 8.649 1.98e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2078 on 49 degrees of freedom ## Multiple R-squared: 0.6042, Adjusted R-squared: 0.5962 ## F-statistic: 74.81 on 1 and 49 DF, p-value: 1.975e-11 Results indicate a significant relationship (\\(p\\)-value &lt; .001). Percent college graduates explains 60% of the variation in average income by state. The significant effect implies it is unlikely that income and education have no relationship. Not only is the effect significant but it is large. For every 1 percentage point increase in graduates, average annual household income increases by $545. From the small standard error (relative to the estimate) on the slope extracted as summary(model1)$coefficients[2, 2] ## [1] 63.04127 You can say that the effect is quite precise. This precision shows in the narrow uncertainty interval for the slope estimate. confint(model1) ## 2.5 % 97.5 % ## (Intercept) 7214.9929 13704.7171 ## College 418.5846 671.9569 Is the model adequate? Check linearity and equal spread. ggplot(data = Inc.df, mapping = aes(x = cut(College, breaks = 6), y = Income)) + geom_boxplot() + xlab(&quot;Percentage of College Graduates&quot;) + ylab(&quot;Average Income ($)&quot;) It looks favorable for these two assumptions. Although the effect seems to level off for the highest percentage of graduates. Next look at the distribution of the residuals. Does the distribution look like a normal distribution? res &lt;- residuals(model1) sm::sm.density(res, model = &quot;Normal&quot;) The black curve falls within the blue envelope of a normal distribution, so you have no reason to suspect the assumption of normally distributed residuals. Another check is to use the quantile-quantile plot. A quantile-quantile plot (or Q-Q plot) is a graph of the quantiles of one distribution against the quantiles of another distribution. If the distributions have similar shapes, the points on the plot fall roughly along the straight line. To check the normality assumption of a regression model you want to compare the quantiles of the residuals against the quantiles of a normal distribution. You do that with the qqnorm() function. qqnorm(res) qqline(res) Departures from normality are seen as a systematic departure of the points from a straight line on the Q-Q plot. Interpreting Q-Q plots is somewhat subjective. Here are the common situations. Description: interpretation All but a few points fall on a line: few outliers in the data Left end of pattern is below the line; right end of pattern is above the line: long tails at both ends of the distribution Left end of pattern is above the line; right end of pattern is below the line: short tails at both ends of the data distribution Curved pattern with slope increasing from left to right: data is skewed to the right Curved pattern with slope decreasing from left to right: data is skewed to the left Staircase pattern (plateaus and gaps): data have been rounded or are discrete Outliers What state is most/least favorable with respect to income after graduating from college? These are the states where incomes for a given % graduates fall farthest from the regression line. The residuals are what is left over once percent graduated is in the model—or in statistical language, “after controlling for percent graduated.” The above assumptions concern the conditional distribution of the residuals. The residuals for each of the observations are computed when you use the lm() function to fit the model. In the code below, the fortify() function from the {ggplot2} package creates a data frame with elements computed from the fitted model. model1.df &lt;- fortify(model1) head(model1.df) ## Income College .hat .sigma .cooksd .fitted .resid ## 1 20487 20.4 0.04076339 2093.191 0.0061668998 21583.38 -1096.3784 ## 2 26171 28.1 0.02738038 2098.646 0.0005072964 25781.96 389.0367 ## 3 21947 24.6 0.01993274 2080.543 0.0089192327 23873.52 -1926.5156 ## 4 19479 18.4 0.06209651 2093.973 0.0084026089 20492.84 -1013.8369 ## 5 26808 27.5 0.02450204 2090.083 0.0054600928 25454.80 1353.1992 ## 6 27573 34.6 0.10104189 2082.383 0.0445065637 29326.22 -1753.2232 ## .stdresid ## 1 -0.5387362 ## 2 0.1898443 ## 3 -0.9365331 ## 4 -0.5038109 ## 5 0.6593668 ## 6 -0.8899099 model1.df$State &lt;- Inc.df$stusps The variables in the model are given in the first two columns with information about the regression that is specific to each of the cases (observations) given in the next six columns. The first three of those columns contain information that allows you to assess how influential that observation is to the model. If that particularly observation is removed and the regression model refit without it, how much difference would it make in terms of the coefficients? If removing an observation changes the coefficients a lot then that observation is said to be influential. A plot of the model residuals as a function of the explanatory variables lets us easily identify which states have the largest positive and negative residuals. To make the plot more readable use the geom_label_repel() function from the {ggrepel} package. This labels the points while avoiding overlapping labels. library(ggrepel) ggplot(data = model1.df, mapping = aes(x = College, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + xlab(&quot;Percent College Graduates&quot;) + ylab( &quot;Model Residuals&quot;) + geom_label_repel(aes(label = State), color = &quot;darkblue&quot;) Nevada and Connecticut stand out as states where the model underestimates income from percent graduation rates. While Utah, New Mexico, and Montana are states where the model over estimates income. Statistically significant outliers are those that are outside +/- 2 standard deviations from the regression line. The standardized residuals are plotted and the corresponding significance lines drawn. ggplot(data = model1.df, mapping = aes(x = College, y = .stdresid)) + geom_point() + geom_hline(yintercept = 0) + geom_hline(yintercept = -2, lty = &quot;dotted&quot;) + geom_hline(yintercept = 2, lty = &quot;dotted&quot;) + xlab(&quot;Percent College Graduates&quot;) + ylab(&quot;Studentized Residuals&quot;) + geom_label_repel(aes(label = State), color = &quot;darkblue&quot;) Summarizing Regression modeling is statistical control. You often want to do more than just summarize the relationship between variables. That is go beyond reporting the correlation. Regression provides a strategy to control for effects of an explanatory variable to see what is left over. These left overs (residuals) are interpreted as “controlled observations” (e.g. percent income controlling for percent graduates). Observations that result in large residuals are called outliers. Outliers can distort regression results or they can be interesting on their own (e.g. unusually destructive tornadoes). Inspect scatter plots and plots of residuals to determine whether there are outliers that have a strong influence on the regression line. If there are you should re-fit the regression model without those observations and compare results. Regardless of how you decide to handle them, you need to let our readers know about these unusual cases. When interpreting a regression model fit to our data, you are making some implicit assumptions. Before accepting a model you need to examine those assumptions to make sure they are tenable. The model fit may be excellent, but you can’t be sure your conclusions are correct unless you can defend the assumptions. Examining the model residuals helps you defend (if warranted) the assumptions. Example: A regression model for trend The rate at which something is changing over time is called a trend. Trend analysis is common in climate change studies and it often involves fitting a linear regression model to quantify the trend where the “explanatory variable” is some index of time. Returning to the Florida precipitation data. Import the data and make a line plot showing March values each year. Add the best-fit line through these values. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot; FLp &lt;- readr::read_table(loc, na = &quot;-9.900&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## Jan = col_double(), ## Feb = col_double(), ## Mar = col_double(), ## Apr = col_double(), ## May = col_double(), ## Jun = col_double(), ## Jul = col_double(), ## Aug = col_double(), ## Sep = col_double(), ## Oct = col_double(), ## Nov = col_double(), ## Dec = col_double() ## ) library(ggplot2) ggplot(FLp, aes(x = Year, y = Mar)) + geom_line() + geom_smooth(method = lm, color = &quot;blue&quot;) + ylab(&quot;Statewide March Precipitation (in)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; There is an upward trend (increasing precipitation) but you can see that the uncertainty ribbon would allow a horizontal line. Thus you do not anticipate a significant trend term in a regression model. To check on this expectation, you regress March precipitation onto year. model &lt;- lm(Mar ~ Year, data = FLp) summary(model) ## ## Call: ## lm(formula = Mar ~ Year, data = FLp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7877 -1.4772 -0.3558 1.0638 4.9677 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.206620 10.260642 -1.872 0.0638 . ## Year 0.011710 0.005253 2.229 0.0277 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.919 on 115 degrees of freedom ## Multiple R-squared: 0.04142, Adjusted R-squared: 0.03309 ## F-statistic: 4.969 on 1 and 115 DF, p-value: 0.02774 You see that the statewide annual average precipitation has been increasing by .01 inches per year. This upward trend has a \\(p\\)-value of .04 providing suggestive but inconclusive evidence against the null hypothesis of no trend. The larger the \\(p\\)-value the more evidence you have that the trend is not significant. You can fit regression models for all months. First convert the wide to a long data frame. FLpL &lt;- FLp |&gt; tidyr::pivot_longer(cols = Jan:Dec, names_to = &quot;Month&quot;, values_to = &quot;Precipitation&quot;) Then to fit trend models separately for each month you use the do() function from the {dplyr} package together with the tidy() function from the {broom} package. FLpL |&gt; dplyr::mutate(MonthF = factor(Month, levels = month.abb)) |&gt; dplyr::group_by(MonthF) |&gt; dplyr::do(broom::tidy(lm(Precipitation ~ Year, data = .))) ## # A tibble: 24 × 6 ## # Groups: MonthF [12] ## MonthF term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan (Intercept) -10.6 8.14 -1.30 0.197 ## 2 Jan Year 0.00691 0.00417 1.66 0.100 ## 3 Feb (Intercept) 4.46 8.22 0.543 0.588 ## 4 Feb Year -0.000664 0.00421 -0.158 0.875 ## 5 Mar (Intercept) -19.2 10.3 -1.87 0.0638 ## 6 Mar Year 0.0117 0.00525 2.23 0.0277 ## 7 Apr (Intercept) 2.61 8.34 0.313 0.755 ## 8 Apr Year 0.000162 0.00427 0.0380 0.970 ## 9 May (Intercept) 8.76 9.72 0.901 0.369 ## 10 May Year -0.00252 0.00498 -0.506 0.614 ## # … with 14 more rows The table shows the intercept and slope coefficients for each month. Simpson’s paradox Simpson’s paradox occurs when a trend appears in different groups of data but disappears or reverses when the groups are combined. It is encountered in social-science and medical-science statistics and is problematic when data are aggregated at various scales (spatial or otherwise). The paradox can be resolved when causal relations are appropriately addressed in the statistical modeling. As an example, consider again the relationship between bill length and bill depth in the Palmer penguins data without regards to species. library(palmerpenguins) ( p &lt;- ggplot(data = penguins, mapping = aes(y = bill_depth_mm, x = bill_length_mm)) + geom_point() + geom_smooth(method = lm) ) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). lm(bill_depth_mm ~ bill_length_mm, data = penguins) ## ## Call: ## lm(formula = bill_depth_mm ~ bill_length_mm, data = penguins) ## ## Coefficients: ## (Intercept) bill_length_mm ## 20.88547 -0.08502 The plot shows an inverse relationship between bill depth and bill length. And the regression model shows a statistically significant relationship with bill depth decreasing by .85 cm for every one mm increase in bill length. However the relationship is across all species of penguins. If you group by species you see that for each species the relationship is the opposite. p + geom_point(mapping = aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_smooth(method = lm, mapping = aes(color = species)) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 2 rows containing non-finite values (stat_smooth). ## Warning: Removed 2 rows containing missing values (geom_point). ## Removed 2 rows containing missing values (geom_point). That is bill depth increases with increasing bill length. So you conclude that the negative relationship is an artifact of grouping the penguins with different body masses together. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
