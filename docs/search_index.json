[["syllabus.html", "GEO5156: Quantitative Geography Fall 2022 Syllabus GEO5165C: Quantitative Geography Contact information Course description Grades Ethics Schedule (subject to change with notice) Deeper dives", " GEO5156: Quantitative Geography Fall 2022 James B. Elsner Date compiled: 2022-08-12 Syllabus GEO5165C: Quantitative Geography Contact information Instructor Name: Professor James B. Elsner Instructor Location: Bellamy Building, Room 323a Lesson Hours: TR 3:05-4:20 p.m. Student Hours: TR 9:15-10:30 a.m., 2-3 p.m. Email: jelsner@fsu.edu Links to my stuff Website GitHub Twitter Course description This course is an introduction to the quantitative analysis of geographic data (data analysis for geographers). The course content is available on GitHub https://github.com/jelsner/QG-2022 Expected learning outcomes You will describe and demonstrate the principles of data science. You will do this with a grammar for manipulating data and a grammar for making graphs. The grammars are implemented in R using the syntax of tidyverse. Materials You will need access to the internet and either a laptop or desktop computer. All course materials are available through GitHub. There is no required textbook. Much of the material for the course comes from the online book: R for Data Science https://r4ds.had.co.nz/ Additional help is available online (e.g., https://tinystats.github.io/teacups-giraffes-and-statistics/index.html) Class meetings During each lesson I will work through and explain the R code within an xx-Lesson.Rmd file in the physical classroom. The notes in the lesson files are comprehensive, so you can work through them on your own if you can’t make it to class. The notes are written using the markdown language. Markdown is a way to write content for the Web. An R markdown file has the suffix .Rmd (R markdown file). The file is opened using the RStudio application. Grades You are responsible for: Reading and running the code in the lesson R markdown files (.Rmd) files. You can do this during the remote lessons as I talk and run my code or outside of class on your own. Completing and returning the lab assignments on time. Grades are determined only by how well you do on the assignments. There are no quizzes, tests, or exams. Grades are determined only by how well you do on the assignments. based on the following standard: A: Outstanding: few, in any, errors/omissions B: Good: only minor errors/omissions C: Satisfactory: minor omissions, at least one major error/omission D: Poor: several major errors/omissions F: Fail: many major errors/omissions I’ll use the +/- grading system. Grades will be posted as they are recorded on FSU Canvas Ethics Academic honor code https://fda.fsu.edu/academic-resources/academic-integrity-and-grievances/academic-honor-policy Americans With Disabilities Act Students with disabilities needing academic accommodation should: (1) register with and provide documentation to the Student Disability Resource Center; (2) bring a letter indicating the need for accommodation and what type. This should be done during the first week of classes. Diversity and inclusiveness It is my intent to present notes and data that are respectful of diversity: gender identity, sexuality, disability, age, socioeconomic status, ethnicity, race, nationality, religion, and culture. Schedule (subject to change with notice) Week Dates Topic 1 August 23, 25 RStudio Cloud and R 2 August 30, September 1 Working with R 3 September 6, 8 Data and data frames 4 September 13, 15 Data analysis 5 September 20, 22 Graphical analysis 6 September 27, 29 Mapping data 7 October 4, 6 Bayesian data analysis 8 October 11, 13 Regression 9 October 18, 20 Multiple regression 10 October 25, 27 Regression trees 11 November 1, 3 Spatial data 12 November 8, 10 Spatial autocorrelation 13 November 15, 17 Geographic regression 14 November 29, December 1 Finish assignments 28 dates, 26 lessons, 11 Assignments Assignment Due Date (5 pm) 1 September 1 2 September 8 3 September 15 4 September 29 5 October 6 6 October 20 7 October 27 8 November 3 9 November 10 10 November 17 11 December 1 Deeper dives Best practices for working with R Develop a project-oriented work flow and don’t use setwd(). Don’t hard-coded file path names. Use version control (github). Manage package dependencies. Pick a style of writing code and stick with it. Do as much as you can in RMarkdown (notes, lectures, slides, etc). Check out Openscapes. Towards a more open and reproducible approach to geographic and spatial data science Opening practice: supporting reproducibility and critical spatial data science Analyzing US Census Data: Methods, Maps, and Models in R "],["tuesday-august-23-2022.html", "Tuesday, August 23, 2022 What is this course? Where are the materials for this course? Syllabus Getting setup RStudio’s integrated development environment (IDE) Example: Tallahassee daily temperatures", " Tuesday, August 23, 2022 Is it getting hotter here in Tallahassee? Are Atlantic hurricanes getting stronger? Data science is an exciting discipline that turns data into understanding, insight, and knowledge. This course teaches you data science in the context of geography. Today What this course is about, how it is structured, and what I expect from you Getting started with R and RStudio What is this course? This is a first course in data science for geographers. Q - What statistics background does this course assume? A - None. Q - Is this an intro statistics course? A - Statistics and data science are closely related with much overlap. Hence, this course is a great way to get started with statistics. But this course is not your typical high school/college statistics course. Q - Will you be doing computing? A - Yes. Q - Is this an introduction to computer science course? A - No, but many themes are shared. Q - What computing language will you learn? A - R. Q - Why not language some other language? A - We can discuss that over coffee. Where are the materials for this course? Github Examples Some of my recent research: More hots Stronger tornadoes Other research: A year as told by fitbit by Nick Strayer R-Ladies global tour by Maelle Salmon Syllabus Navigate to https://github.com/jelsner/QG-2022 Click on the file labeled 00-Syllabus.Rmd Click on the button [Raw] You can Save as to download a copy to your computer Getting setup First get R Go to http://www.r-project.org. Select the CRAN (Comprehensive R Archive Network). Scroll to a mirror site. Choose the appropriate file for your hardware. Follow the instructions to install R. Then get RStudio Go to on http://rstudio.org Download RStudio Desktop Install and open RStudio Then get the course materials from GitHub Navigate to https://github.com/jelsner/QG-2022 Click the Green Code button Then Download ZIP Unzip the file on your computer to create the folder QG-2022-main Then open the first two Rmd files Open the folder to see all the lesson files (and more) Click on the files 00-Syllabus.Rmd and 01-Lesson.Rmd This should bring up the files in RStudio. Finally, learn git (optional) https://happygitwithr.com/install-git.html RStudio’s integrated development environment (IDE) Written in HTML Top menus File &gt; New File &gt; R Markdown Tools &gt; Global Options &gt; Appearance Upper left panel is the markdown file. This is where we put our text and code. Run code chunks from this panel Output from the operations can be placed in this panel or in the Console (see the gear icon above) All the text, code, and output can be rendered to an HTML file or a PDF or Word document (see the Knit button above) Upper right panel shows what is in your current environment and the history of the commands you issued. This is also where you can connect to github Lower left panel is the Console I think of this as a sandbox where you try out small bits of code. If it works and is relevant move it to the markdown file. This is also where output from running code will be placed. Not a place for plain text Lower right panel shows your project files, the plots that get made, and all the packages associated with the project. The File tab shows the files in the project. The most important one is the .Rmd. The Plot tab currently shows a blank sheet The Packages tab shows all the packages that have been downloaded from CRAN and are associated with this project. Example: Tallahassee daily temperatures Packages &gt; Install In the Packages window, type tidyverse, lubridate, here, ggplot2 then select Install Get the data into your environment. TLH.df &lt;- readr::read_csv(file = here::here(&#39;data&#39;, &#39;TLH_SOD1892.csv&#39;), show_col_types = FALSE) |&gt; dplyr::filter(STATION == &#39;USW00093805&#39;) |&gt; dplyr::mutate(Date = as.Date(DATE)) |&gt; dplyr::mutate(Year = lubridate::year(Date), Month = lubridate::month(Date), Day = lubridate::day(Date), doy = lubridate::yday(Date)) |&gt; dplyr::select(Date, Year, Month, Day, doy, TMAX, TMIN, PRCP) package::function (:: is called a library specifier). Or, load the packages into your current environment with the library() function. Create a plot showing the frequency of daily high temperatures in Tallahassee. library(ggplot2) TLH.df |&gt; dplyr::group_by(TMAX) |&gt; dplyr::summarize(nH = dplyr::n()) |&gt; ggplot(mapping = aes(x = TMAX, y = nH)) + geom_col(col = &#39;white&#39;, fill = &quot;gray70&quot;) + labs(title = &quot;Frequency of Daily High Temperatures&quot;, subtitle = &quot;Tallahassee, FL, USA (1940-2018)&quot;, x = &quot;Daily High Temperature (°F)&quot;, y = &quot;Number of Days&quot;) + scale_x_continuous(breaks = seq(from = 20, to = 110, by = 10)) + theme_minimal() ## Warning: Removed 1 rows containing missing values (position_stack). "],["thursday-august-25-2022.html", "Thursday, August 25, 2022 Data science Structure of markdown files How to make a plot Getting started using R", " Thursday, August 25, 2022 Any questions about assignments or grades? Make sure you have a copy of the file 02_Lesson.Rmd and have it opened in the RStudio IDE. Follow along in your copy of the lesson file as I go line by line through the file Note that your file’s background and text might look different. You change things by going to Tools &gt; Global Options &gt; Appearance Much of the lesson materials come from online books: https://www.bigbookofr.com/index.html Datasets: https://kieranhealy.org/blog/archives/2020/08/25/some-data-packages/ Today Data science Structure of markdown files How to make a plot Getting started using R Data science Data science is done on a computer. You have two choices: use a spreadsheet or write code. A spreadsheet is convenient, but it makes the three conditions for a good data analysis reproducibility, communication, and automation difficult to achieve. Reproducibility A scientific paper is advertisement for a claim. But the proof of the claim is the procedure that was used to obtain the result. If your data science is to be convincing, the trail from the data you started with all the way to the final output must be freely available to the public. A reproducible trail with a spreadsheet is hard. It is easy to make mistakes (e.g., accidentally sorting just a column rather than the entire table). A set of instructions written as computer code is the exact procedure. (Open stronger-hur.Rmd in the folder Other_Rmds. Communication Computer code is a recipe. It communicates precisely what you did to get the result. Communication to others and to your future self. It’s hard to explain precisely what you did when working with a spreadsheet. Click here, then right click here, then choose menu X, etc. The words needed to describe these procedures are not standard. Code is standardized so it provides an efficient way to communicate because all important information is given as plain text with no ambiguity. Automation If you’ve ever made a map using a geographic information system (GIS) you know how hard it is to make another one (even one that is quite similar) with a new set of data (even a very similar one). Running code with new data is simple. Being able to code is an important skill for nearly all technical jobs. Here you will learn how to code. But keep in mind: Just like learning to write doesn’t mean you will be a writer (i.e., make a living writing), learning to code doesn’t mean you will be a coder. The R programming language R is a leading open source programming language for data science. R and Python. Free, open-source, runs on Windows, Macs, etc. Excellent graphing capabilities. Powerful, extensible, and relatively easy to learn syntax. Thousands of functions. Has all the cutting edge statistical methods including methods in spatial statistics. Used by scientists of all stripes. Most of the world’s statisticians use it (and contribute to it). Overview of this course We start by making graphs. You will learn to make clear, informative plots that help you understand your data. You will learn the basic structure of a making a plot. Visualization alone is not enough, so you will also learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries (data wrangling). You will then combine data wrangling and visualization with your curiosity to ask and answer interesting questions by learning how to fit models to your data. Data models extend your ability to ask and answer questions about this wonderful world of ours. Finally, with geographic and environmental data collected the models will include a spatial component. Structure of markdown files The ability to exactly reproduce your work is important to a scientific process. It is also pragmatic. The person most likely to reproduce your work is you. This is especially true for graphs and figures. These often have a finished quality to them as a result of tweaking and adjustments to the details. This makes it hard to reproduce them later. The goal is to do as much of this tweaking as possible with the code you write, rather than in a way that is invisible (retrospectively). Contrast editing an image in Adobe Illustrator. You will find yourself constantly going back and forth between three things: Writing code: You will write code to produce plots. You will also write code to load your data (get your data into R), to look quickly at tables of that data. Sometimes you will want to summarize, rearrange, subset, or augment your data, or fit a statistical model to it. You will want to be able to write that code as easily and effectively as possible. Looking at the output generated by your code. Your code is a set of instructions that produces the output you want: a table, a model, or a figure. It is helpful to be able to see and interpret the output. Taking notes. You will also write about what you are doing, and what your results mean. To do these things efficiently you want to write your code along with comments. This is where markdown files come into play (files that end with .Rmd) A R markdown file is a plain text document where text (such as notes or discussion) is interspersed with R code. When you Knit the markdown file the R code is executed line by line starting at the top of the file and the output supplements or replaces the code. The resulting file is then converted into a easily readable document formatted in HTML, PDF, or Word. The non-code segments of the document are plain text with simple formatting instructions (e.g., ## for section header). There is a set of conventions for marking up plain text in a way that indicates how it should be formatted. Markdown treats text surrounded by asterisks, double asterisks, and backticks in special ways. It is R Markdown’s way of saying that these words are in italics also italics bold, and code font Your class notes include code. There is a set format for including code into your markdown file (lines of code; code chunk). They look like this. library(ggplot2) These markings are called code chunk delimiters. Three back ticks (on a U.S. keyboard, the character under the escape key) followed by a pair of curly braces containing the name of the language you are using. The format is language-agnostic and can be used with, e.g. Python and other languages. The back ticks-and-braces signals that what follows is code. You write your code as needed, and then end the chunk with a new line containing three more back ticks. If you keep your notes in this way, you will be able to see the code you write, the output it produces, and your own commentary or clarification on what the code did, all in a convenient way. Moreover, you can turn it into a good-looking document straight away with the Knit button. This is how you will do everything in this course. In the end you will have a set of notes that you can turn into a book with bookdown. To learn more about markdown see: Everything markdown How to make a plot To help motivate your interest in this course, you start by making a graph. There are three things to learn: How to create a graph with a reusable {ggplot2} template How to add variables to a graph with aesthetics How to select the ‘type’ of graph with geoms The following examples are taken from the online book entitled: R for Data Science by Hadley Wickham and Garrett Grolemund, published by O’Reilly Media, Inc., 2016, ISBN: 9781491910399. https://r4ds.had.co.nz/. Let’s begin with a question. Do cars with big engines use more fuel than cars with small engines? A: Cars with bigger engines use more fuel. B: Cars with bigger engines use less fuel. You check which answer is correct using the mpg data that comes in {ggplot2} and a plot. The mpg object contains observations collected on 38 models of cars by the US Environmental Protection Agency. Among the variables in mpg are: displ, a car’s engine size, in liters. hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. To see a portion of the mpg data, type mpg after you loaded the package using the library() function. library(ggplot2) mpg ## # A tibble: 234 × 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto… f 18 29 p comp… ## 2 audi a4 1.8 1999 4 manu… f 21 29 p comp… ## 3 audi a4 2 2008 4 manu… f 20 31 p comp… ## 4 audi a4 2 2008 4 auto… f 21 30 p comp… ## 5 audi a4 2.8 1999 6 auto… f 16 26 p comp… ## 6 audi a4 2.8 1999 6 manu… f 18 26 p comp… ## 7 audi a4 3.1 2008 6 auto… f 18 27 p comp… ## 8 audi a4 quattro 1.8 1999 4 manu… 4 18 26 p comp… ## 9 audi a4 quattro 1.8 1999 4 auto… 4 16 25 p comp… ## 10 audi a4 quattro 2 2008 4 manu… 4 20 28 p comp… ## # … with 224 more rows ## # ℹ Use `print(n = ...)` to see more rows You see the first 10 rows and 10 columns of the data. Note that there are 234 rows and 11 columns so you are only viewing a portion of this spreadsheet. Each row is a different car. The first row is the Audi A4 1999 model with automatic transmission (5 gears). The tenth car listed is the Audi A4 Quattro with manual transmission (6 gears). The column labeled displ is the engine size in liters. Bigger number means the car has a larger engine. The column labeled hwy is the miles per gallon. Bigger number means the car uses more fuel to go the same distance (lower efficiency). It is hard to check which answer is correct by only looking at these 10 cars. Note that bigger engines appear to have smaller values of highway mileage but it is far from clear. You want to look at all 234 cars. The code below uses functions from the {ggplot2} package to plot the relationship between displ and hwy for all cars. Let’s look at the plot and then talk about the code itself. To see the plot, click on the little green triangle in the upper right corner of the gray shaded region. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) The plot shows an inverse relationship between engine size (displ) and fuel efficiency (hwy). Each point is a different car. Cars that have a large value of displ tend to have a small value of hwy and cars with a small value of displ tend to have a large value of hwy. In other words, cars with big engines use more fuel. If that was your answer, you were right! Now let’s look at how you made the plot. Here’s the code used to make the plot. Notice that it contains three functions: ggplot(), geom_point(), and aes(). ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) The first function, ggplot(), creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. By itself, ggplot(data = mpg) creates an empty graph, but it is not very interesting so I’m not going to show it here. The function geom_point() adds a layer of points to the empty plot created by ggplot(). As a result, you get a scatterplot. The function geom_point() takes a mapping argument, which defines which variables in your dataset are mapped to which axes in your graph. The mapping argument is always paired with the function aes(), which you use to bring together the mappings you want to create. Here, you want to map the displ variable to the x axis (horizontal axis) and the hwy variable to the y axis (vertical axis), so you add x = displ and y = hwy inside of aes() (and you separate them with a comma). Where will ggplot() look for these mapped variables? In the data frame that you passed to the data argument, in this case, mpg. Knit to generate HTML. Compare the HTML with the Rmd. A graphing workflow The code above follows the common work flow for making graphs. To make a graph, you: Start the graph with ggplot() Add elements to the graph with a geom_ function Select variables with the mapping = aes() argument A graphing template In fact, you can turn your code into a reusable template for making graphs. To make a graph, replace the bracketed sections in the code below with a data set, a geom_ function, or a collection of mappings. Give it a try! Copy and paste the above code chunk, including the code chunk delimiters, and replace the y = hwy with y = cty. ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = cty)) Replace the bracketed sections &lt; &gt; with mpg, geom_boxplot, and x = class, y = hwy to make a slightly different graph. ggplot(data = mpg) + geom_boxplot(mapping = aes(x = class, y = hwy)) Common problems As you start to work with R code, you are likely to run into problems. Don’t worry — it happens to everyone. I’ve been writing R code for decades, and I still write code that doesn’t work! Start by comparing the code that you are running to the code in the examples in these notes. R is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". Also pay attention to capitalization; R is case sensitive. Location of the + sign One common problem when creating {ggplot2} graphics is to put the + in the wrong place: it must come at the end of a line, not the start. In other words, make sure you haven’t accidentally written code like this: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) Help! If you’re still stuck, try the help. You can get help about any R function by running ?function_name in a code chunk, e.g. ?geom_point. Don’t worry if the help doesn’t seem that helpful — instead skip down to the bottom of the help page and look for a code example that matches what you’re trying to do. If that doesn’t help, carefully read the error message that appears when you run your (non-working) code. Sometimes the answer will be buried there! But when you’re new to R, you might not yet know how to understand the error message. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online. Getting started using R You are getting oriented to the language itself (what happens at the console), while learning to take notes in what might seem like an odd format (chunks of code interspersed with plain-text comments), in an IDE (integrated development environment) that that has many features designed to make your life easier in the long run, but which can be hard to decipher at the beginning. Here are some general points to keep in mind about how R is designed. They might help you get a feel for how the language works. Everything has a name In R, everything you deal with has a name. You refer to things by their names as you examine, use, or modify them. Named entities include variables (like x, or y), data that you have loaded (like my_data), and functions that you use. (More about functions soon.) You will spend a lot of time talking about, creating, referring to, and modifying things with names. Things are listed under the Environment tab in the upper right panel. Some names are forbidden. These include reserved words like FALSE and TRUE, core programming words like Inf, for, else, break, function, and words for special entities like NA and NaN. (These last two are codes designating missing data and “Not a Number”, respectively.) You probably won’t use these names by accident, but it’s good do know that they are not allowed. Some names you should not use, even if they are technically permitted. These are mostly words that are already in use for objects or functions that form part of the core of R. These include the names of basic functions like q() or c(), common statistical functions like mean(), range() or var(), and built-in mathematical constants like pi. Names in R are case sensitive. The object my_data is not the same as the object My_Data. When choosing names for things, be concise, consistent, and informative. Follow the style of the tidyverse and name things in lower case, separating words with the underscore character, _, as needed. Do not use spaces when naming things, including variables in your data. Everything is an object Some objects are part of R, some are added via packages, and some are created by you. But almost everything is some kind of object. The code you write will create, manipulate, and use named objects. Here you create a vector of numbers. The command c() is a function. It’s short for “combine” or “concatenate”. It will take a sequence of comma-separated things inside the parentheses and join them into a vector where each element is still accessible. c(1, 2, 3, 1, 3, 5, 25) ## [1] 1 2 3 1 3 5 25 Instead of sending the result to the console, here you instead assign the result to an object. my_numbers &lt;- c(1, 2, 3, 1, 3, 5, 25) your_numbers &lt;- c(5, 31, 71, 1, 3, 21, 6) To see what you created, type the name of the object and hit return. my_numbers ## [1] 1 2 3 1 3 5 25 Each of our numbers is still there, and can be accessed directly if you want. They are now just part of a new object, a vector, called my_numbers. You create objects by assigning them to names. The assignment operator is &lt;-. Think of assignment as the verb “gets”, reading left to right. So, the bit of code above is read as “The object my_numbers gets the result of concatenating the following numbers: 1, 2, …” The operator is two separate keys on your keyboard: the &lt; key and the - (minus) key. When you create objects by assigning things to names, they come into existence in R’s workspace or environment. You do things using functions You do almost everything in R using functions. Think of a function as a special kind of object that can perform actions for you. It produces output based on the input that it receives. Like a good dog, when you want a function to do something, you call it. Somewhat less like a dog, it will reliably do what you tell it. You give the function some information, it acts on that information, and some results come out the other side. Functions can be recognized by the parentheses at the end of their names. This distinguishes them from other objects, such as single numbers, named vectors, tables of data, and so on. You send information to the function between the parentheses. Most functions accept at least one argument. A function’s arguments are the things it needs to know in order to do something. They can be some bit of your data (data = my_numbers), or specific instructions (title = \"GDP per Capita\"), or an option you want to choose (smoothing = \"splines\", show = FALSE). For example, the object my_numbers is a numeric vector: my_numbers ## [1] 1 2 3 1 3 5 25 But the thing you used to create it, c(), is a function. It combines the items into a vector composed of the series of comma-separated elements you give it. Similarly, mean() is a function that calculates a simple average for a vector of numbers. What happens if you just type mean() without any arguments inside the parentheses? mean() The error message is terse but informative. The function needs an argument to work, and you haven’t given it one. In this case, ‘x’, the name of another object that mean() can perform its calculation on: mean(x = my_numbers) ## [1] 5.714286 Or mean(x = your_numbers) ## [1] 19.71429 While the function arguments have names that are used internally, (here, x =), you don’t strictly need to specify the name for the function to work: mean(my_numbers) ## [1] 5.714286 If you omit the name of the argument, R will just assume you are giving the function what it needs, and in some order. The documentation for a function will tell you what the order of required arguments is for any particular function. For simple functions that only require one or two arguments, omitting their names is usually not confusing. For more complex functions, you will typically want to use the names of the arguments rather than try to remember what the ordering is. In general, when providing arguments to a function the syntax is &lt;argument&gt; = &lt;value&gt;. If &lt;value&gt; is a named object that already exists in your workspace, like a vector of numbers of a table of data, then you provide it unquoted, as in mean(my_numbers). If &lt;value&gt; is not an object, a number, or a logical value like TRUE, then you usually put it in quotes, e.g., labels(x = \"X Axis Label\"). Functions take inputs via their arguments, do something, and return outputs. What the output is depends on what the function does. The c() function takes a sequence of comma-separated elements and returns a vector consisting of those same elements. The mean() function takes a vector of numbers and returns a single number, their average. Functions can return far more than single numbers. The output returned by functions can be a table of data, or a complex object such as the results of a linear model, or the instructions needed to draw a plot. They can even be other functions. For example, the summary() function performs a series of calculations on a vector and produces what is in effect a little table with named elements. A function’s argument names are internal to that function. Say you have created an object in your environment named x, for example. A function like mean() also has a named argument, x, but R will not get confused by this. It will not use your x object by mistake. As you have already seen with c() and mean(), you can assign the result of a function to an object: my_summary &lt;- summary(my_numbers) When you do this, there’s no output to the console. R just puts the results into the new object, as you instructed. To look inside the object you can type its name and hit return: my_summary ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.500 3.000 5.714 4.000 25.000 Functions come in packages (libraries) The code you write will be more or less complex depending on the task you want to accomplish. Families of useful functions are bundled into packages that you can install, load into your R session, and make use of as you work. Packages save you from reinventing the wheel. They make it so that you do not, for example, have to figure out how to write code from scratch to draw a shape on screen, or load a data file into memory. Packages are also what allow you to build on the efforts of others in order to do your own work. {ggplot2} is a package of functions. There are many other such packages and you will make use of several throughout this course, either by loading them with the library() function, or “reaching in” to them and pulling a useful function from them directly. All of the work you will do this semester will involve choosing the right function or functions, and then giving those functions the right instructions through a series of named arguments. Most of the mistakes you will make, and the errors you will fix, will involve having not picked the right function, or having not fed the function the right arguments, or having failed to provide information in a form the function can understand. For now, just remember that you do things in R by creating and manipulating named objects. You manipulate objects by feeding information about them to functions. The functions do something useful with that information (calculate a mean, re-code a variable, fit a model) and give you the results back. Try these out. table(my_numbers) ## my_numbers ## 1 2 3 5 25 ## 2 1 2 1 1 sd(my_numbers) ## [1] 8.616153 my_numbers * 5 ## [1] 5 10 15 5 15 25 125 my_numbers + 1 ## [1] 2 3 4 2 4 6 26 my_numbers + my_numbers ## [1] 2 4 6 2 6 10 50 The first two functions here gave us a simple table of counts and calculated the standard deviation of my_numbers. It’s worth noticing what R did in the last three cases. First you multiplied my_numbers by two. R interprets that as you asking it to take each element of my_numbers one at a time and multiply it by five. It does the same with the instruction my_numbers + 1. The single value is “recycled” down the length of the vector. By contrast, in the last case we add my_numbers to itself. Because the two objects being added are the same length, R adds each element in the first vector to the corresponding element in the second vector. Your turn Create a code chunk to compute the coefficient of variation (standard deviation divided by the mean) for your numbers (my_numbers). "],["tuesday-august-30-2022.html", "Tuesday, August 30, 2022 Graphing examples Working with R Data vectors", " Tuesday, August 30, 2022 Today Graphing examples Working with R Data vectors If your analysis is to be a convincing, the trail from data to final output must be open and available to all. Markdown helps you create scientific reports that are a mixture of text and code. This makes it easy to create an understandable trail from hypothesis, to data, to analysis, to results. Reproducible research. Graphing examples Scatter plots Functions from the {ggplot2} package are used to make graphs. You make these graphing functions available for a given session of R (every time you open RStudio) with the library(ggplot2) function. As an example, consider the data frame called airquality. The data contains daily air quality measurements from a location in New York City between May and September of 1973. Follow along by pressing the green arrows when you get to a code chunk. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 dim(airquality) ## [1] 153 6 The data contains 153 rows and 6 columns. Each row is a set of measurements across six variables on a given day. Most data you will work with are like this. Each row is a set of measurements (a case) and each column is a variable. The columns (variables) include the measurements of ozone concentration (Ozone) (ppm), solar radiation (Solar.R) (langley), wind speed (Wind) (mph), temperature (Temp) (F), as well as Month and Day. Question: Are ozone concentrations higher on warmer days? Let’s see what the data say. The scatter plot is one of the most useful statistical graphs. It describes the relationship between two variables. It is made by plotting the variables in a plane defined by the values of the variables. Using the {ggplot2} functions, you answer the question above by mapping the Temp variable to the x aesthetic and the Ozone variable to the y aesthetic. More simply you could say that you plot Temp on the x axis and Ozone on the y axis. Put you want to recognize that the axes are aesthetics (there are other aesthetics like color, size, etc). library(ggplot2) ggplot(data = airquality) + geom_point(mapping = aes(x = Temp, y = Ozone)) ## Warning: Removed 37 rows containing missing values (geom_point). What do you see? Why the warning? To suppress the warning, you add the argument na.rm = TRUE in the geom_point() function. ggplot(data = airquality) + geom_point(mapping = aes(x = Temp, y = Ozone), na.rm = TRUE) To help us better describe the relationship you add another layer. This layer is defined by geom_smooth() which takes the same aesthetics. ggplot(data = airquality) + geom_point(mapping = aes(x = Temp, y = Ozone), na.rm = TRUE) + geom_smooth(mapping = aes(x = Temp, y = Ozone), na.rm = TRUE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; The smooth line describes how the average ozone concentration varies with temperature. For lower temperatures there is not much change in ozone concentrations as temperatures increase, but for higher temperatures the increase in ozone concentrations is more pronounced. In the above code you used the same mapping for the point layer and the smooth layer. You can simplify the code by putting the mapping = argument into the ggplot() function. ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point(na.rm = TRUE) + geom_smooth(na.rm = TRUE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Question: On average is ozone concentration higher on windy days? Create a graph to help you answer this question. ggplot(data = airquality, mapping = aes(x = Wind, y = Ozone)) + geom_point(na.rm = TRUE) + geom_smooth(na.rm = TRUE) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; What is the answer? You can use a label instead of a dot for the locations in this two-dimensional scatter plot by adding the label aesthetic and using geom_text. ggplot(data = airquality, mapping = aes(x = Wind, y = Ozone, label = Ozone)) + geom_text(na.rm = TRUE) To map an aesthetic to a variable, associate the name of the aesthetic to the name of the variable inside aes(). You can make the plot interactive by using the ggplotly() function from the {plotly} package. You simply put the above code inside this function. plotly::ggplotly( ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point(na.rm = TRUE) + geom_smooth(na.rm = TRUE) ) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Hover/zoom etc. As another example, consider the Palmer penguin data set from https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/. The data are located on the web at the following URL. You first save the location as an object called loc. loc &lt;- &quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv&quot; Note that this object is now located in our environment. It is simply a string of characters (letters, backslashes, etc) in quotes. A character object. Next you get the data and save it as an object called penguins with the read_csv() function from the {readr} package. Inside the parentheses of the function you put the name of the location. penguins &lt;- readr::read_csv(loc) ## Rows: 344 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): species, island, sex ## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Note that the object penguins is now in your environment. It is a data frame containing 344 rows (observations) and 8 variables. You list the first 10 rows and 7 columns by typing the name of the object as follows. penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## 7 Adelie Torgersen 38.9 17.8 181 3625 fema… 2007 ## 8 Adelie Torgersen 39.2 19.6 195 4675 male 2007 ## 9 Adelie Torgersen 34.1 18.1 193 3475 &lt;NA&gt; 2007 ## 10 Adelie Torgersen 42 20.2 190 4250 &lt;NA&gt; 2007 ## # … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows The data are 344 individual penguins each described by species (Adelie, Chinstrap, Gentoo), where it was found (island name), length of bill (mm), depth of bill (mm), body mass (g), male or female, and year. Each penguin belongs to one of three species. To see how many of the 344 penguins are in each species you use the table() function. Between the parentheses of this function you put the name of the data penguins followed by the $ sign followed by the name of the column species. table(penguins$species) ## ## Adelie Chinstrap Gentoo ## 152 68 124 Said another way, you reference columns in the data with the $ sign so that penguins$species is how you refer to the column species in the data object named penguins. There are 152 Adelie, 68 Chinstrap, and 124 Gentoo penguins. You plot the relationship between flipper length and body mass for each of the three species. ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + geom_point() ## Warning: Removed 2 rows containing missing values (geom_point). Penguin flipper length and body mass show a positive relationship (association). Penguins with longer flippers tend to be larger. How does this positive relationship vary by species? You answer this question with another aesthetic. You assign a level of the aesthetic (here a color) to each unique value of the variable, a process known as scaling. The ggplot() function also adds a legend that explains which levels correspond to which values. ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + geom_point() + scale_color_manual(values = c(&quot;darkorange&quot;,&quot;darkorchid&quot;,&quot;cyan4&quot;)) ## Warning: Removed 2 rows containing missing values (geom_point). Returning to the mpg data set from last time. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = class)) + geom_point() The colors reveal that the unusual points (on the right side of the plot) are two-seaters. Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines. In the above example, you mapped class to the color aesthetic, but you could have mapped class to the shape aesthetic, which controls point shapes. ggplot(data = mpg, mapping = aes(x = displ, y = hwy, shape = class)) + geom_point() + geom_smooth(method = lm, se = FALSE) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: The shape palette can deal with a maximum of 6 discrete values because ## more than 6 becomes difficult to discriminate; you have 7. Consider ## specifying shapes manually if you must have them. ## Warning: Removed 62 rows containing missing values (geom_point). What happened to the SUVs? The ggplot() function will only use six shapes at a time. By default, additional groups will go un-plotted when you use the shape aesthetic. For each aesthetic, you use aes() to associate the name of the aesthetic with a variable to display. The aes() function gathers together each of the aesthetic mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves aesthetics, visual properties that you can map to variables to display information about the data. You can also set the aesthetic properties of your geom manually. For example, you can make all of the points in our plot blue. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(color = &quot;blue&quot;) Here, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an aesthetic manually, set the aesthetic by name as an argument of your geom function; i.e. it goes outside of aes(). You’ll need to pick a level that makes sense for that aesthetic: The name of a color as a character string (with quotes). The size of a point in millimeters. The shape of a point as a number, as shown below. R has 25 shapes that are identified by numbers. There are some seeming duplicates: for example, 0, 15, and 22 are all squares. The difference comes from the interaction of the color and fill aesthetics. The hollow shapes (0–14) have a border determined by color; the solid shapes (15–18) are filled with color; the filled shapes (21–24) have a border of color and are filled with fill. Facets Another way to add additional variables useful for categorical variables is to split the plot into facets. A facet is a subplot on one subset of the data. A categorical variable is one that can take only a limited, and usually fixed, number of possible values so you can split the plot for each value of the categorical variable. You can use facet_wrap() to create a faceted plot. The first argument of facet_wrap() is a formula, which you create with ~ followed by a variable name (here “formula” is the name of a data structure in R). The variable that you pass to facet_wrap() should only have a limited number of values (categorical). The variable class in the data frame mpg is a character string. You can see this by typing str(mpg) ## tibble [234 × 11] (S3: tbl_df/tbl/data.frame) ## $ manufacturer: chr [1:234] &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; &quot;audi&quot; ... ## $ model : chr [1:234] &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; &quot;a4&quot; ... ## $ displ : num [1:234] 1.8 1.8 2 2 2.8 2.8 3.1 1.8 1.8 2 ... ## $ year : int [1:234] 1999 1999 2008 2008 1999 1999 2008 1999 1999 2008 ... ## $ cyl : int [1:234] 4 4 4 4 6 6 6 4 4 4 ... ## $ trans : chr [1:234] &quot;auto(l5)&quot; &quot;manual(m5)&quot; &quot;manual(m6)&quot; &quot;auto(av)&quot; ... ## $ drv : chr [1:234] &quot;f&quot; &quot;f&quot; &quot;f&quot; &quot;f&quot; ... ## $ cty : int [1:234] 18 21 20 21 16 18 18 18 16 20 ... ## $ hwy : int [1:234] 29 29 31 30 26 26 27 26 25 28 ... ## $ fl : chr [1:234] &quot;p&quot; &quot;p&quot; &quot;p&quot; &quot;p&quot; ... ## $ class : chr [1:234] &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; &quot;compact&quot; ... There are seven car classes. You put class in the facet_wrap() function. Everything is the same as before on the first two code lines but you add the facet_wrap() function. ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point() + facet_wrap(~ class, nrow = 2) The output produces separate scatter plots one for each of the seven classes. More on graphs later. Working with R Calculations Let’s see how you can do some arithmetic in R. R evaluates commands typed at the prompt and returns the result to the screen. The prompt is the blue greater than symbol (&gt;). To find the sum of the square root of 25 and 2, at the prompt type sqrt(25) + 2 ## [1] 7 The number inside the brackets indexes the output. Here there is only one bit of output, the answer 7. The prompt that follows indicates R is ready for another command. 12/3 - 5 ## [1] -1 How would you calculate the 5th power of 2? How would you find the product of 10.3 &amp; -2.9? How would you find the average of 8.3 and 10.2? How about 4.5% of 12,000? .045 * 12000 ## [1] 540 Functions Many math and statistical functions are available. A function has a name followed by a pair of parentheses. Arguments are placed inside the parentheses as needed. For example, sqrt(2) ## [1] 1.414214 sin(pi) ## [1] 1.224647e-16 How do you interpret this output? Type (highlight then click Run): .0000000000000001224647 Why not zero? What does the e-16 mean? exp(1) ## [1] 2.718282 log(10) ## [1] 2.302585 Many functions have arguments with default values. For example, you only need to tell the random number generator rnorm() how many numbers to produce. The default mean is zero. To replace the default value, specify the corresponding argument name. rnorm(10) ## [1] 0.7025177 -0.9754995 -0.4720601 0.3511049 0.4590852 -0.9718834 ## [7] 2.1482690 1.6257874 -1.5347617 0.2246139 rnorm(10, mean = 5) ## [1] 6.413220 5.863069 5.178179 4.728924 4.689347 4.992154 5.209839 5.167819 ## [9] 6.672935 3.879096 Syntax is important You get an error message when you type a function that R does not understand. For example: squareroot(2) Error: could not find function “squareroot” sqrt 2 Error: syntax error sqrt(-2) ## Warning in sqrt(-2): NaNs produced ## [1] NaN sqrt(2 The last command shows what happens if R encounters a line that is not complete. The continuation prompt (+) is printed, indicating you did not finish the command. Saving an object Use the assignment operator to save an object. You put a name on the left-hand side of the left pointing arrow (&lt;-) and the value on the right. Assignments do not produce output. x &lt;- 2 x + 3 ## [1] 5 x &lt;- 10 Here you assigned x to be a numeric object. Assignments are made using the left-pointing arrow (less than followed by a dash) [or an equal sign.] Object names You are free to make object names out of letters, numbers, and the dot or underline characters. A name starts with a letter or a dot (a leading dot may not be followed by a number). But you can’t use mathematical operators, such as +, -, *, and /. Some examples of names include: x &lt;- 2 n &lt;- 25 a.long.number &lt;- 123456789 ASmallNumber &lt;- .001 Case matters. DF is different than df or Df. Some names are commonly used to represent certain types of data. For instance, n is for length; x or y are data vectors; and i and j are integers and indices. These conventions are not forced, but consistent use of them makes it easier for you (and others) to understand what you’ve done. Entering data The c() function is useful for getting a small amount of data into R. The function combines (concatenates) items (elements). Example: consider a set of hypothetical annual land falling hurricane counts over a ten-year period. 2 3 0 3 1 0 0 1 2 1 To enter these into your environment, type counts &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) counts ## [1] 2 3 0 3 1 0 0 1 2 1 Notice a few things. You assigned the values to an object called counts. The assignment operator is an equal sign (=). Values do not print. They are assigned to an object name. They are printed by typing the object name as you did on the second line. Finally, the values when printed are prefaced with a [1]. This indicates that the object is a vector and the first entry in the vector is a value of 2 (The number immediately to the right of [1]). More on this later. You can save some typing by using the arrow keys to retrieve previous commands. Each command is stored in a history file and the up arrow key will move backwards through the history file and the down arrow forwards. The left and right arrow keys will work as expected. Applying a function Once the data are stored in an object, you use functions on them. R comes with all sorts of functions that you can apply to your counts data. sum(counts) ## [1] 13 length(counts) ## [1] 10 sum(counts)/length(counts) ## [1] 1.3 For this example, the sum() function returns the total number of hurricanes making landfall. The length() function returns the number of years, and sum(counts)/length(counts) returns the average number of hurricanes per year. Other useful functions include, sort(), min(), max(), range(), diff(), and cumsum(). Try these on the landfall counts. What does range() do? What does diff() do? Averge The average (or mean) value of a set of numbers (\\(x\\)’s) is defined as: \\[ \\bar x = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\] The function mean() makes this calculation on your set of counts. mean(counts) ## [1] 1.3 Data vectors The count data is stored as a vector. R keeps track of the order that the data were entered. First element,second element, and so on. This is good for a couple of reasons. Here the data has a natural order - year 1, year 2, etc. You don’t want to mix these. You would like to be able to make changes to the data item by item instead of entering the entire data again. Also vectors are math objects making them easy to manipulate. Suppose counts contain the annual number of land-falling hurricanes from the first decade of a longer record. You want to keep track of counts over other decades. This could be done by the following, example. cD1 &lt;- counts cD2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Note that you make a copy of the first decade of counts and save the vector using a different object name. Most functions operate on each element of the data vector at the same time. cD1 + cD2 ## [1] 2 8 4 5 4 0 3 4 4 2 The first year of the first decade is added from the first year of the second decade and so on. What happens if you apply the c() function to these two vectors? c(cD1, cD2) ## [1] 2 3 0 3 1 0 0 1 2 1 0 5 4 2 3 0 3 3 2 1 If you are interested in each year’s count as a difference from the decade mean, you type: cD1 - mean(cD1) ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 In this case a single number (the mean of the first decade) is subtracted from each element of the vector of counts. This is an example of data recycling. R repeats values from one vector so that its length matches the other vector. Here the mean is repeated 10 times. Variance Suppose you are interested in the variance of the set of landfall counts. The formula is given by: \\[ \\hbox{var}(x) = \\frac{(x_1 - \\bar x)^2 + (x_2 - \\bar x)^2 + \\cdots + (x_n - \\bar x)^2}{n-1} \\] Note: The formula is given as LaTeX math code with the double dollar signs starting (and ending) the math mode. It’s a bit hard to read but it translates exactly to math as you would read it in a scientific article or textbook. Look at the HTML file. Although the var() function will compute this for you, here you see how you could do this directly using the vectorization of functions. The key is to find the squared differences and then add up the values. The key is to find the squared differences and then add them up. x &lt;- cD1 xbar &lt;- mean(x) x - xbar ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 (x - xbar)^2 ## [1] 0.49 2.89 1.69 2.89 0.09 1.69 1.69 0.09 0.49 0.09 sum((x - xbar)^2) ## [1] 12.1 n &lt;- length(x) n ## [1] 10 sum((x - xbar)^2)/(n - 1) ## [1] 1.344444 To verify type var(x) ## [1] 1.344444 Data vectors have a type One restriction on data vectors is that all the values have the same type. This can be numeric, as in counts, character strings, as in simpsons &lt;- c(&quot;Homer&quot;, &quot;Marge&quot;, &quot;Bart&quot;, &quot;Lisa&quot;, &quot;Maggie&quot;) simpsons ## [1] &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Lisa&quot; &quot;Maggie&quot; Note that character strings are made with matching quotes, either double, “, or single, ’. If you mix the type within a data vector, the data will be coerced into a common type, which is usually a character. Arithmetic operations do not work on characters. Returning to the land falling hurricane counts. cD1 &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) cD2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Now suppose the National Hurricane Center (NHC) reanalyzes a storm, and that the 6th year of the 2nd decade is a 1 rather than a 0 for the number of landfalls. In this case you type cD2[6] &lt;- 1 # assign the 6 year of the decade a value of 1 landfall The assignment to the 6th entry in the vector cD2 is done by referencing the 6th entry of the vector with square brackets []. It’s important to keep this in mind: Parentheses () are used for functions and square brackets [] are used to extract values from vectors (and arrays, lists, etc). REPEAT: [] are used to extract or subset values from vectors, data frames, matrices, etc. cD2 #print out the values ## [1] 0 5 4 2 3 1 3 3 2 1 cD2[2] # print the number of landfalls during year 2 of the second decade ## [1] 5 cD2[4] # 4th year count ## [1] 2 cD2[-4] # all but the 4th year ## [1] 0 5 4 3 1 3 3 2 1 cD2[c(1, 3, 5, 7, 9)] # print the counts from the odd years ## [1] 0 4 3 3 2 One way to remember how to use functions is to think of them as pets. They don’t come unless they are called by name (spelled properly). They have a mouth (parentheses) that likes to be fed (arguments), and they will complain if they are not feed properly. Working smarter R’s console keeps a history of your commands. The previous commands are accessed using the up and down arrow keys. Repeatedly pushing the up arrow will scroll backward through the history so you can reuse previous commands. Many times you wish to change only a small part of a previous command, such as when a typo is made. With the arrow keys you can access the previous command then edit it as desired. "],["thursday-september-1-2022.html", "Thursday, September 1, 2022 Sample statistics Structured data Tables and summaries", " Thursday, September 1, 2022 Today Sample statistics Structured data Tables and summaries Sample statistics Once data are stored as an object, you use functions on them. Some common functions used on simple data objects include sum(counts) ## [1] 13 length(counts) ## [1] 10 sum(counts)/length(counts) ## [1] 1.3 For this example, the sum() function returns the total number of hurricanes making landfall. The length() function returns the number of years, and sum(counts)/length(counts) returns the average number of hurricanes per year. Mean The average (or mean) value of a set of numbers (\\(x\\)’s) is defined as: \\[ \\bar x = \\frac{x_1 + x_2 + \\cdots + x_n}{n} \\] Note: The formula is given as LaTeX math code with the double dollar signs starting (and ending) the math mode. It’s a bit hard to read but it translates exactly to math as you would read in a scientific article or textbook. The function mean() makes this calculation on your set of counts. mean(counts) ## [1] 1.3 The counts data is stored as a vector. R keeps track of the order that the data were entered. First element, second element, and so on. This is good for a couple of reasons. Here the data have a natural order - year 1, year 2, etc. You don’t want to mix these. You would like to be able to make changes to the data item by item instead of entering the entire data again. Also vectors are math objects making them easy to manipulate. Suppose counts contain the annual number of land-falling hurricanes from the first decade of a longer record. You want to keep track of counts over other decades. This could be done by the following, example. cD1 &lt;- counts cD2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Note that you make a duplicate copy of the vector called counts giving it a different name. Most functions operate on each element of the data vector at the same time. cD1 + cD2 ## [1] 2 8 4 5 4 0 3 4 4 2 The first year of the first decade is added to the first year of the second decade and so on. What happens if you apply the c() function to these two vectors? c(cD1, cD2) ## [1] 2 3 0 3 1 0 0 1 2 1 0 5 4 2 3 0 3 3 2 1 If you are interested in each year’s count as a difference from the decade mean, you type: cD1 - mean(cD1) ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 In this case a single number (the mean of the first decade) is subtracted from each element of the vector of counts. This is an example of data recycling. R repeats values from one vector so that the length of this vector matches the other, longer vector. Here the mean is repeated 10 times. Variance Suppose you are interested in by how much the set of annual landfall counts varies from year to year. The formula for the variance is given by: \\[ \\hbox{var}(x) = \\frac{(x_1 - \\bar x)^2 + (x_2 - \\bar x)^2 + \\cdots + (x_n - \\bar x)^2}{n-1} \\] Although the var() function will compute this, here you see how it can be computed from other simpler functions. The first step is to find the squared difference between each value and the mean. To simplify things first create a new vector x and assign the mean of the x’s to xbar. x &lt;- cD1 xbar &lt;- mean(x) x - xbar ## [1] 0.7 1.7 -1.3 1.7 -0.3 -1.3 -1.3 -0.3 0.7 -0.3 (x - xbar)^2 ## [1] 0.49 2.89 1.69 2.89 0.09 1.69 1.69 0.09 0.49 0.09 The sum of the differences is zero, but not the sum of the squared differences. sum((x - xbar)^2) ## [1] 12.1 n &lt;- length(x) n ## [1] 10 sum((x - xbar)^2)/(n - 1) ## [1] 1.344444 So the variance is 1.344. To verify with the var() function type var(x) ## [1] 1.344444 Median Recall that the mean is a statistic calculated on our data. Typically there are more data values close to the mean than far from it. A normal random variable is within two standard deviations of its mean about 95% of the time. The median is a statistic defined exactly as the middle value. For example, consider a set of seven data values. Here the seven values are generated randomly. The set.seed() function guarantees that everyone (with a particular seed number) will get the same set of values. set.seed(3043) y &lt;- rnorm(n = 7) sort(y) ## [1] -1.855028975 -1.536523195 -1.113848013 -0.863720993 -0.813241685 ## [6] 0.002064746 1.024752099 The argument value n = 7 guarantees seven values. They are sorted from lowest on the left to highest on the right with the sort() function. The middle value is the fourth value from the left in the ordered list of data values. median(y) ## [1] -0.863721 The median divides the data set into the top half (50%) of the data values and the bottom half of the data values. With an odd number of values, the median is the middle one; with an even number of values, the median is the average of the two middle values. y &lt;- rnorm(n = 8) sort(y) ## [1] -2.03716871 -1.32753574 -0.74852359 -0.62357212 0.07656504 0.50029011 ## [7] 1.38629034 1.42971671 median(y) ## [1] -0.2735035 You check to see this is true no matter what the values are or what even number of values you choose. N = 20 y &lt;- rnorm(n = N) y_sorted &lt;- sort(y) median(y) == (y_sorted[N/2] + y_sorted[N/2 + 1]) / 2 ## [1] TRUE The median value, as a statistic representing the middle of a set of data values, is said to be resistant to extreme values (outliers). Consider the wealth (in 1000s of $) of five bar patrons. patrons &lt;- c(50, 60, 100, 75, 200) Now consider the same bar and patrons after a multimillionaire walks in. patrons_with_mm &lt;- c(patrons, 50000) mean(patrons) ## [1] 97 mean(patrons_with_mm) ## [1] 8414.167 median(patrons) ## [1] 75 median(patrons_with_mm) ## [1] 87.5 The difference in the mean wealth with and without the millionaire present is substantial while the difference in median wealth with and without the millionaire is small. Statistics that are not greatly influenced be a few values far from the bulk of the data are called resistant. The cfb data set from the {UsingR} package has data from the Survey of Consumer Finances conducted by the U.S. Federal Reserve Board (in 2001). Some of the income values are much higher than the bulk of the data. This tendency is common in income distributions. A few people tend to accumulate enormous wealth. Make the data available with the library() function, then show the first ten rows and ten columns by typing the name of the data object (cfb). library(UsingR) ## Loading required package: MASS ## Loading required package: HistData ## Loading required package: Hmisc ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units ## ## Attaching package: &#39;UsingR&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## cancer cfb ## WGT AGE EDUC INCOME CHECKING SAVING NMMF STOCKS ## X17470 5749.9746 54 14 66814.1946 6000 2000 0 500 ## X315 5870.6340 40 12 42144.3381 400 0 0 0 ## X8795 8043.6950 35 14 25697.7671 1000 160 0 0 ## X10720 6092.8720 55 12 35976.8740 2600 19100 0 0 ## X19170 7161.7566 40 12 39060.6061 1000 8300 0 3500 ## X22075 11429.6335 82 12 13362.8389 1000 0 50000 0 ## X12235 5988.0417 26 16 61674.6411 3000 0 0 0 ## X7670 7111.7751 50 14 53451.3557 3100 0 0 0 ## X16555 7602.8631 71 12 16446.5710 1000 0 0 0 ## X370 9917.0148 70 6 9867.9426 50 0 0 0 ## X7680 7263.7921 52 12 35976.8740 1700 3000 2000 0 ## X6880 7039.9174 53 11 7195.3748 0 0 0 0 ## X16570 6523.7932 27 16 78121.2121 8500 8000 1100 1500 ## X12945 6490.4551 27 12 28781.4992 0 0 4000 0 ## X6725 8265.3192 69 12 12334.9282 500 1600 0 0 ## X15725 1616.6743 55 17 459476.0766 0 0 0 0 ## X19880 6805.1027 42 14 54479.2663 3200 55 0 0 ## X225 6865.3880 73 12 43172.2488 11000 2000 100000 0 ## X4995 7731.3206 76 12 69897.9266 296440 0 0 0 ## X7700 5693.9061 43 12 58590.9091 750 1700 13000 0 ## X11375 6660.1557 48 11 52423.4450 0 1200 17600 0 ## X17920 6764.6424 57 12 25697.7671 1600 900 0 0 ## X12365 5591.7642 44 16 51395.5343 590 1780 0 14000 ## X920 5812.9110 44 15 87372.4083 300 32700 0 0 ## X19050 1022.1029 59 13 59618.8198 1500 0 0 75000 ## X19555 8909.1588 47 14 25697.7671 17320 730 0 0 ## X10520 4336.5281 25 14 26725.6778 800 1500 0 0 ## X18705 8691.5555 28 16 71953.7480 4020 11830 0 0 ## X5095 7620.1135 74 12 48311.8022 3500 0 0 0 ## X11010 7683.5398 62 11 6475.8373 0 250 0 0 ## X3540 10144.6672 23 12 28781.4992 420 340 0 0 ## X14950 7328.9577 40 14 71953.7480 20800 0 0 0 ## X4830 7069.5583 44 13 3700.4785 350 1000 0 0 ## X2865 10911.3427 65 11 26725.6778 7000 6000 7500 22000 ## X20945 6415.1554 35 17 54479.2663 1200 9310 0 0 ## X13040 5263.6488 40 13 66814.1946 0 380 0 0 ## X4515 5360.7266 33 11 28781.4992 500 0 0 0 ## X145 5696.8902 21 11 513.9553 20 20 0 0 ## X18685 8417.3121 63 13 41116.4274 180 0 0 0 ## X17585 6373.6917 52 17 57562.9984 1000 0 0 0 ## X10090 5114.4060 24 14 28781.4992 0 0 0 0 ## X13235 5454.0787 29 14 9251.1962 2000 20000 0 0 ## X3045 5454.0787 46 14 92511.9617 2500 1500 88000 0 ## X21425 5696.2367 38 12 11307.0175 50 40 0 0 ## X11840 5361.2218 34 13 7400.9569 0 0 0 0 ## X3400 6327.2872 47 12 30837.3206 3500 0 0 0 ## X6635 7173.3284 49 14 37004.7847 3200 2300 0 0 ## X19815 6188.2375 83 12 25697.7671 5100 1800 0 0 ## X19565 5788.8378 50 16 25697.7671 1500 350 0 0 ## X12135 7998.0705 68 16 104846.8899 14600 7100 0 35000 ## X10700 6501.2709 83 14 58590.9091 0 0 330000 275000 ## X2600 7956.8927 28 17 61674.6411 6000 0 20000 0 ## X2860 6604.7905 45 12 20558.2137 1500 530 0 0 ## X2175 4522.0593 57 11 33921.0526 600 3000 0 0 ## X14915 9185.1147 50 12 169605.2632 3500 0 0 0 ## X66351 7173.3284 49 14 37004.7847 3200 2300 0 0 ## X6575 1688.0257 40 14 153158.6922 2000 0 6000 0 ## X8410 6793.3807 29 13 15418.6603 3500 3000 0 0 ## X7230 5859.0521 59 15 15418.6603 2000 1850 200000 0 ## X12955 10373.1531 69 6 12334.9282 1000 0 0 0 ## X19205 7691.5051 44 12 53451.3557 1000 1050 0 0 ## X600 5976.9863 59 12 38032.6954 990 1200 0 0 ## X1290 6655.6238 22 14 15418.6603 250 0 0 0 ## X17070 243.6350 40 16 925119.6172 22000 0 275000 175000 ## X16140 6677.0208 68 14 44200.1595 22000 50000 0 0 ## X17935 9636.8011 52 16 81204.9442 0 0 0 0 ## X3605 5198.3414 39 16 47283.8915 4000 3000 25000 0 ## X10275 5933.3748 43 16 144935.4067 7700 15300 17000 17000 ## X19930 7944.1474 28 13 75037.4801 3200 50 0 500 ## X15360 7421.8016 40 12 123349.2823 1000 6000 50000 20000 ## X1075 7485.5250 77 12 12334.9282 1700 12390 43000 0 ## X7770 9527.8477 78 16 34948.9633 2000 100 0 0 ## X1010 6341.0975 58 8 12334.9282 1500 1200 0 0 ## X7095 4293.7517 37 14 14390.7496 660 0 0 0 ## X14255 7427.1703 78 12 11307.0175 2200 0 0 0 ## X20075 10164.9687 56 14 37004.7847 1000 0 0 0 ## X2610 5551.9820 28 12 28781.4992 270 300 0 0 ## X965 5837.2792 83 12 14390.7496 600 20000 0 0 ## X17515 6220.5890 48 16 41116.4274 2000 5000 0 0 ## X1755 6270.0639 24 9 8223.2855 0 0 0 0 ## X16440 11386.7530 57 13 113070.1754 0 3500 0 2000 ## X14750 7029.1679 29 16 37004.7847 400 100 0 0 ## X16960 8067.4672 36 17 63730.4625 2000 0 40000 20000 ## X575 5111.3136 24 11 24669.8565 0 0 0 0 ## X12340 7216.5318 79 16 25697.7671 7500 0 28000 190000 ## X3250 5516.1522 40 17 12334.9282 300 0 0 0 ## X21805 3597.7161 51 16 81204.9442 5000 0 50000 30000 ## X17860 2751.6615 49 12 8223.2855 14600 500 0 0 ## X6260 3036.6357 44 14 177828.5486 0 0 50000 85000 ## X8435 4689.7790 25 15 40088.5167 160 0 0 0 ## X10795 6313.4185 55 17 87372.4083 1500 96000 27000 0 ## X9785 6018.8547 48 6 35976.8740 2850 0 0 0 ## X17455 8340.9656 40 14 46255.9809 4000 0 0 0 ## X11275 10483.6685 68 12 27753.5885 3300 8000 32000 116000 ## X6785 7596.6439 45 12 56535.0877 500 505 0 0 ## X12920 6468.5210 25 14 4625.5981 0 0 0 0 ## X12685 6937.5423 50 17 50367.6236 1800 1850 0 0 ## X7575 5875.4599 67 14 66814.1946 200 0 0 0 ## X16745 8034.5602 30 14 52423.4450 2000 0 0 0 ## X3925 6698.1550 28 12 37004.7847 0 0 0 0 ## X13715 7485.8803 21 15 11307.0175 800 340 0 0 ## X2630 6623.7739 31 16 149047.0494 2000 2300 0 1700 ## X1880 7673.4807 42 17 12334.9282 220 0 0 0 ## X16810 5375.4516 29 14 38032.6954 20 0 0 0 ## X7535 5532.8460 23 14 25697.7671 1200 1 0 0 ## X17395 4448.8961 36 13 31865.2313 0 0 0 0 ## X20265 4733.4575 40 16 28781.4992 820 400 0 18000 ## X16645 6010.7120 58 13 113070.1754 6030 10000 0 0 ## X18180 4583.3587 52 12 117181.8182 5400 21000 250000 0 ## X4825 5070.4577 38 13 33921.0526 0 0 0 0 ## X1845 8154.7752 78 16 64758.3732 3500 1500 0 26000 ## X5425 10038.8263 40 12 113070.1754 3000 1500 0 8300 ## X10600 8502.3051 68 14 61674.6411 4000 36000 0 0 ## X10360 8298.7768 68 11 10279.1069 0 0 0 0 ## X19890 4456.3079 27 13 16446.5710 750 1550 0 17000 ## X20500 8349.2691 26 10 19530.3030 0 0 0 0 ## X2565 6641.8552 42 16 88400.3190 2000 44000 100000 20000 ## X26002 7956.8927 28 17 61674.6411 6000 0 20000 0 ## X19845 4405.0395 26 12 38032.6954 140 0 0 0 ## X18965 8152.5724 59 12 43172.2488 9500 7700 0 0 ## X11230 3934.7121 28 11 15418.6603 0 0 0 0 ## X11260 7423.0858 75 14 67842.1053 3500 3400 45000 122000 ## X3200 7098.8499 42 12 98679.4258 1600 13040 1000 600 ## X5965 5871.1832 37 16 41116.4274 470 600 1300 0 ## X107953 6313.4185 55 17 87372.4083 1500 96000 27000 0 ## X11035 9078.7938 85 12 20558.2137 0 0 0 0 ## X18245 5659.8661 54 17 91484.0510 7000 8200 0 0 ## X11955 7244.9139 46 12 16446.5710 10000 10000 0 0 ## X9345 6726.9283 92 9 3392.1053 80 0 0 0 ## X2320 6434.5102 49 14 71953.7480 1300 1600 0 0 ## X9295 1158.4185 71 16 65786.2839 3000 400 0 0 ## X20110 5731.2341 48 14 51395.5343 5900 11000 0 0 ## X680 6833.6584 48 16 100735.2472 3800 0 13000 0 ## X13270 7537.6703 37 17 82232.8549 3200 1950 0 0 ## X3075 7190.2136 42 17 40088.5167 200 2150 0 0 ## X13160 9388.0984 42 17 61674.6411 15000 100 0 0 ## X20435 3133.2430 35 17 1182097.2887 0 0 0 375000 ## X12465 2146.5932 52 16 51395.5343 18000 40000 0 600000 ## X4440 4599.0191 60 12 8223.2855 660 100 0 0 ## X3870 7560.2604 63 12 29809.4099 1500 0 0 0 ## X3510 6655.2299 40 12 80177.0335 1000 0 0 0 ## X13795 6664.1853 18 10 7812.1212 0 0 0 0 ## X18155 4538.6282 50 16 61674.6411 500 500 0 0 ## X4685 7123.2132 57 16 51395.5343 1000 30000 0 5700 ## X20135 4921.4820 44 12 29809.4099 470 300 0 0 ## X7975 10857.6915 77 8 15418.6603 0 40000 0 0 ## X16425 6688.8349 53 11 82232.8549 2000 300 0 0 ## X84354 4689.7790 25 15 40088.5167 160 0 0 0 ## X12905 7233.3450 62 2 71953.7480 1000 500 0 2500 ## X15095 7819.0561 86 14 7195.3748 1010 132000 0 0 ## X3625 7581.8314 34 11 6989.7927 0 0 0 0 ## X198455 4405.0395 26 12 38032.6954 140 0 0 0 ## X570 10431.8465 47 12 45228.0702 500 1 0 0 ## X21195 6578.5191 74 16 38032.6954 5200 0 190 50000 ## X16470 3597.7161 43 12 1408237.6396 10 0 0 0 ## X14880 5711.2392 52 12 94567.7831 2150 2610 0 0 ## X9485 8780.1580 55 10 1439.0750 5 400 0 0 ## X17090 5797.9275 33 12 10279.1069 700 0 0 0 ## X9670 11386.7530 45 16 92511.9617 5500 0 0 0 ## X15945 4792.5122 44 12 10279.1069 0 0 0 0 ## X13535 5532.8460 23 10 29809.4099 200 200 0 0 ## X3685 7486.2704 48 10 16446.5710 0 0 0 0 ## X540 6746.5369 45 12 40088.5167 750 180 0 0 ## X17780 6655.8875 51 16 71953.7480 500 8000 0 400 ## X21100 3253.9699 49 14 90456.1404 0 10000 100000 300000 ## X4310 9939.8329 34 17 47283.8915 1500 0 0 0 ## X2010 8301.2131 49 12 38032.6954 500 700 0 0 ## X8785 6388.3726 55 12 48311.8022 2000 0 0 0 ## X1045 7700.3724 56 9 34948.9633 500 5000 0 3000 ## X2935 8045.5847 76 12 24669.8565 1500 0 0 0 ## X11195 7192.3659 21 12 46255.9809 0 0 0 120000 ## X110356 9078.7938 85 12 20558.2137 0 0 0 0 ## X3410 6611.1226 50 16 67842.1053 10 700 0 1 ## X17765 6235.1707 83 15 7812.1212 2000 0 0 0 ## X9175 3265.4868 46 16 71953.7480 1000 1200 10000 0 ## X6395 5644.9880 28 12 26725.6778 0 300 0 5700 ## X485 5154.0603 49 16 19530.3030 0 0 0 0 ## X870 1173.9354 40 16 80177.0335 1640 4100 0 1700 ## X9220 4897.5131 37 9 12334.9282 40 0 0 0 ## X1920 7487.6105 63 12 12334.9282 400 0 0 0 ## X19230 8742.7099 63 15 27753.5885 1850 0 80000 75000 ## X18475 2133.9750 67 17 53451.3557 4000 0 421000 375000 ## X5895 5446.1083 45 7 17474.4817 0 520 0 0 ## X3695 10109.2136 49 17 76065.3908 2000 500 0 0 ## X17075 7726.8088 31 17 63730.4625 0 5000 0 0 ## X21685 6899.7143 37 13 21586.1244 800 0 0 0 ## X10410 5134.3240 25 16 50367.6236 0 300 0 0 ## X1350 5540.6097 31 13 12334.9282 0 0 0 0 ## X18760 5988.3062 43 17 71953.7480 500 0 0 0 ## X3405 5303.8926 27 16 26725.6778 510 15 0 0 ## X12035 5803.8741 35 12 46255.9809 770 0 0 0 ## X305 6313.9774 47 14 68870.0159 2000 16100 5500 0 ## X17850 7666.5600 72 12 185023.9234 3500 0 0 0 ## X4110 1503.1836 38 16 1541866.0287 0 0 1530000 300000 ## X4605 6478.4991 62 12 19530.3030 1800 15000 0 0 ## X12555 4686.2076 25 11 26725.6778 0 0 0 0 ## X5915 3330.3623 54 17 332015.1515 15000 23500 125000 0 ## X22035 4823.1376 58 1 6578.6284 0 0 0 0 ## X6930 5808.7163 31 12 58590.9091 12000 16500 0 5500 ## X17060 10597.7984 80 10 23641.9458 4800 0 0 0 ## X13760 6133.1493 57 12 53451.3557 50 18700 0 0 ## X5825 6661.3144 56 16 31865.2313 1300 0 0 0 ## X34057 5303.8926 27 16 26725.6778 510 15 0 0 ## X20180 8410.7240 61 15 101763.1579 450 0 0 0 ## X21130 11097.5342 78 12 15418.6603 310 0 0 0 ## X12205 4681.8403 46 14 12334.9282 0 0 0 0 ## X1265 9929.1222 77 12 25697.7671 0 0 0 0 ## X13645 10246.9474 81 12 44200.1595 51000 0 0 0 ## X905 7456.1503 23 11 12334.9282 700 0 0 0 ## X21995 5929.3158 83 12 10279.1069 300 6000 0 0 ## X6975 9338.9337 78 16 24669.8565 0 1100 0 0 ## X16450 5872.4153 40 12 22614.0351 2500 120 0 0 ## X14840 5671.0347 80 13 15418.6603 3700 3300 0 0 ## X8300 6136.6248 46 14 52423.4450 1700 6150 0 0 ## X645 2797.2649 52 17 192219.2982 2000 2000 0 0 ## X2770 7022.5454 62 16 47283.8915 2700 2000 0 0 ## X147508 7029.1679 29 16 37004.7847 400 100 0 0 ## X1540 6385.0040 35 11 40088.5167 320 3240 0 0 ## X19435 5019.3357 27 11 5961.8820 0 0 0 0 ## X6765 9419.1196 72 16 30837.3206 400 0 0 0 ## X54259 10038.8263 40 12 113070.1754 3000 1500 0 8300 ## X19980 7630.0979 86 17 37004.7847 10000 20000 0 0 ## X54010 6746.5369 45 12 40088.5167 750 180 0 0 ## X21890 6316.2726 39 12 87372.4083 1000 15650 0 0 ## X1220 8765.8772 76 8 23641.9458 0 18000 0 0 ## X16615 837.3098 46 16 153158.6922 5000 0 0 750000 ## X16905 11386.7530 76 16 28781.4992 5600 6800 48000 100000 ## X9050 1101.0772 46 17 223056.6188 0 0 80000 14000 ## X21165 5386.4622 40 14 71953.7480 0 110000 135000 0 ## X16350 5073.3726 26 11 5653.5088 0 0 0 0 ## X14085 5169.3498 56 14 35976.8740 200 0 0 0 ## X11465 5134.4672 54 12 68870.0159 400 900 0 0 ## X12610 1725.2995 60 14 20558.2137 20000 0 0 0 ## X785 5496.3173 24 16 30837.3206 1000 0 0 0 ## X14485 6354.0137 45 13 17474.4817 1500 360 0 0 ## X8580 7333.3380 40 12 39060.6061 1000 0 0 0 ## X10340 6355.6933 25 14 67842.1053 500 0 0 0 ## X20855 5483.6654 75 8 6270.2552 200 450 0 0 ## X5420 7143.7855 43 15 58590.9091 50 1960 1500 0 ## X1200 7770.1210 49 12 66814.1946 1200 4900 0 0 ## X13395 6239.6906 29 11 125405.1037 2000 7000 0 6000 ## X10230 7426.1415 49 14 30837.3206 0 10000 0 0 ## X17945 10038.8263 39 17 37004.7847 700 580 0 0 ## X565 6382.7943 47 12 10279.1069 300 0 0 0 ## X18070 7659.5207 88 7 16446.5710 900 2700 0 2000 ## X509511 7620.1135 74 12 48311.8022 3500 0 0 0 ## X8940 5120.7298 43 12 54479.2663 3500 15000 0 0 ## X11575 9604.9903 66 17 43172.2488 1400 0 0 3600 ## X1213512 7998.0705 68 16 104846.8899 14600 7100 0 35000 ## X14770 8130.4052 44 14 96623.6045 1800 3800 0 0 ## X22015 6105.5579 54 14 14390.7496 1000 1300 0 0 ## X4965 5025.3417 49 10 0.0000 0 0 0 0 ## X1660 8149.6942 44 13 62702.5518 2500 6000 0 0 ## X20795 6880.3630 19 13 19530.3030 470 40 0 4000 ## X2045 7719.5388 21 13 35976.8740 2000 0 0 0 ## X10235 9916.6980 85 16 24669.8565 1 0 0 180000 ## X12060 7335.8692 23 12 22614.0351 100 120 0 0 ## X5680 8288.4412 57 14 129516.7464 4900 5000 0 0 ## X20215 5133.1927 52 12 19530.3030 2250 410 0 0 ## X15375 7898.4771 80 6 17474.4817 0 2500 0 0 ## X10740 9507.0043 78 5 7709.3301 0 0 0 0 ## X4160 4372.7256 68 16 76065.3908 7000 0 163000 112000 ## X310 5950.2488 40 12 21586.1244 2000 500 0 0 ## X3235 6509.0382 57 12 51395.5343 700 2200 0 0 ## X21055 8250.0749 67 12 12334.9282 10220 0 0 0 ## X2620 5284.0466 28 12 0.0000 0 0 0 0 ## X1600 4660.6242 37 17 75037.4801 2500 220 0 0 ## X1751513 6220.5890 48 16 41116.4274 2000 5000 0 0 ## X5765 6225.7422 59 14 41116.4274 0 0 0 0 ## X16945 6440.1730 79 12 39060.6061 27000 15000 0 0 ## X20830 236.7943 57 17 429666.6667 0 0 150000 0 ## X10105 10483.6685 65 17 35976.8740 0 0 0 9000 ## X4895 8641.0258 36 12 30837.3206 400 700 0 0 ## X9895 4920.1955 55 14 14390.7496 0 0 0 0 ## X10650 7902.0620 48 12 19530.3030 100 0 0 0 ## X8705 6661.6101 54 12 85316.5869 2500 500 0 0 ## X1490 7291.4425 85 12 82232.8549 16000 0 0 0 ## X341014 6611.1226 50 16 67842.1053 10 700 0 1 ## X1408515 5169.3498 56 14 35976.8740 200 0 0 0 ## X16235 7640.4959 28 13 106902.7113 1200 2420 0 0 ## X2201516 6105.5579 54 14 14390.7496 1000 1300 0 0 ## X17115 6487.3485 43 15 57562.9984 2500 4600 0 1000 ## X22110 8414.4992 38 12 56535.0877 1500 1750 0 0 ## X5075 8472.5536 78 11 75037.4801 8600 950 0 0 ## X3895 6208.4334 54 16 83260.7655 16300 0 62700 56000 ## X18550 6467.8824 41 14 152130.7815 150 0 0 0 ## X1998017 7630.0979 86 17 37004.7847 10000 20000 0 0 ## X10815 5992.9396 56 12 28781.4992 900 560 0 0 ## X130 6832.8261 50 17 64758.3732 1100 14700 0 0 ## X15700 9210.3635 42 17 115125.9968 5600 1900 0 0 ## X10560 7091.0393 78 6 8223.2855 660 0 0 0 ## X8180 7339.2602 55 12 15418.6603 0 0 0 0 ## X6115 7434.1715 55 13 61674.6411 2000 1000 0 0 ## X11495 9240.9040 44 16 81204.9442 5700 2200 0 8800 ## X17710 8507.0410 77 9 23641.9458 13000 0 10000 80000 ## X10510 7534.6363 19 12 5653.5088 0 0 0 0 ## X10990 5651.0074 45 11 35976.8740 2400 0 0 1200 ## X13300 5711.5575 59 12 16446.5710 100 70 0 0 ## X19315 6143.5717 37 12 25697.7671 1600 5 0 0 ## X10685 7444.5161 45 12 23641.9458 0 0 0 0 ## X19330 6984.1467 48 14 71953.7480 2000 5600 0 15000 ## X16260 6003.7896 46 10 6167.4641 0 10 0 0 ## X13945 9197.4307 35 9 34948.9633 1500 5100 0 0 ## X2330 6659.7322 46 17 20558.2137 1000 400 6000 40000 ## X12080 5664.1469 20 14 21586.1244 700 0 0 0 ## X16900 3237.7455 69 17 415275.9171 234400 0 0 1000000 ## X1080 11386.7530 31 17 55507.1770 810 45000 0 0 ## X19180 2812.2327 53 16 87372.4083 0 80350 236000 20000 ## X2925 6746.5369 38 16 149047.0494 2000 15000 0 0 ## X7555 7486.2469 62 12 35976.8740 7500 0 42000 0 ## X16600 8180.4213 57 17 166521.5311 3000 2700 0 0 ## X16795 7270.1531 37 12 65786.2839 400 50 0 0 ## X16545 7485.1324 60 12 44200.1595 2000 1500 0 0 ## X20245 6984.7124 25 12 56535.0877 2000 0 0 0 ## X9180 6503.6143 32 17 87372.4083 0 5500 4000 50000 ## X16480 9494.7847 76 14 69897.9266 1500 0 173000 100000 ## X17355 11253.9904 57 12 66814.1946 1610 400 0 0 ## X5875 7786.5650 80 10 8326.0766 6000 1000 0 0 ## X16145 7168.1489 45 14 50367.6236 0 3000 0 2500 ## X21770 5079.5219 62 8 7298.1659 0 0 0 0 ## X11820 5689.9210 38 16 69897.9266 2501 1200 48000 0 ## X9390 7393.9024 65 16 14390.7496 1000 3000 2000 2000 ## X12520 6332.1609 67 10 14390.7496 6000 0 0 0 ## X12040 11386.7530 76 16 37004.7847 2500 11000 0 38000 ## X8890 10431.8465 50 11 114098.0861 15000 0 0 8300 ## X150 6767.4428 29 13 47283.8915 100 0 0 7000 ## X4600 7132.4826 66 7 34948.9633 1500 800 0 0 ## X12490 7344.0563 70 12 13362.8389 40 0 0 0 ## X5640 8473.5305 43 12 17474.4817 0 1700 0 0 ## X1758518 6373.6917 52 17 57562.9984 1000 0 0 0 ## X3105 1720.8497 54 17 444057.4163 0 0 3900000 100000 ## X1070019 6501.2709 83 14 58590.9091 0 0 330000 275000 ## X7035 8463.5537 49 16 54479.2663 0 0 0 0 ## X19950 4181.7756 53 16 243614.8325 8300 3010 0 2300 ## X12835 4692.9691 39 12 40088.5167 1500 15 0 0 ## X12050 5651.0074 48 12 67842.1053 2500 2800 0 0 ## X12605 7292.5295 48 14 116153.9075 2800 410 0 0 ## X16605 10121.8582 74 14 67842.1053 6500 0 0 0 ## X100 6078.5087 65 9 7914.9123 0 0 0 0 ## X21095 6777.8412 51 14 76065.3908 600 0 0 0 ## X9640 8069.9291 52 12 35976.8740 400 0 0 0 ## X18820 1099.8573 50 17 248754.3860 237500 250000 0 2200 ## X20565 11386.7530 40 17 193247.2089 5000 1000 0 23000 ## X13035 8220.9304 83 14 81204.9442 7000 193000 0 0 ## X15555 307.3822 48 16 176800.6380 5200 1000 650000 0 ## X8315 6657.7220 69 16 51395.5343 1000 115000 0 10000 ## X990 6273.6366 28 11 69897.9266 3000 0 0 0 ## X19305 4792.5614 47 17 93539.8724 5400 0 0 15000 ## X14165 8909.1588 54 14 69897.9266 2500 0 28000 62000 ## X1785 9471.2816 63 12 21586.1244 2300 0 0 0 ## X22045 3394.2432 68 12 80177.0335 2000 15000 0 25000 ## X31520 5870.6340 40 12 42144.3381 400 0 0 0 ## X21535 8618.8325 66 12 9765.1515 550 5600 0 0 ## X8005 7407.0569 45 17 51395.5343 8000 0 0 0 ## X21855 5619.8455 28 14 56535.0877 1600 1430 0 0 ## X2965 8016.8139 54 16 92511.9617 1100 400 11000 10000 ## X19925 8390.7838 38 12 35976.8740 3500 77000 63000 0 ## X21305 7544.4668 35 5 10279.1069 0 0 0 0 ## X11315 11291.1815 63 17 75037.4801 7200 6200 0 45000 ## X2870 6938.1564 37 8 16446.5710 0 200 0 0 ## X7845 5128.3639 24 9 37004.7847 200 0 0 0 ## X11325 9377.8584 41 13 80177.0335 200 2200 0 300 ## X7135 6633.8745 24 14 32893.1419 1700 0 600 0 ## X1223521 5988.0417 26 16 61674.6411 3000 0 0 0 ## X19955 5867.5632 55 12 43172.2488 7000 0 62000 0 ## X12115 4344.4525 36 13 25697.7671 0 2000 0 0 ## X20770 4390.4068 26 10 10279.1069 0 0 0 0 ## X695 7345.0840 45 12 39060.6061 0 0 0 0 ## X11320 10038.8263 41 16 128488.8357 5000 0 0 20000 ## X7080 6071.8608 61 12 27753.5885 1300 0 330000 90000 ## X21705 6325.1681 42 12 20558.2137 0 0 0 0 ## X10855 7418.4265 50 14 102791.0686 5300 1700 0 0 ## X6340 7390.7142 46 16 82232.8549 2000 100 0 0 ## X17450 7513.2426 57 12 11307.0175 0 0 0 0 ## X2895 8886.6097 83 12 13362.8389 1 1000 0 0 ## X8115 8155.4622 78 17 98679.4258 7000 0 140000 0 ## X430 4603.5133 35 14 88400.3190 810 6000 20000 0 ## X1027522 5933.3748 43 16 144935.4067 7700 15300 17000 17000 ## X387023 7560.2604 63 12 29809.4099 1500 0 0 0 ## X13920 6330.8427 47 12 24669.8565 700 0 0 0 ## X3160 4384.6392 46 13 33921.0526 300 0 0 0 ## X19125 3276.5295 59 13 154186.6029 2700 0 0 10000 ## X21480 1832.4461 60 14 431722.4880 220000 0 990000 156000 ## X13180 4989.6261 47 12 32893.1419 750 0 0 0 ## X7970 9005.8034 88 10 16446.5710 11000 0 0 0 ## X11435 9493.2555 52 15 100735.2472 0 4100 0 0 ## X16800 6750.1669 23 13 15418.6603 200 0 0 0 ## X5575 6212.7090 86 12 18502.3923 100 0 0 0 ## X9880 4017.0656 33 12 29809.4099 700 0 0 0 ## X13440 5195.9143 53 8 9765.1515 10 0 0 0 ## X17370 5668.8620 46 12 32893.1419 600 0 0 0 ## X17200 10178.6915 76 12 26725.6778 1000 5000 0 0 ## X3905 6746.5369 41 17 183996.0128 15800 18000 0 60000 ## X14000 7172.7607 29 12 4214.4338 0 0 0 0 ## X9710 5155.4648 54 12 15418.6603 1600 0 0 0 ## X5300 6574.6565 76 12 42144.3381 8500 5200 86000 100000 ## X12985 8867.1917 40 12 24669.8565 1200 0 0 0 ## X2007524 10164.9687 56 14 37004.7847 1000 0 0 0 ## X15575 6836.0441 39 9 32893.1419 2000 0 0 0 ## X4245 8696.1124 67 12 20558.2137 400 0 0 0 ## X21505 5686.0635 43 12 29809.4099 0 0 0 0 ## X4215 7936.4547 22 15 8223.2855 0 30 0 0 ## X12535 11794.4073 78 12 14390.7496 700 0 0 0 ## X16475 9521.9200 75 10 10279.1069 6800 0 0 0 ## X4570 6836.3888 53 13 10279.1069 50 0 0 0 ## X15300 8124.9062 45 12 10279.1069 0 0 0 0 ## X18200 6704.2828 40 12 121293.4609 810 1400 0 50000 ## X2325 7803.2247 45 15 113070.1754 4900 2000 12000 1000 ## X3430 4850.2392 61 2 35976.8740 0 1000 0 0 ## X7495 6984.7124 32 10 12334.9282 300 0 0 0 ## X489525 8641.0258 36 12 30837.3206 400 700 0 0 ## X1680026 6750.1669 23 13 15418.6603 200 0 0 0 ## X21375 7709.7569 42 14 101763.1579 1000 4201 1400 170000 ## X11115 6525.0952 64 9 21586.1244 800 0 0 0 ## X5220 5976.1950 26 16 30837.3206 6000 0 0 1000 ## X1488027 5711.2392 52 12 94567.7831 2150 2610 0 0 ## X21940 6131.7347 46 12 167549.4418 2530 0 7000 0 ## X1364528 10246.9474 81 12 44200.1595 51000 0 0 0 ## X21040 5845.6749 25 16 25697.7671 50 0 0 0 ## X7125 6563.5801 49 13 31865.2313 0 600 0 0 ## X8670 5196.5893 21 12 7195.3748 400 110 0 0 ## X10640 6599.9746 47 12 136712.1212 1500 0 0 0 ## X18375 3907.1193 26 12 29809.4099 1000 2500 0 0 ## X20845 7247.0973 25 12 35976.8740 2400 205 0 2500 ## X595 6578.6259 51 17 162409.8884 5000 27000 0 3000 ## X1455 5851.6456 43 16 103818.9793 1000 0 8000 0 ## X8760 4733.7759 81 4 4625.5981 0 0 0 0 ## X626029 3036.6357 44 14 177828.5486 0 0 50000 85000 ## X8475 11291.1815 53 17 137740.0319 0 2000 350000 150000 ## X3085 6093.4654 30 14 54479.2663 300 2000 15000 0 ## X9285 3597.7161 67 16 129516.7464 4000 70000 50000 300000 ## X6940 7270.8124 49 13 48311.8022 1200 180 0 0 ## X557530 6212.7090 86 12 18502.3923 100 0 0 0 ## X16710 7779.0121 64 12 6167.4641 4500 0 0 0 ## X15515 5222.0284 20 12 14390.7496 20 0 0 0 ## X3530 4372.7256 48 16 102791.0686 3200 0 24000 0 ## X6860 5487.6011 43 12 52423.4450 0 0 0 0 ## X14630 4457.1985 34 12 31865.2313 1100 80 0 0 ## X14705 9293.5925 76 14 28781.4992 27000 0 0 0 ## X13010 4911.3897 30 12 70925.8373 8800 5000 0 0 ## X1792031 6764.6424 57 12 25697.7671 1600 900 0 0 ## X12705 5259.2512 65 11 25697.7671 2000 5000 0 0 ## X9870 5335.8581 30 12 44200.1595 500 0 0 0 ## X17305 6237.3654 32 15 55507.1770 1150 1500 0 0 ## X21595 7445.0778 82 12 20558.2137 0 2000 0 2000 ## X13725 7519.7320 55 11 27753.5885 0 700 0 4000 ## X10040 7735.4996 88 13 11307.0175 0 0 0 0 ## X7005 5575.5340 52 10 41116.4274 0 1000 0 0 ## X3760 5696.2367 41 12 29809.4099 5000 120 0 0 ## X14910 5933.3748 43 16 308373.2057 4000 100000 202000 0 ## X13365 9269.3339 55 9 6270.2552 0 10 0 0 ## X11410 5960.0579 33 16 119237.6396 2400 0 0 0 ## X12220 2814.9047 36 16 855221.6906 60000 230000 350000 0 ## X18420 7029.1679 31 13 80177.0335 1550 0 0 3000 ## X9005 7842.3749 40 15 21586.1244 0 0 0 0 ## X11855 7333.3380 38 12 12334.9282 200 0 0 7000 ## X21405 3960.9948 55 11 30837.3206 0 0 0 0 ## X21260 9441.6518 64 12 28781.4992 600 0 0 0 ## X8020 4444.7730 55 17 8634.4498 0 500 0 0 ## X10370 7938.8328 34 14 49339.7129 500 550 0 0 ## X15255 7597.5728 73 3 23641.9458 3000 0 0 0 ## X12735 5620.7655 43 12 72981.6587 1500 0 0 0 ## X2635 8359.0939 58 17 57562.9984 2000 6000 0 0 ## X4765 6452.8207 37 14 13362.8389 140 0 0 0 ## X20295 6316.2726 38 16 82232.8549 500 200 3000 12000 ## X4030 5316.4354 44 12 1233.4928 0 0 0 0 ## X21360 9418.0637 31 11 69897.9266 0 2330 0 0 ## X858032 7333.3380 40 12 39060.6061 1000 0 0 0 ## X15240 11386.7530 30 17 125405.1037 2800 800 100000 15000 ## X1052033 4336.5281 25 14 26725.6778 800 1500 0 0 ## X8230 2913.2891 66 16 237447.3684 105000 0 0 375000 ## X6565 6185.1676 45 17 51395.5343 590 5500 22000 0 ## X8210 8582.8214 69 8 21586.1244 0 4000 0 0 ## X16370 7369.8167 55 12 12334.9282 2000 0 0 0 ## X2495 5634.0684 68 13 46255.9809 4500 15000 0 0 ## X4950 9383.2642 78 12 21586.1244 3000 700 0 0 ## X20625 3005.5195 67 17 169605.2632 5000 0 0 0 ## X12640 6976.6009 38 17 85316.5869 3500 5800 0 0 ## X16455 9143.1550 48 16 102791.0686 2000 35000 0 30000 ## X20670 7144.1292 36 13 25697.7671 0 0 0 0 ## X9855 5315.0495 28 11 2569.7767 0 0 0 0 ## X7590 6391.5128 50 12 29809.4099 0 0 0 0 ## X10390 8251.3463 82 1 6887.0016 160 0 0 0 ## X6885 4867.2383 54 17 239503.1898 12700 0 0 0 ## X12630 4832.6968 87 10 35976.8740 1000 95000 190000 0 ## X587534 7786.5650 80 10 8326.0766 6000 1000 0 0 ## X6415 5711.2392 45 16 97651.5152 1050 10000 0 0 ## X13800 5809.2268 72 14 49339.7129 4500 0 184000 0 ## X21210 5990.8378 39 12 64758.3732 1000 2500 0 0 ## X20775 5111.3136 19 14 6887.0016 700 0 0 0 ## X16165 6421.3632 33 16 65786.2839 3500 0 0 1200 ## X249535 5634.0684 68 13 46255.9809 4500 15000 0 0 ## X18530 5463.0804 57 17 76065.3908 2500 36000 0 0 ## X1182036 5689.9210 38 16 69897.9266 2501 1200 48000 0 ## X2485 2708.6805 49 17 61674.6411 400 3000 0 65000 ## X16785 10609.3570 74 12 15418.6603 1900 0 0 0 ## X11750 6777.0313 67 17 104846.8899 10000 22000 3000 0 ## X3025 6021.0394 36 14 41116.4274 800 0 0 3500 ## X3470 6106.0776 50 12 59618.8198 3000 13900 0 0 ## X1860 7640.0210 43 14 97651.5152 4200 1000 0 0 ## X3920 5952.7513 28 15 10279.1069 800 0 0 0 ## X19430 8253.7779 46 12 19530.3030 500 0 0 0 ## X16535 9276.8733 39 13 26725.6778 200 0 0 200 ## X13620 7102.5441 50 13 32893.1419 1500 5000 0 0 ## X17880 7448.2342 50 14 67842.1053 1700 0 0 0 ## X4875 8104.8327 47 15 82232.8549 2000 0 0 50000 ## X19300 8951.3784 49 12 7195.3748 0 1500 0 0 ## X7075 4214.2722 27 17 32893.1419 400 0 0 0 ## X15130 1258.0767 58 17 145963.3174 10000 100000 1350000 500000 ## X13555 7031.0802 53 17 204554.2265 35000 0 0 0 ## X8385 5165.9872 42 12 98679.4258 200 200 0 0 ## X831537 6657.7220 69 16 51395.5343 1000 115000 0 10000 ## X1330 6038.9240 21 12 55507.1770 200 550 0 2300 ## X6710 6046.9947 44 14 77093.3014 2000 2000 0 0 ## X6055 3727.8709 55 16 236419.4577 0 0 0 0 ## X20455 2617.2982 46 12 263145.1356 8000 11000 405000 60000 ## X2025 1782.7152 44 16 211749.6013 7000 1500 200000 70000 ## X8485 8332.8960 69 12 24669.8565 0 0 0 0 ## X6475 6829.6252 30 14 42144.3381 1000 0 0 0 ## X4305 3570.0930 59 14 74009.5694 14000 0 296000 0 ## X6900 9220.5450 71 16 51395.5343 15000 14000 95000 153000 ## X14525 3787.5754 39 17 290898.7241 9000 900 0 300000 ## X3070 5948.8229 51 16 34948.9633 1000 2620 0 0 ## X811538 8155.4622 78 17 98679.4258 7000 0 140000 0 ## X1420 5491.3648 58 16 58590.9091 7000 10000 0 0 ## X542039 7143.7855 43 15 58590.9091 50 1960 1500 0 ## X18380 3800.5231 81 9 101763.1579 36100 1700 135000 100000 ## X4185 9295.7487 68 16 34948.9633 630 0 0 0 ## X13830 6275.9318 41 14 5139.5534 0 20 10000 0 ## X6590 8483.7784 51 12 88400.3190 480 3050 0 0 ## X13340 4841.2957 41 8 32893.1419 1500 0 0 0 ## X5625 673.0648 35 17 170633.1738 6000 0 7000 0 ## X9625 8567.4103 58 14 93539.8724 4100 6000 0 150000 ## X12020 7758.8189 75 9 17474.4817 3000 0 0 0 ## X9580 3597.7161 53 16 170633.1738 10000 30 500000 0 ## X277040 7022.5454 62 16 47283.8915 2700 2000 0 0 ## X50 4662.1601 32 14 79149.1228 0 2000 0 0 ## X369541 10109.2136 49 17 76065.3908 2000 500 0 0 ## X7540 5516.1131 45 13 22614.0351 300 0 0 0 ## X1030 10556.2567 77 12 29809.4099 1000 1000 0 0 ## X14400 8454.0719 72 10 4008.8517 0 0 0 0 ## X7415 3639.7242 41 14 29809.4099 0 0 0 2500 ## X3990 6367.9364 65 5 12334.9282 0 0 0 0 ## X3245 6936.3230 63 12 53451.3557 3300 0 0 60000 ## X2575 5576.2209 67 17 65786.2839 3000 200 4000 0 ## X9105 7529.2122 60 12 20558.2137 1500 0 0 0 ## X7985 5978.3895 26 14 89428.2297 2000 0 0 0 ## X1300 8554.5639 79 14 31865.2313 810 5100 0 17000 ## X4760 4867.2383 39 17 78121.2121 400 0 0 116000 ## X16305 6168.6436 22 12 33921.0526 60 0 0 0 ## X21035 6648.1001 48 14 45228.0702 0 7500 30000 0 ## X2905 5929.3158 75 13 9251.1962 3000 30000 0 0 ## X1610 8138.5659 72 12 25697.7671 11000 8500 0 0 ## X3490 5496.9282 56 12 15418.6603 4000 500 0 0 ## X16585 9117.2509 21 12 28781.4992 50 0 0 0 ## X4145 9942.5215 66 10 25697.7671 0 0 0 0 ## X3135 7973.0718 35 12 61674.6411 0 0 0 0 ## X6000 5336.4730 25 16 20558.2137 1500 3000 20000 0 ## X10420 5478.6751 39 12 51395.5343 0 0 0 0 ## X1655 5586.1606 58 14 34948.9633 0 0 0 0 ## X10705 7409.4581 48 7 12334.9282 4000 0 0 0 ## X11735 6187.6895 49 16 122321.3716 20000 0 71000 0 ## X6720 5215.7507 22 12 18502.3923 1200 500 0 0 ## X12680 8925.5408 27 12 37004.7847 2150 500 0 1000 ## X7530 10000.0270 66 12 25697.7671 1200 6000 0 0 ## X7795 6111.3967 41 13 40088.5167 120 1 0 0 ## X1480 5140.6393 49 16 29809.4099 1800 21600 0 0 ## X21575 6161.6719 43 2 68870.0159 1740 10500 0 0 ## X2585 9636.8011 58 10 83260.7655 8000 0 0 0 ## X16595 7065.6086 52 6 5550.7177 0 0 0 0 ## X4040 7888.3857 68 7 14390.7496 100 0 0 0 ## X1630542 6168.6436 22 12 33921.0526 60 0 0 0 ## X16330 6551.0792 25 16 71953.7480 5000 2740 0 0 ## X15665 8384.9108 36 17 12334.9282 2500 26000 0 21000 ## X690043 9220.5450 71 16 51395.5343 15000 14000 95000 153000 ## X6795 8112.0825 43 16 121293.4609 300 100 0 0 ## X21350 9864.7351 68 12 24669.8565 1200 200 0 0 ## X6700 1503.1836 66 16 289870.8134 0 0 275000 88000 ## X18665 6813.5822 45 14 53451.3557 0 5000 0 0 ## X19580 8548.9432 29 14 98679.4258 32800 20000 0 20000 ## X20130 8003.3654 32 14 45228.0702 500 1700 10000 2500 ## X10325 4593.4824 25 15 23641.9458 2300 3420 0 0 ## X4130 3284.9326 35 16 122321.3716 6000 5000 110000 0 ## X5475 6092.8720 57 3 6373.0463 3290 9200 0 0 ## X1790 7500.1937 66 12 67842.1053 1300 0 0 0 ## X17480 8048.4219 41 16 60646.7305 1000 3850 0 0 ## X12830 5932.7371 47 12 55507.1770 1100 41000 0 0 ## X1865 6282.7566 51 9 62702.5518 3500 0 0 0 ## X768044 7263.7921 52 12 35976.8740 1700 3000 2000 0 ## X457045 6836.3888 53 13 10279.1069 50 0 0 0 ## X11490 8507.8199 88 12 5756.2998 2620 12400 0 5000 ## X1146546 5134.4672 54 12 68870.0159 400 900 0 0 ## X10180 5948.7354 23 16 20558.2137 2000 100 0 1000 ## X3910 5973.1357 35 15 44200.1595 3000 55000 550000 0 ## X11565 11386.7530 49 16 100735.2472 2500 450 0 4000 ## X21825 6245.2759 24 14 30837.3206 1500 0 0 0 ## X4525 4297.7367 42 16 17474.4817 1000 0 0 0 ## X3060 8414.8911 76 12 14390.7496 900 0 0 0 ## X9250 7609.5092 61 10 25697.7671 1000 1100 0 0 ## X17500 5861.6929 56 12 53451.3557 21000 20800 0 0 ## X1100 5302.7948 40 12 35976.8740 0 0 0 0 ## X16025 7138.9429 42 12 56535.0877 1200 1251 0 0 ## X12380 5632.2290 40 16 66814.1946 1000 1500 0 0 ## X753547 5532.8460 23 14 25697.7671 1200 1 0 0 ## X12850 4689.7790 33 16 61674.6411 3500 5600 0 0 ## X1955548 8909.1588 47 14 25697.7671 17320 730 0 0 ## X19775 6366.6587 81 12 9353.9872 750 2000 0 0 ## X11525 4683.3579 52 10 8017.7033 390 0 0 0 ## X2975 9116.0082 72 12 24669.8565 100 500 0 0 ## X18895 10098.3165 23 12 18502.3923 2600 0 0 0 ## X1602549 7138.9429 42 12 56535.0877 1200 1251 0 0 ## X345 6272.7234 44 9 12334.9282 9200 0 0 0 ## X490 6976.6009 44 16 116153.9075 3000 0 0 0 ## X14580 1435.8448 50 16 488257.5758 10000 0 10000000 8000000 ## X10875 7706.5913 41 17 63730.4625 2000 11500 0 0 ## X5270 5113.1426 32 16 21586.1244 40 0 0 0 ## X9400 8294.9122 57 14 24669.8565 0 230 0 0 ## X12900 8693.4985 65 15 27753.5885 7200 3200 0 0 ## X4530 8281.4292 79 12 14390.7496 0 35000 0 0 ## X17670 6587.6319 42 8 30837.3206 0 0 0 0 ## X5440 7096.7500 34 14 54479.2663 300 30 0 0 ## X8875 4908.0300 36 12 62702.5518 3000 1030 0 0 ## X2060 10858.1982 85 11 22614.0351 340 3500 0 0 ## X2153550 8618.8325 66 12 9765.1515 550 5600 0 0 ## X5080 6983.5455 49 12 51395.5343 1500 0 0 0 ## X12500 7868.5326 45 16 68870.0159 1000 10000 300 22000 ## X830 5696.6397 28 14 8634.4498 100 0 0 0 ## X495051 9383.2642 78 12 21586.1244 3000 700 0 0 ## X1304052 5263.6488 40 13 66814.1946 0 380 0 0 ## X16685 6956.0287 24 12 30837.3206 300 200 0 0 ## X5695 6842.0098 82 16 17474.4817 1900 0 0 0 ## X2026553 4733.4575 40 16 28781.4992 820 400 0 18000 ## X215 6221.1912 53 17 74009.5694 11000 0 0 25000 ## X7460 9759.8671 45 12 43172.2488 3200 600 5000 0 ## X21060 7675.9421 92 16 9251.1962 2500 18000 0 0 ## X3770 6271.7559 61 12 56535.0877 20000 7000 35000 0 ## X940054 8294.9122 57 14 24669.8565 0 230 0 0 ## X15320 7356.9559 61 14 11307.0175 600 0 0 0 ## X96555 5837.2792 83 12 14390.7496 600 20000 0 0 ## X19340 9125.7408 78 8 18502.3923 0 0 0 0 ## X1395 6579.0643 47 16 35976.8740 1300 500 0 0 ## X939056 7393.9024 65 16 14390.7496 1000 3000 2000 2000 ## X5245 8229.2596 61 6 11307.0175 430 0 0 0 ## X18830 9253.5661 45 14 22614.0351 450 600 0 0 ## X15215 6956.5608 39 16 66814.1946 2000 500 15000 0 ## X496557 5025.3417 49 10 0.0000 0 0 0 0 ## X12210 7636.2540 35 12 24669.8565 0 22000 0 0 ## X17560 9704.9669 66 12 66814.1946 2500 5000 0 0 ## X19625 8823.5948 63 1 17474.4817 4000 14700 0 0 ## X2530 7480.5053 41 12 20558.2137 1000 0 46000 500 ## X9075 6474.0924 41 12 66814.1946 1000 1000 0 0 ## X1925 8462.7026 88 11 11307.0175 0 9900 0 0 ## X21010 7610.4252 28 16 59618.8198 0 23000 50000 0 ## X1745058 7513.2426 57 12 11307.0175 0 0 0 0 ## X17555 2695.1985 39 13 42144.3381 700 0 0 0 ## X2018059 8410.7240 61 15 101763.1579 450 0 0 0 ## X5330 2017.1634 77 16 117181.8182 7000 0 0 100000 ## X2150560 5686.0635 43 12 29809.4099 0 0 0 0 ## X2970 4411.1662 56 12 13362.8389 0 2100 0 0 ## X19190 9186.7808 35 17 68870.0159 200 1700 0 0 ## X12570 8712.1533 39 17 23641.9458 1200 0 0 0 ## X1325 6437.8045 66 12 32893.1419 900 0 200 0 ## X4195 6522.8860 33 14 81204.9442 5000 3000 0 0 ## X20915 2494.8521 45 12 61674.6411 3000 0 0 0 ## X14145 6934.1103 46 12 77093.3014 87800 8400 19600 1000 ## X13090 5132.5279 52 16 33921.0526 300 0 0 0 ## X2211061 8414.4992 38 12 56535.0877 1500 1750 0 0 ## X13062 6832.8261 50 17 64758.3732 1100 14700 0 0 ## X7190 5867.3346 59 12 4522.8070 0 10 0 0 ## X10690 8483.6259 79 9 12334.9282 200 0 0 0 ## X21495 5951.9803 29 11 25697.7671 2000 0 0 0 ## X3745 5958.4027 47 13 87372.4083 1000 370 0 0 ## X2315 6840.0838 22 12 18502.3923 0 10 0 0 ## X3170 6539.6647 51 3 10279.1069 160 0 0 0 ## X10940 9740.7156 76 14 46255.9809 3200 1700 103000 20000 ## X2116563 5386.4622 40 14 71953.7480 0 110000 135000 0 ## X2109564 6777.8412 51 14 76065.3908 600 0 0 0 ## X233065 6659.7322 46 17 20558.2137 1000 400 6000 40000 ## X17530 5609.5420 38 16 31865.2313 1000 0 34000 0 ## X12410 1832.4461 66 16 430694.5773 4500 0 1220000 0 ## X1694566 6440.1730 79 12 39060.6061 27000 15000 0 0 ## X1250 7269.0062 41 12 52423.4450 4270 6010 0 0 ## X1323567 5454.0787 29 14 9251.1962 2000 20000 0 0 ## X17190 11386.7530 42 12 207637.9585 2100 1100 6200 0 ## X103068 10556.2567 77 12 29809.4099 1000 1000 0 0 ## X1018069 5948.7354 23 16 20558.2137 2000 100 0 1000 ## X18295 10394.6521 53 11 16446.5710 100 200 0 0 ## X8770 10597.7984 76 16 18502.3923 11300 0 0 0 ## X585 8414.4992 38 12 82232.8549 2100 3000 0 0 ## X8750 3316.2325 35 17 105874.8006 0 0 0 0 ## X13955 5457.0780 63 9 16446.5710 100 7800 0 0 ## X18825 216.1459 60 16 256977.6714 1000 0 500000 5000000 ## X12280 7685.7706 42 16 75037.4801 0 0 0 0 ## X21780 5711.2392 53 14 56535.0877 1000 0 0 0 ## X17810 6746.5369 62 17 105874.8006 4000 14000 0 15000 ## X2535 5622.6830 62 16 64758.3732 1500 12000 0 0 ## X1614070 6677.0208 68 14 44200.1595 22000 50000 0 0 ## X17290 6934.7265 60 14 22614.0351 1500 3000 0 0 ## X15400 1832.4461 59 12 325847.6874 1500 56000 17000 18000 ## X1842071 7029.1679 31 13 80177.0335 1550 0 0 3000 ## X1650 7512.8658 52 13 80177.0335 1000 0 0 0 ## X2050072 8349.2691 26 10 19530.3030 0 0 0 0 ## X360 9617.6686 29 11 17474.4817 0 100 0 0 ## X1895 9047.5141 41 17 118209.7289 2550 2200 35000 95000 ## X7350 5915.7400 40 12 89428.2297 20 0 0 0 ## X107573 7485.5250 77 12 12334.9282 1700 12390 43000 0 ## X2204574 3394.2432 68 12 80177.0335 2000 15000 0 25000 ## X10345 6435.6192 41 9 18502.3923 20 0 0 0 ## X90575 7456.1503 23 11 12334.9282 700 0 0 0 ## X75 5826.0300 41 16 135684.2105 3000 2000 0 3000 ## X1550 8535.9389 32 14 72981.6587 1050 310 0 0 ## X6040 6911.1151 50 12 27753.5885 500 0 0 0 ## X3525 11291.1815 75 17 104846.8899 5000 0 0 0 ## X6980 6885.7776 65 12 69897.9266 590 8450 0 900 ## X178576 9471.2816 63 12 21586.1244 2300 0 0 0 ## X16935 6093.3713 22 12 24669.8565 8000 0 0 0 ## X928577 3597.7161 67 16 129516.7464 4000 70000 50000 300000 ## X9615 6226.2105 42 15 19530.3030 0 0 0 0 ## X17940 7647.2525 21 12 33921.0526 280 500 0 0 ## X2855 5717.5206 66 12 9148.4051 0 0 0 0 ## X17900 8173.5225 56 6 27753.5885 590 0 0 0 ## X1222078 2814.9047 36 16 855221.6906 60000 230000 350000 0 ## X2020 11188.2330 75 10 12334.9282 1050 4000 0 0 ## X10380 6301.9789 24 15 3083.7321 0 0 0 0 ## X4000 9404.8200 56 12 32893.1419 2000 0 19200 0 ## X90 7762.3371 63 12 49339.7129 500 3000 0 400 ## X19145 6279.5012 46 17 88400.3190 1000 1500 69600 0 ## X9140 4396.2365 59 15 0.0000 1600 0 0 0 ## X2300 5313.0456 29 11 15418.6603 30 5 0 0 ## X13560 4701.5738 50 12 18502.3923 600 130 0 0 ## X1767079 6587.6319 42 8 30837.3206 0 0 0 0 ## X9665 8183.9394 41 17 141851.6746 230 590 200 14000 ## X9065 6428.1923 62 14 28781.4992 500 4500 0 0 ## X12715 5000.2165 52 12 87372.4083 380 0 0 0 ## X1069080 8483.6259 79 9 12334.9282 200 0 0 0 ## X15150 5438.2853 42 15 41116.4274 0 48000 0 0 ## X14780 3597.7161 71 16 121293.4609 27000 12000 900000 1000000 ## X3080 5431.6661 46 16 131572.5678 1410 3340 0 0 ## X1023081 7426.1415 49 14 30837.3206 0 10000 0 0 ## X9725 8641.0258 42 12 107930.6220 2500 3700 0 0 ## X1330082 5711.5575 59 12 16446.5710 100 70 0 0 ## X3215 11386.7530 41 16 136712.1212 4000 103000 18000 70000 ## X1069083 8483.6259 79 9 12334.9282 200 0 0 0 ## X19635 4799.3712 49 9 10279.1069 0 0 0 0 ## X14800 6505.3275 35 12 82232.8549 3000 0 0 0 ## X2105584 8250.0749 67 12 12334.9282 10220 0 0 0 ## X21380 4683.3579 51 6 11307.0175 0 0 0 0 ## X2024585 6984.7124 25 12 56535.0877 2000 0 0 0 ## X11040 6972.1301 33 16 34948.9633 300 1050 0 800 ## X12070 8012.1578 73 8 18502.3923 920 0 0 0 ## X3465 9199.2632 30 16 141851.6746 6000 510 13000 23000 ## X20725 9942.5215 90 10 6167.4641 0 0 0 0 ## X15730 8244.9929 29 12 113070.1754 100 30000 0 0 ## X17005 1503.1836 73 17 154186.6029 2500 3000 315000 18000 ## X4065 6197.2818 59 13 34948.9633 2200 0 0 0 ## X620 6750.9058 56 12 18502.3923 300 0 0 0 ## X1776586 6235.1707 83 15 7812.1212 2000 0 0 0 ## X2715 8028.6114 32 12 57562.9984 0 0 0 1000 ## X5210 9050.9752 48 14 46255.9809 800 1000 5000 0 ## X1303587 8220.9304 83 14 81204.9442 7000 193000 0 0 ## X18625 6316.2726 43 14 49339.7129 700 3310 30000 0 ## X20460 4841.8475 25 12 28781.4992 0 300 0 0 ## X4700 6088.9010 49 16 30837.3206 2000 0 0 0 ## X256588 6641.8552 42 16 88400.3190 2000 44000 100000 20000 ## X4180 146.7205 49 16 678421.0526 0 10000 1000000 0 ## X5760 7480.3327 51 16 178856.4593 3750 0 0 0 ## X286589 10911.3427 65 11 26725.6778 7000 6000 7500 22000 ## X5745 5771.2818 34 12 18502.3923 2700 0 0 0 ## X5175 5026.4489 36 11 19530.3030 0 300 0 0 ## X15105 6017.3025 48 8 26725.6778 10 4000 0 0 ## X19895 5145.1112 58 12 5139.5534 310 0 0 0 ## X1210 6281.8256 51 16 140823.7640 11200 28000 0 0 ## X1998090 7630.0979 86 17 37004.7847 10000 20000 0 0 ## X202091 11188.2330 75 10 12334.9282 1050 4000 0 0 ## X14570 6568.5137 33 9 42144.3381 1000 1700 9000 0 ## X2100 9341.9182 47 12 35976.8740 480 0 0 0 ## X1208092 5664.1469 20 14 21586.1244 700 0 0 0 ## X21340 6229.3682 22 14 22614.0351 1000 2000 0 25000 ## X14250 6519.0232 30 7 24669.8565 0 0 0 0 ## X1719093 11386.7530 42 12 207637.9585 2100 1100 6200 0 ## X2105594 8250.0749 67 12 12334.9282 10220 0 0 0 ## X11425 6084.3695 54 17 160354.0670 7230 1500 39000 20000 ## X2135095 9864.7351 68 12 24669.8565 1200 200 0 0 ## X13565 6017.3025 50 15 35976.8740 50 0 0 0 ## X17540 3468.5962 42 17 164465.7097 0 8100 84000 100000 ## X2985 8494.8290 77 7 15418.6603 0 0 0 0 ## X15070 4397.4675 30 16 15418.6603 2200 500 4800 0 ## X3505 9857.4584 70 13 19530.3030 1800 11000 0 0 ## X15015 9887.6811 55 14 66814.1946 1530 0 0 0 ## X16815 4348.7029 32 12 13362.8389 500 0 0 0 ## X7485 6228.4329 45 16 41116.4274 0 0 0 0 ## X18460 5665.4249 64 16 65786.2839 0 0 65000 0 ## X9465 5717.0688 42 13 54479.2663 0 0 0 0 ## X10825 6128.8931 32 9 15418.6603 50 2500 0 0 ## X8105 7108.2801 34 16 71953.7480 1150 11500 0 170000 ## X5820 6043.2868 54 13 38032.6954 1000 22000 0 0 ## X14765 11386.7530 71 12 38032.6954 4500 3000 0 160000 ## X5340 2818.4551 54 17 175772.7273 6000 35010 140000 7000 ## X3720 4804.4293 34 9 22614.0351 0 0 0 0 ## X4475 6685.2809 59 17 25697.7671 350 0 35000 0 ## X15185 9717.4090 36 12 30837.3206 520 1900 6000 0 ## X68096 6833.6584 48 16 100735.2472 3800 0 13000 0 ## X13745 7513.1999 39 11 11307.0175 0 0 0 0 ## X125097 7269.0062 41 12 52423.4450 4270 6010 0 0 ## X8345 6991.5808 33 11 53451.3557 400 1200 0 0 ## X2435 6752.1781 57 13 17474.4817 280 5 0 0 ## X1900 3591.7791 77 17 38032.6954 6000 15000 175000 250000 ## X11670 5731.3659 58 16 18502.3923 5700 2500 24000 0 ## X18465 8998.8304 61 12 47283.8915 5200 4000 0 0 ## X20605 9663.4260 39 12 27753.5885 1000 1500 0 0 ## X1127598 10483.6685 68 12 27753.5885 3300 8000 32000 116000 ## X15815 7402.3350 41 12 49339.7129 1500 100 0 0 ## X4465 10051.0999 69 15 61674.6411 4500 0 28000 0 ## X14585 6609.2130 49 2 134656.2998 500 0 30000 1000 ## X12930 6801.3996 40 14 43172.2488 2500 1000 20000 0 ## X3875 6076.2830 33 14 41116.4274 1000 6000 0 0 ## X11340 5351.9437 79 14 41116.4274 2300 15000 0 50000 ## X4985 7666.4941 56 12 113070.1754 880 0 0 50000 ## X21245 7308.9786 33 12 29809.4099 1 0 0 0 ## X1203599 5803.8741 35 12 46255.9809 770 0 0 0 ## X18640 9424.8965 70 16 21586.1244 150 3000 0 0 ## X3875100 6076.2830 33 14 41116.4274 1000 6000 0 0 ## X7570 7388.8300 37 12 48311.8022 2500 150 0 0 ## X16115 7197.0427 46 16 102791.0686 0 5000 0 200000 ## X6355 6830.9965 46 12 29809.4099 0 0 0 0 ## X17615 7230.1995 48 12 30837.3206 30 400 0 0 ## X6920 4859.1365 80 3 5550.7177 0 0 0 0 ## X8960 4327.1130 32 16 83260.7655 3540 330 0 0 ## X2325101 7803.2247 45 15 113070.1754 4900 2000 12000 1000 ## X255 8959.1771 57 14 77093.3014 2000 10000 0 0 ## X19515 4547.3081 28 16 82232.8549 1110 0 20000 25000 ## X19205102 7691.5051 44 12 53451.3557 1000 1050 0 0 ## X15825 6525.3098 38 16 58590.9091 1000 0 0 0 ## X9090 5431.0831 47 14 39060.6061 500 580 0 0 ## X4540 10335.8356 83 8 7709.3301 700 8000 0 0 ## X15225 1483.1859 46 16 575629.9840 13460 59970 330000 600000 ## X10300 4752.0953 24 12 7400.9569 0 120 0 0 ## X21650 2980.8019 36 14 41116.4274 2000 200 0 0 ## X3780 4750.7510 55 12 10279.1069 10 0 0 0 ## X13835 7147.1438 54 15 57562.9984 2500 0 0 0 ## X5100 7138.9429 41 13 51395.5343 2000 0 0 0 ## X11025 5317.0913 43 2 65786.2839 3000 8700 0 0 ## X13740 5573.7811 36 12 32893.1419 2700 6000 0 0 ## X17905 8897.0175 33 16 32893.1419 900 60 0 0 ## X925 4893.8646 54 13 41116.4274 2000 500 0 0 ## X11925 9050.7760 42 17 12334.9282 510 0 0 0 ## X5210103 9050.9752 48 14 46255.9809 800 1000 5000 0 ## X14005 4375.7084 24 12 11307.0175 0 0 0 0 ## X17815 6270.3754 56 16 102.7911 1000 0 0 0 ## X14200 6430.0038 43 12 57562.9984 610 2890 0 0 ## X2855104 5717.5206 66 12 9148.4051 0 0 0 0 ## X310105 5950.2488 40 12 21586.1244 2000 500 0 0 ## X15355 10936.6997 71 17 26725.6778 4000 0 2000 0 ## X15135 7500.0875 30 13 43172.2488 6500 3800 0 0 ## X9020 8109.0436 30 16 83260.7655 11000 0 0 0 ## X18630 9692.4653 42 10 82232.8549 2520 800 0 0 ## X17315 9117.2509 23 12 15418.6603 0 105 0 0 ## X19685 5755.9082 49 12 53451.3557 1000 300 0 0 ## X7100 5700.7692 42 12 30837.3206 400 220 0 0 ## X12945106 6490.4551 27 12 28781.4992 0 0 4000 0 ## X7655 1537.8345 39 16 411164.2743 18000 0 50000 50000 ## X20875 4540.8913 31 12 6681.4195 0 1650 0 0 ## X9300 7690.7114 32 13 52423.4450 210 700 0 0 ## X16905107 11386.7530 76 16 28781.4992 5600 6800 48000 100000 ## X155 6972.1301 31 12 46255.9809 4300 300 0 0 ## X15280 4540.8913 32 13 14390.7496 0 0 0 0 ## X17385 5748.4001 85 14 9045.6140 1200 0 0 0 ## X12880 9610.2593 29 16 69897.9266 2400 250 0 0 ## X1595 8089.9766 43 17 175772.7273 2990 1800 103000 2500 ## X5720 7359.6699 30 12 51395.5343 200 3000 0 0 ## X17375 5950.0337 58 14 100735.2472 500 18000 0 0 ## X11495108 9240.9040 44 16 81204.9442 5700 2200 0 8800 ## X7680109 7263.7921 52 12 35976.8740 1700 3000 2000 0 ## X2590 6645.0764 48 15 28781.4992 1820 850 0 0 ## X7200 9629.4836 72 12 10279.1069 1000 0 0 0 ## X1575 6805.1027 38 10 61674.6411 850 0 0 0 ## X12065 4620.4798 37 12 6887.0016 0 0 0 0 ## X9715 8414.4992 39 17 123349.2823 2500 2000 0 0 ## X5065 5298.4252 62 10 11307.0175 0 0 0 0 ## X9520 4900.7021 39 13 30837.3206 1000 2300 0 0 ## X20565110 11386.7530 40 17 193247.2089 5000 1000 0 23000 ## X2100111 9341.9182 47 12 35976.8740 480 0 0 0 ## X5175112 5026.4489 36 11 19530.3030 0 300 0 0 ## X15880 10431.8465 53 12 40088.5167 1300 4500 0 0 ## X615 6944.9344 22 12 18502.3923 1000 0 0 0 ## X19490 6198.3243 47 17 144935.4067 2500 8000 0 7500 ## X13850 5611.6356 32 13 38032.6954 400 0 0 0 ## X14070 7228.2467 74 12 33921.0526 520 3900 0 0 ## X16555113 7602.8631 71 12 16446.5710 1000 0 0 0 ## X21750 6950.8753 52 17 39060.6061 300 1200 0 0 ## X1305 7662.7554 35 13 123349.2823 1600 15500 0 35000 ## X5210114 9050.9752 48 14 46255.9809 800 1000 5000 0 ## X14400115 8454.0719 72 10 4008.8517 0 0 0 0 ## X4120 3435.4000 83 16 30837.3206 10000 0 0 0 ## X13600 9383.0100 32 14 92511.9617 0 1300 0 55000 ## X1670 11259.5753 66 12 18502.3923 800 0 0 0 ## X8790 7029.6621 44 16 100735.2472 400 1900 0 1300 ## X8150 6310.8060 47 12 53451.3557 750 5000 0 0 ## X14155 6695.2174 73 4 33921.0526 11500 0 0 0 ## X17905116 8897.0175 33 16 32893.1419 900 60 0 0 ## X16735 5804.3179 66 16 51395.5343 1000 0 0 0 ## X21095117 6777.8412 51 14 76065.3908 600 0 0 0 ## X10280 9401.2080 80 16 42144.3381 9200 0 0 0 ## X8695 6033.5784 22 12 29809.4099 0 0 0 0 ## X15485 5972.4428 50 14 4933.9713 0 0 0 0 ## X920118 5812.9110 44 15 87372.4083 300 32700 0 0 ## X525 8516.9613 73 12 9559.5694 1300 0 0 0 ## X10740119 9507.0043 78 5 7709.3301 0 0 0 0 ## X8885 5016.1313 47 12 41116.4274 240 250 0 0 ## X20200 5845.6749 26 14 52423.4450 1000 300 0 0 ## X2295 7105.8186 69 6 17474.4817 720 0 0 0 ## X14855 4017.0656 34 12 27753.5885 800 0 0 0 ## X20390 10431.8465 50 13 48311.8022 3000 0 0 0 ## X13895 5545.0508 35 12 37004.7847 1020 0 770 0 ## X12335 8147.8136 76 12 30837.3206 18000 11000 0 0 ## X11880 5934.1263 54 12 63730.4625 0 0 0 0 ## X3750 1541.4366 65 15 65786.2839 3000 2000 0 2500 ## X16305120 6168.6436 22 12 33921.0526 60 0 0 0 ## X11875 6985.4172 42 9 45228.0702 0 100 0 0 ## X7670121 7111.7751 50 14 53451.3557 3100 0 0 0 ## X6130 7499.2157 41 12 11307.0175 0 0 0 0 ## X8050 7657.5067 56 7 47283.8915 150 0 0 0 ## X17970 8302.0167 72 16 15418.6603 0 0 0 0 ## X18735 4949.8349 49 16 10279.1069 0 50 0 0 ## X6920122 4859.1365 80 3 5550.7177 0 0 0 0 ## X235 4923.1401 48 12 42144.3381 0 0 0 0 ## X5530 6282.3042 52 17 118209.7289 0 0 10000 10000 ## X18720 5887.3046 49 12 2055.8214 0 400 0 0 ## X2330123 6659.7322 46 17 20558.2137 1000 400 6000 40000 ## X335 4177.0529 41 12 25697.7671 0 1500 0 0 ## X19405 5895.3995 29 12 69897.9266 2830 910 0 0 ## X2615 8241.5555 46 12 60646.7305 2400 1500 0 0 ## X8060 6654.5552 67 12 61674.6411 2200 15000 0 100000 ## X7990 4922.4516 27 14 24669.8565 630 0 0 0 ## X17205 8028.6114 34 5 37004.7847 1400 200 0 0 ## X110 10373.1531 79 12 16446.5710 800 0 0 0 ## X16470124 3597.7161 43 12 1408237.6396 10 0 0 0 ## X8690 7624.4548 48 12 82232.8549 4000 7000 0 0 ## X3350 11386.7530 58 13 30837.3206 11000 159900 0 2500 ## X2440 5845.6749 30 15 46255.9809 1020 0 0 5700 ## X9570 6262.5384 20 14 24669.8565 100 0 0 0 ## X19650 7270.8124 48 13 29809.4099 650 7000 40000 25000 ## X12680125 8925.5408 27 12 37004.7847 2150 500 0 1000 ## X6175 7664.6497 52 11 68870.0159 1800 0 0 1200 ## X2860126 6604.7905 45 12 20558.2137 1500 530 0 0 ## X21470 9663.4260 42 12 25697.7671 600 0 0 0 ## X9360 5700.4279 22 13 82232.8549 300 5500 0 0 ## X3235127 6509.0382 57 12 51395.5343 700 2200 0 0 ## X10540 3529.8025 52 17 102791.0686 11000 0 464000 120000 ## X18595 1503.9178 49 17 116153.9075 5000 66500 115000 19000 ## X13935 5057.4235 86 17 72981.6587 24000 0 60000 35000 ## X16950 5315.7504 52 16 114098.0861 10000 10000 0 0 ## X9715128 8414.4992 39 17 123349.2823 2500 2000 0 0 ## X11980 8233.0605 20 11 20558.2137 200 0 0 0 ## X2345 6804.0267 40 17 53451.3557 4500 520 70000 0 ## X21130129 11097.5342 78 12 15418.6603 310 0 0 0 ## X12600 6482.1927 53 12 35976.8740 600 2500 0 0 ## X3470130 6106.0776 50 12 59618.8198 3000 13900 0 0 ## X9425 5837.2792 89 14 13362.8389 800 10000 0 0 ## X21625 6119.4284 27 12 32893.1419 1000 0 0 0 ## X13110 6654.0201 68 16 15418.6603 160 16000 0 0 ## X10765 8414.4992 37 12 48311.8022 1000 770 0 2000 ## X10290 6556.6396 37 14 95595.6938 1500 5600 80000 0 ## X20650 11386.7530 48 16 31865.2313 5000 200 40000 0 ## X20680 5009.8727 42 12 53451.3557 2000 0 84000 0 ## X20325 7403.3590 38 13 70925.8373 2500 38800 0 0 ## X15740 5752.8640 54 6 15418.6603 0 0 0 0 ## X10040131 7735.4996 88 13 11307.0175 0 0 0 0 ## X2085 7857.7937 38 10 10176.3158 780 0 0 0 ## X18375132 3907.1193 26 12 29809.4099 1000 2500 0 0 ## X15970 3029.4357 58 14 50367.6236 3000 0 180000 29000 ## X15490 6835.9388 43 16 56535.0877 1200 24150 3500 0 ## X9805 7615.4140 47 14 14390.7496 200 0 0 0 ## X19805 5974.4062 32 13 62702.5518 70 0 0 0 ## X6710133 6046.9947 44 14 77093.3014 2000 2000 0 0 ## X20265134 4733.4575 40 16 28781.4992 820 400 0 18000 ## X16850 4798.3702 27 7 32893.1419 20 0 0 0 ## X10875135 7706.5913 41 17 63730.4625 2000 11500 0 0 ## X10560136 7091.0393 78 6 8223.2855 660 0 0 0 ## X19625137 8823.5948 63 1 17474.4817 4000 14700 0 0 ## X13545 6930.3176 64 10 37004.7847 300 3000 0 0 ## X13725138 7519.7320 55 11 27753.5885 0 700 0 4000 ## X13385 6683.4423 40 12 81204.9442 2000 0 1000 0 ## X16935139 6093.3713 22 12 24669.8565 8000 0 0 0 ## X4930 11139.3320 79 7 21586.1244 8000 15000 0 0 ## X20930 7581.9742 44 12 41116.4274 530 800 0 500 ## X17100 5971.9347 57 14 28781.4992 8600 13000 0 5000 ## X18795 8287.0909 78 11 19530.3030 3000 14000 0 0 ## X1315 4712.3192 42 12 29809.4099 0 1500 0 0 ## X3990140 6367.9364 65 5 12334.9282 0 0 0 0 ## X6590141 8483.7784 51 12 88400.3190 480 3050 0 0 ## X10940142 9740.7156 76 14 46255.9809 3200 1700 103000 20000 ## X17560143 9704.9669 66 12 66814.1946 2500 5000 0 0 ## X300 3764.5960 41 12 31865.2313 60 200 0 0 ## X11475 7178.3649 23 16 74009.5694 810 0 3000 0 ## X15370 5071.1464 52 12 44200.1595 600 340 0 0 ## X12230 5301.4026 31 10 15418.6603 130 0 0 0 ## X6570 7914.2643 58 13 21586.1244 200 200 0 0 ## X13610 7169.1759 36 11 51395.5343 80 10 0 0 ## X1940 7388.8300 37 17 59618.8198 4500 6000 0 0 ## FIN VEHIC HOMEEQ OTHNFIN DEBT NETWORTH ## X17470 39600 6400 84000 0 40200.0 170800 ## X315 5400 21000 8000 0 58640.0 17760 ## X8795 15460 2000 12000 0 19610.0 9850 ## X10720 54700 18250 90000 0 8000.0 284950 ## X19170 12800 9100 47000 0 21000.0 268900 ## X22075 70500 7500 175000 0 0.0 253000 ## X12235 16000 16000 0 0 31000.0 1000 ## X7670 12200 34000 22000 0 60600.0 45600 ## X16555 13000 1800 15000 0 0.0 29800 ## X370 50 1300 0 0 9800.0 -450 ## X7680 12700 4200 8000 0 92000.0 24900 ## X6880 0 3300 15000 0 3400.0 14900 ## X16570 64100 31000 0 0 36200.0 58900 ## X12945 4000 9400 0 0 1500.0 11900 ## X6725 9050 8800 75000 0 0.0 92850 ## X15725 1238000 69000 1600000 0 0.0 4032000 ## X19880 4015 38000 7000 0 147400.0 -7385 ## X225 813000 15000 130000 0 0.0 975000 ## X4995 393440 14400 315000 0 0.0 722840 ## X7700 48750 15400 20000 0 230000.0 194150 ## X11375 20300 37800 52000 0 18810.0 91290 ## X17920 111500 11000 88000 0 32340.0 200160 ## X12365 120520 26900 87500 0 17300.0 230120 ## X920 93000 7700 59000 0 66000.0 159700 ## X19050 313500 7300 500000 0 0.0 1079800 ## X19555 18650 30300 64000 0 125900.0 383050 ## X10520 2300 0 0 0 600.0 1700 ## X18705 16550 15200 111000 0 142000.0 129750 ## X5095 60100 9600 333000 0 0.0 402700 ## X11010 250 5800 0 0 840.0 5210 ## X3540 760 4100 0 0 30.0 4830 ## X14950 162700 3700 67000 800 43300.0 223900 ## X4830 1350 4800 6000 0 0.0 12150 ## X2865 167500 9300 75000 0 0.0 251800 ## X20945 122710 29000 110000 0 159500.0 242210 ## X13040 19880 13500 0 0 4400.0 28980 ## X4515 135700 7900 0 0 780.0 142820 ## X145 40 0 0 0 400.0 -360 ## X18685 2480 8600 40000 0 5100.0 45980 ## X17585 92100 22000 43000 0 92000.0 148900 ## X10090 0 0 0 0 1300.0 -1300 ## X13235 25000 0 0 0 0.0 525000 ## X3045 287000 14700 34000 0 137100.0 324600 ## X21425 90 0 0 0 5450.0 -5360 ## X11840 0 0 0 0 6000.0 -6000 ## X3400 25100 9900 44000 0 16600.0 73400 ## X6635 25600 4200 5000 0 2100.0 32700 ## X19815 80900 4700 0 0 300.0 85300 ## X19565 7350 11800 9000 0 84770.0 26380 ## X12135 67650 38900 108000 0 181400.0 168850 ## X10700 640000 22000 100000 0 0.0 836000 ## X2600 54700 4600 32000 0 98000.0 91300 ## X2860 4830 2800 0 0 650.0 6980 ## X2175 3600 0 0 0 900.0 2700 ## X14915 72800 28800 15000 0 78100.0 108500 ## X66351 25600 4200 5000 0 2100.0 32700 ## X6575 21000 38000 19000 0 16000.0 709000 ## X8410 12800 30000 18000 0 73000.0 39800 ## X7230 360350 11000 60000 0 0.0 493850 ## X12955 1000 16020 31000 0 20900.0 77120 ## X19205 57550 13000 136000 0 60200.0 200350 ## X600 9190 19700 27000 8000 139900.0 46990 ## X1290 4750 6300 0 0 13400.0 -2350 ## X17070 1189000 47000 425000 20000 340000.0 2031000 ## X16140 226800 12000 100000 0 0.0 338800 ## X17935 36000 31100 102000 0 93500.0 301100 ## X3605 92000 17700 0 0 8800.0 100900 ## X10275 274000 26500 106000 5000 180600.0 404900 ## X19930 32450 23000 178000 0 40000.0 225450 ## X15360 137000 40000 185000 0 162900.0 444100 ## X1075 158090 14300 37000 2500 0.0 223890 ## X7770 172600 6100 65000 0 0.0 243700 ## X1010 2700 0 21000 0 108000.0 23700 ## X7095 660 0 0 0 0.0 660 ## X14255 2960 1900 33000 0 940.0 42920 ## X20075 30000 7100 170000 0 11030.0 196070 ## X2610 570 2700 0 0 1200.0 2070 ## X965 49600 3400 0 0 20.0 52980 ## X17515 37000 5000 115000 0 21200.0 155800 ## X1755 0 0 0 0 0.0 0 ## X16440 329500 81800 91000 0 59000.0 509300 ## X14750 9200 2300 12000 0 62000.0 19500 ## X16960 82800 31400 100000 0 115000.0 199200 ## X575 2000 3800 0 0 0.0 5800 ## X12340 317500 5600 217000 0 0.0 540100 ## X3250 300 3800 0 0 0.0 4100 ## X21805 1325001 15000 570000 25000 188000.0 2087001 ## X17860 42400 30000 90000 0 72550.0 164850 ## X6260 379000 31000 389000 0 254400.0 794600 ## X8435 7160 14000 0 0 13480.0 7680 ## X10795 151530 22800 125000 0 0.0 299330 ## X9785 6650 11800 15000 0 55990.0 32460 ## X17455 6000 2100 23000 0 52500.0 30600 ## X11275 351300 12500 200000 0 0.0 563800 ## X6785 61005 0 38000 0 93040.0 90965 ## X12920 0 0 0 0 0.0 0 ## X12685 49650 7400 62000 0 18000.0 114050 ## X7575 31700 6800 0 0 300.0 38200 ## X16745 3300 5000 2000 0 79800.0 3500 ## X3925 0 13200 0 0 20000.0 -6800 ## X13715 1140 0 0 0 0.0 1140 ## X2630 50500 38000 7000 0 220500.0 224000 ## X1880 1220 3100 0 0 1000.0 8320 ## X16810 1520 7500 0 0 14100.0 -5080 ## X7535 31251 16100 0 0 24700.0 24951 ## X17395 0 0 0 0 0.0 0 ## X20265 19220 4300 0 0 0.0 23520 ## X16645 105330 9900 276000 0 254000.0 386230 ## X18180 565400 24600 209000 0 840.0 798160 ## X4825 1000 11300 0 0 3020.0 9280 ## X1845 168000 21900 250000 0 18600.0 539300 ## X5425 22800 21800 80000 0 220300.0 124300 ## X10600 214000 54800 89000 30000 12800.0 386000 ## X10360 0 0 27000 0 10000.0 27000 ## X19890 19300 8800 0 0 5600.0 22500 ## X20500 0 0 163000 0 0.0 163000 ## X2565 430000 26600 70000 0 5000.0 526600 ## X26002 54700 4600 32000 0 98000.0 91300 ## X19845 3640 17000 0 0 13000.0 7640 ## X18965 88600 42600 67000 0 28000.0 198200 ## X11230 0 0 0 0 0.0 0 ## X11260 173900 11600 420000 0 300.0 605200 ## X3200 45740 41200 201200 0 60500.0 236440 ## X5965 24020 7300 0 0 350.0 30970 ## X107953 151530 22800 125000 0 0.0 299330 ## X11035 51400 2500 60000 0 0.0 113900 ## X18245 35800 122300 98800 0 104800.0 491300 ## X11955 20000 27900 3000 0 47600.0 80300 ## X9345 80 0 0 0 0.0 80 ## X2320 25970 18100 118000 0 36680.0 147390 ## X9295 46400 6000 290100 0 13100.0 339300 ## X20110 16900 0 8000 0 167750.0 24150 ## X680 327800 43000 220000 0 0.0 590800 ## X13270 81350 9800 0 0 6500.0 84650 ## X3075 78050 7800 29000 0 110840.0 71010 ## X13160 85100 8800 37000 0 103000.0 120900 ## X20435 780000 0 850000 0 850000.0 2780000 ## X12465 783000 12100 170000 0 30000.0 1015100 ## X4440 760 0 0 0 0.0 760 ## X3870 78700 9400 20000 0 9800.0 98300 ## X3510 7900 47500 29000 0 86300.0 73100 ## X13795 0 0 0 0 0.0 0 ## X18155 9000 18200 0 0 68300.0 76900 ## X4685 115700 28700 141000 0 73900.0 270500 ## X20135 770 17100 0 0 3610.0 14260 ## X7975 40000 3500 210000 0 0.0 253500 ## X16425 116300 23600 37000 0 201120.0 178780 ## X84354 7160 14000 0 0 13480.0 7680 ## X12905 196000 9000 58000 0 164000.0 298000 ## X15095 133010 34200 0 0 0.0 167210 ## X3625 0 0 10000 0 0.0 10000 ## X198455 3640 17000 0 0 13000.0 7640 ## X570 57001 8400 20000 0 11200.0 74201 ## X21195 279390 21700 125000 0 0.0 506340 ## X16470 400060 26000 380000 0 0.0 2856060 ## X14880 6340 9900 106000 0 49000.0 98240 ## X9485 11405 21700 47000 0 37950.0 55155 ## X17090 700 4500 0 0 0.0 5200 ## X9670 55500 31000 170000 0 139600.0 596900 ## X15945 0 10100 0 0 70.0 10030 ## X13535 400 22800 0 0 18800.0 4400 ## X3685 17370 6400 4800 0 11360.0 25410 ## X540 4930 5300 46000 0 258800.0 88930 ## X17780 253900 13000 112000 0 28000.0 378900 ## X21100 903000 9300 109000 0 34000.0 1093300 ## X4310 2800 7700 25000 0 147100.0 3400 ## X2010 1200 3000 20000 0 100000.0 24200 ## X8785 406000 28800 80000 0 0.0 787800 ## X1045 8500 6700 62000 0 23720.0 76480 ## X2935 1500 0 70000 0 0.0 71500 ## X11195 135000 2200 0 0 2300.0 134900 ## X110356 51400 2500 60000 0 0.0 113900 ## X3410 88711 25600 62000 0 38700.0 175611 ## X17765 12000 4500 0 0 0.0 16500 ## X9175 22200 31900 25000 0 107700.0 46400 ## X6395 19000 12000 0 0 520.0 30480 ## X485 5400 12600 0 0 4140.0 13860 ## X870 52440 50700 149000 0 174000.0 208140 ## X9220 40 0 0 0 300.0 -260 ## X1920 400 6300 28000 0 1600.0 33100 ## X19230 169350 18200 300000 0 0.0 487550 ## X18475 875950 9100 240000 0 0.0 1125050 ## X5895 1120 1700 0 0 0.0 2820 ## X3695 40100 5900 8000 0 169000.0 54000 ## X17075 126500 13000 10000 0 96700.0 92800 ## X21685 800 13900 23000 0 39200.0 24500 ## X10410 18300 15000 0 0 0.0 86300 ## X1350 0 0 0 0 0.0 0 ## X18760 61400 17000 124000 0 303000.0 487900 ## X3405 825 13000 0 0 28800.0 -14975 ## X12035 1170 8900 28000 0 108800.0 26270 ## X305 223800 16200 60000 0 0.0 300000 ## X17850 83500 13000 201000 0 129700.0 416800 ## X4110 3577000 188000 1400000 0 400000.0 5465000 ## X4605 16800 10900 100000 0 0.0 127700 ## X12555 0 0 0 0 0.0 0 ## X5915 478500 0 410000 0 290000.0 2278500 ## X22035 0 0 0 0 0.0 0 ## X6930 64000 8000 50000 0 128200.0 303800 ## X17060 13800 17000 143000 0 0.0 173800 ## X13760 71750 50600 100000 0 10000.0 517350 ## X5825 1300 11600 0 0 0.0 12900 ## X34057 825 13000 0 0 28800.0 -14975 ## X20180 3350 49700 10000 0 156300.0 39750 ## X21130 310 4400 83000 0 0.0 87710 ## X12205 0 0 0 0 30000.0 -30000 ## X1265 7300 9100 85000 0 9100.0 94300 ## X13645 108000 17500 200000 0 0.0 325500 ## X905 700 0 0 0 0.0 700 ## X21995 36300 0 0 0 60.0 36240 ## X6975 1100 7600 0 0 870.0 7830 ## X16450 20920 12000 0 0 20700.0 12220 ## X14840 7000 0 0 0 0.0 7000 ## X8300 348950 27200 25000 0 126500.0 370650 ## X645 132000 27000 195000 0 155000.0 1074000 ## X2770 11400 6300 115600 0 41400.0 96300 ## X147508 9200 2300 12000 0 62000.0 19500 ## X1540 3560 13000 2000 0 91140.0 3420 ## X19435 0 0 0 0 0.0 0 ## X6765 10400 38800 32000 0 76200.0 73000 ## X54259 22800 21800 80000 0 220300.0 124300 ## X19980 45000 13000 100000 0 0.0 158000 ## X54010 4930 5300 46000 0 258800.0 88930 ## X21890 200650 38400 60000 0 176000.0 263050 ## X1220 23200 2900 27000 0 0.0 53100 ## X16615 1107000 43700 179000 0 103400.0 1622300 ## X16905 680430 9900 200000 0 0.0 1020330 ## X9050 780200 33000 360000 0 135000.0 1577800 ## X21165 426300 79200 60000 70000 143000.0 632500 ## X16350 0 0 0 0 0.0 0 ## X14085 200 11000 0 0 12500.0 -1300 ## X11465 1600 12800 0 0 800.0 13600 ## X12610 30000 23100 4000000 120000 0.0 4173100 ## X785 18900 24700 0 0 38000.0 20600 ## X14485 1860 9300 31000 0 42200.0 41960 ## X8580 101400 6700 24000 0 66300.0 128800 ## X10340 500 33000 27000 0 93000.0 35500 ## X20855 650 0 0 0 0.0 650 ## X5420 12310 13700 -7000 0 168930.0 -32920 ## X1200 68700 18600 77000 0 2400.0 161900 ## X13395 39500 39600 60000 0 274000.0 145100 ## X10230 10000 10000 159000 0 41000.0 179000 ## X17945 8960 5200 11000 5100 192650.0 -101390 ## X565 300 6700 0 0 1200.0 5800 ## X18070 39300 1500 160000 0 0.0 200800 ## X509511 60100 9600 333000 0 0.0 402700 ## X8940 29000 17500 0 0 9800.0 36700 ## X11575 154000 4000 211000 0 9950.0 368050 ## X1213512 67650 38900 108000 0 181400.0 168850 ## X14770 81950 28200 62000 0 139000.0 151150 ## X22015 44800 20800 125000 0 13200.0 277400 ## X4965 0 3600 0 0 15000.0 -11400 ## X1660 148000 15000 133000 0 23800.0 272200 ## X20795 4510 5500 0 0 4200.0 8910 ## X2045 2000 27000 6000 0 195800.0 -1800 ## X10235 395201 0 400000 0 5000.0 790201 ## X12060 220 4400 0 0 900.0 3720 ## X5680 203700 31100 27000 50000 253610.0 294190 ## X20215 43660 9500 0 0 12000.0 41160 ## X15375 3200 6400 20000 0 0.0 29600 ## X10740 0 0 102000 0 28600.0 101400 ## X4160 389500 0 300000 0 30000.0 714500 ## X310 2500 2000 0 0 600.0 3900 ## X3235 6200 20100 70000 0 24670.0 91630 ## X21055 104220 3900 135000 0 7000.0 254120 ## X2620 0 0 0 0 0.0 0 ## X1600 35820 0 0 5000 5860.0 34960 ## X1751513 37000 5000 115000 0 21200.0 155800 ## X5765 0 2000 0 0 650.0 1350 ## X16945 413300 20800 90000 0 180.0 553920 ## X20830 320000 7600 60000 0 580000.0 7547600 ## X10105 412500 21000 12000 0 187600.0 420900 ## X4895 63100 9980 18000 0 62000.0 91080 ## X9895 0 0 0 0 0.0 0 ## X10650 100 2600 40000 0 0.0 42700 ## X8705 88000 32100 125000 0 25800.0 219300 ## X1490 68000 21600 29088 0 0.0 629600 ## X341014 88711 25600 62000 0 38700.0 175611 ## X1408515 200 11000 0 0 12500.0 -1300 ## X16235 8370 52000 9000 0 89400.0 32970 ## X2201516 44800 20800 125000 0 13200.0 277400 ## X17115 9100 27700 49000 0 16500.0 144300 ## X22110 41450 9800 20000 0 111800.0 62450 ## X5075 9550 6600 87000 0 0.0 103150 ## X3895 515650 20580 346000 0 54080.0 882150 ## X18550 4850 15010 77000 0 56910.0 82950 ## X1998017 45000 13000 100000 0 0.0 158000 ## X10815 8960 9700 90000 0 0.0 148660 ## X130 67800 10000 185000 0 40080.0 497720 ## X15700 43950 62000 41000 0 190500.0 103450 ## X10560 660 3700 0 0 0.0 4360 ## X8180 9500 2600 20000 0 0.0 32100 ## X6115 58000 7300 0 0 9850.0 55450 ## X11495 182500 17100 47000 0 97100.0 222500 ## X17710 187500 24000 125000 6000 0.0 342500 ## X10510 0 0 0 0 0.0 0 ## X10990 12500 11200 18000 0 31100.0 38600 ## X13300 5220 5200 30000 0 93340.0 37080 ## X19315 5605 11100 15000 0 72500.0 24205 ## X10685 0 3400 0 0 2500.0 900 ## X19330 188600 7300 60000 13000 101200.0 267700 ## X16260 10 2100 0 0 0.0 2110 ## X13945 13600 7900 0 0 98000.0 18500 ## X2330 109400 14000 95000 0 100110.0 288290 ## X12080 760 6100 0 0 11770.0 -4910 ## X16900 2940400 60900 400000 0 124000.0 3697300 ## X1080 108810 33000 52000 0 41300.0 320510 ## X19180 867350 41500 301000 0 137000.0 1196850 ## X2925 561000 13000 182000 9000 168000.0 765000 ## X7555 131900 4900 80000 0 0.0 216800 ## X16600 85700 37900 122000 0 36300.0 317300 ## X16795 8450 38000 5000 0 152250.0 36200 ## X16545 132500 17900 110000 0 8050.0 252350 ## X20245 2000 22700 3000 0 36490.0 15210 ## X9180 137000 36400 33000 0 102500.0 185900 ## X16480 448100 38100 239000 50000 16000.0 825200 ## X17355 67010 30000 490000 0 120000.0 527010 ## X5875 7000 1300 40000 0 0.0 48300 ## X16145 26200 11000 137000 0 40500.0 171700 ## X21770 0 0 0 0 0.0 0 ## X11820 298601 18000 44000 0 91000.0 360601 ## X9390 50000 2500 59000 0 0.0 111500 ## X12520 6000 0 0 0 0.0 6000 ## X12040 128500 18000 300000 0 0.0 637500 ## X8890 27700 37300 17000 0 98300.0 83900 ## X150 7100 14000 10000 0 45330.0 29770 ## X4600 21300 64000 72000 0 28000.0 183300 ## X12490 1540 6300 16000 0 42800.0 20040 ## X5640 11200 2900 31300 0 5700.0 45400 ## X1758518 92100 22000 43000 0 92000.0 148900 ## X3105 7885000 92000 900000 0 0.0 9102000 ## X1070019 640000 22000 100000 0 0.0 836000 ## X7035 26000 2500 63000 0 114000.0 124500 ## X19950 949410 38100 30000 0 148750.0 1183760 ## X12835 2515 5700 0 0 9540.0 22675 ## X12050 5300 9100 -2000 0 84400.0 12000 ## X12605 80210 48900 25000 0 119700.0 99410 ## X16605 48500 21000 225000 0 0.0 294500 ## X100 0 0 0 0 0.0 0 ## X21095 386100 24300 70000 0 142560.0 454840 ## X9640 14500 1200 21000 0 69260.0 36440 ## X18820 2251700 121000 301000 2500000 157050.0 5165650 ## X20565 384360 30900 148000 0 187000.0 478260 ## X13035 449800 19600 175000 0 0.0 644400 ## X15555 2187200 179000 170000 2500000 0.0 13036200 ## X8315 286000 8000 300000 0 0.0 664000 ## X990 13500 15700 6000 0 91100.0 33100 ## X19305 20400 0 0 0 0.0 70400 ## X14165 398500 15500 0 0 0.0 564000 ## X1785 17300 5400 95000 0 26000.0 118700 ## X22045 42000 12000 101000 0 42000.0 137000 ## X31520 5400 21000 8000 0 58640.0 17760 ## X21535 71150 4000 39000 0 11000.0 114150 ## X8005 60000 2000 0 0 0.0 132000 ## X21855 3230 4000 0 0 3800.0 78430 ## X2965 93500 11600 33000 0 67930.0 137170 ## X19925 150700 18600 85000 0 7400.0 246900 ## X21305 0 0 0 0 0.0 0 ## X11315 1450900 18400 173800 20000 1200.0 1663100 ## X2870 200 16300 15300 0 3700.0 31800 ## X7845 200 21000 -9000 0 72200.0 -16000 ## X11325 30700 29000 39000 0 113400.0 55300 ## X7135 28300 0 0 0 21900.0 6400 ## X1223521 16000 16000 0 0 31000.0 1000 ## X19955 87600 6800 235000 0 0.0 329400 ## X12115 27500 0 0 0 1900.0 25600 ## X20770 2000 5600 0 0 0.0 7600 ## X695 1200 4300 0 0 15570.0 -10070 ## X11320 85000 33400 65000 0 90000.0 183400 ## X7080 464300 22000 138000 0 0.0 624300 ## X21705 2000 17500 6000 0 0.0 25500 ## X10855 51700 55500 288000 0 201410.0 355790 ## X6340 20100 28600 64000 0 169000.0 99700 ## X17450 0 2600 0 0 0.0 2600 ## X2895 1001 0 1000 0 550.0 1451 ## X8115 716000 5700 200000 0 0.0 921700 ## X430 93810 0 0 0 30200.0 63610 ## X1027522 274000 26500 106000 5000 180600.0 404900 ## X387023 78700 9400 20000 0 9800.0 98300 ## X13920 700 12000 0 0 12190.0 510 ## X3160 300 2300 0 0 100.0 2500 ## X19125 527700 44000 145000 0 30200.0 1066500 ## X21480 1953150 44500 700000 220000 0.0 3427650 ## X13180 750 4200 0 0 2180.0 2770 ## X7970 111000 2500 125000 0 0.0 238500 ## X11435 19100 25500 38000 0 118100.0 81500 ## X16800 200 0 0 0 160.0 40 ## X5575 100 7300 550000 0 0.0 557400 ## X9880 700 0 0 0 0.0 700 ## X13440 10 1200 0 0 330.0 880 ## X17370 600 17400 60000 0 141200.0 66800 ## X17200 6000 13200 250000 0 0.0 269200 ## X3905 365800 48000 428000 0 123800.0 850000 ## X14000 0 0 0 0 0.0 0 ## X9710 4600 0 0 0 5000.0 -400 ## X5300 282700 13000 100000 0 0.0 395700 ## X12985 1200 13820 77000 0 25700.0 691320 ## X2007524 30000 7100 170000 0 11030.0 196070 ## X15575 4000 4500 112000 0 98200.0 120300 ## X4245 400 101900 91000 0 11800.0 190500 ## X21505 0 10100 0 0 1200.0 8900 ## X4215 530 0 0 0 10000.0 -9470 ## X12535 700 3800 165000 0 1200.0 168300 ## X16475 6800 6300 60000 0 0.0 73100 ## X4570 50 17400 32000 0 40100.0 37350 ## X15300 0 0 10000 0 600.0 9400 ## X18200 76210 27400 28000 0 68080.0 131530 ## X2325 214900 23700 215000 0 151600.0 527000 ## X3430 435900 14100 62000 0 3000.0 512000 ## X7495 300 4100 31000 0 0.0 35400 ## X489525 63100 9980 18000 0 62000.0 91080 ## X1680026 200 0 0 0 160.0 40 ## X21375 176601 35500 0 0 33200.0 178901 ## X11115 14800 12500 1000 0 64000.0 19300 ## X5220 22000 6900 0 0 1000.0 27900 ## X1488027 6340 9900 106000 0 49000.0 98240 ## X21940 18230 15900 174000 0 13700.0 194430 ## X1364528 108000 17500 200000 0 0.0 325500 ## X21040 11130 16000 0 0 29000.0 -1870 ## X7125 39000 36000 70000 0 27710.0 117290 ## X8670 510 11200 0 0 10100.0 1610 ## X10640 231500 40600 30000 0 118250.0 260850 ## X18375 3600 11000 0 0 11220.0 3380 ## X20845 6505 20100 4000 0 124900.0 -18295 ## X595 685000 17500 137000 0 38000.0 839500 ## X1455 227000 15700 184000 0 74200.0 491000 ## X8760 0 0 0 0 0.0 0 ## X626029 379000 31000 389000 0 254400.0 794600 ## X8475 631500 30700 500000 0 0.0 1162200 ## X3085 30600 23700 12000 0 56300.0 58000 ## X9285 1805000 14000 750000 0 0.0 2569000 ## X6940 3580 9800 25000 0 84800.0 29580 ## X557530 100 7300 550000 0 0.0 557400 ## X16710 4500 20200 44000 0 850.0 67850 ## X15515 20 0 0 0 0.0 20 ## X3530 108200 12000 130000 0 120000.0 250200 ## X6860 2300 0 0 0 0.0 2300 ## X14630 6180 9900 0 0 4500.0 11580 ## X14705 72760 11000 90000 0 0.0 173760 ## X13010 13800 0 0 0 250.0 13550 ## X1792031 111500 11000 88000 0 32340.0 200160 ## X12705 60030 32500 400000 0 8200.0 484330 ## X9870 3000 36800 0 0 21000.0 18800 ## X17305 8950 19200 10000 0 75000.0 33150 ## X21595 5000 0 2000 0 75400.0 -400 ## X13725 24220 10000 70000 0 21000.0 103220 ## X10040 0 0 88000 0 0.0 88000 ## X7005 29000 16000 58000 0 32900.0 172100 ## X3760 42720 24000 0 0 8100.0 58620 ## X14910 473000 33000 147000 0 222000.0 624000 ## X13365 10 5000 75000 0 0.0 80010 ## X11410 2400 3800 261400 0 67600.0 267600 ## X12220 992100 88000 250000 60000 250000.0 2390100 ## X18420 24450 23100 17000 21000 93000.0 70550 ## X9005 100 3200 0 0 10000.0 -6700 ## X11855 7200 0 -3000 0 143500.0 -16300 ## X21405 0 1800 0 0 0.0 1800 ## X21260 14600 9100 30000 0 600.0 53100 ## X8020 500 0 0 0 0.0 500 ## X10370 4650 5800 10000 0 94100.0 16350 ## X15255 3000 8200 50000 0 2000.0 59200 ## X12735 45500 18000 153000 0 75300.0 198200 ## X2635 133200 28400 32000 0 55000.0 208100 ## X4765 11740 9000 0 0 0.0 25740 ## X20295 62200 13000 16000 0 171600.0 533600 ## X4030 0 3900 0 0 1100.0 2800 ## X21360 3730 22300 59000 0 220300.0 75730 ## X858032 101400 6700 24000 0 66300.0 128800 ## X15240 159600 47000 73900 0 297100.0 389500 ## X1052033 2300 0 0 0 600.0 1700 ## X8230 5358500 49500 400000 0 10000.0 5843315 ## X6565 87890 19400 71000 0 79000.0 178290 ## X8210 4000 14500 75000 0 300.0 93200 ## X16370 16300 0 0 0 52200.0 34100 ## X2495 19500 6000 0 0 150.0 25350 ## X4950 79300 9400 89000 0 0.0 206700 ## X20625 60000 0 0 0 350000.0 3360000 ## X12640 23300 18000 88000 0 173200.0 134100 ## X16455 249000 26500 402000 0 186000.0 979500 ## X20670 50 3500 0 0 6990.0 -790 ## X9855 0 0 0 0 0.0 0 ## X7590 3200 6100 35000 0 45100.0 39200 ## X10390 160 0 0 0 0.0 160 ## X6885 410000 2000 300000 0 18000.0 694000 ## X12630 360000 0 0 0 0.0 360000 ## X587534 7000 1300 40000 0 0.0 48300 ## X6415 115050 32000 135000 0 11190.0 270860 ## X13800 241500 20500 150000 0 0.0 412000 ## X21210 5500 17000 9000 0 98700.0 8800 ## X20775 700 5100 0 0 5000.0 800 ## X16165 7200 59800 30000 0 43400.0 253600 ## X249535 19500 6000 0 0 150.0 25350 ## X18530 57500 15100 135000 0 150410.0 197190 ## X1182036 298601 18000 44000 0 91000.0 360601 ## X2485 133400 33800 26000 0 127000.0 1508200 ## X16785 9900 1700 30000 0 0.0 92600 ## X11750 155000 15100 210000 30000 0.0 440100 ## X3025 7100 18000 0 0 29000.0 -3900 ## X3470 19800 18900 75000 0 13000.0 100700 ## X1860 236700 11300 12000 0 325500.0 422500 ## X3920 800 5500 0 0 10000.0 -3700 ## X19430 5000 3000 25000 0 0.0 33000 ## X16535 400 3000 0 0 1240.0 2160 ## X13620 22500 5400 0 0 50.0 27850 ## X17880 171700 18300 11000 0 85700.0 174300 ## X4875 220300 29200 60000 0 335000.0 419500 ## X19300 4300 2700 16500 0 10400.0 21600 ## X7075 7500 0 0 0 32500.0 -25000 ## X15130 2330000 67400 165000 200000 110000.0 3342400 ## X13555 77000 47000 8000 0 172000.0 212000 ## X8385 400 20700 0 0 11380.0 9720 ## X831537 286000 8000 300000 0 0.0 664000 ## X1330 3940 20600 0 0 32330.0 1710 ## X6710 125000 65200 60000 0 115700.0 194500 ## X6055 20500 47000 600000 0 1800.0 1065700 ## X20455 1384000 82000 350000 70000 70000.0 2643000 ## X2025 558600 43800 441000 0 259000.0 1307480 ## X8485 50800 4700 18000 0 52050.0 73450 ## X6475 1000 0 0 0 10300.0 -9300 ## X4305 1378000 23000 210000 0 0.0 1611000 ## X6900 540000 48800 130000 0 0.0 1018800 ## X14525 607900 36000 408000 0 468410.0 1253490 ## X3070 55020 6200 36000 0 84500.0 96720 ## X811538 716000 5700 200000 0 0.0 921700 ## X1420 104500 0 0 0 0.0 104500 ## X542039 12310 13700 -7000 0 168930.0 -32920 ## X18380 748800 20200 30000 0 0.0 1184500 ## X4185 51230 6100 90000 0 0.0 147330 ## X13830 26020 3200 0 0 14000.0 15220 ## X6590 134930 26900 55000 0 141070.0 195760 ## X13340 10500 4200 0 0 6080.0 8620 ## X5625 197000 32600 185000 0 324570.0 480030 ## X9625 285100 11900 258000 0 30000.0 597000 ## X12020 24000 2600 180000 0 0.0 206600 ## X9580 527330 12300 212000 0 638000.0 1751630 ## X277040 11400 6300 115600 0 41400.0 96300 ## X50 71000 12000 0 0 25110.0 57890 ## X369541 40100 5900 8000 0 169000.0 54000 ## X7540 39300 3200 0 15000 0.0 57500 ## X1030 2000 0 0 0 0.0 2000 ## X14400 0 0 50000 0 0.0 50000 ## X7415 61000 6500 96000 0 105300.0 169200 ## X3990 0 0 0 0 0.0 0 ## X3245 79300 16000 70000 0 0.0 165300 ## X2575 23200 5600 0 30000 142120.0 751680 ## X9105 1500 5700 15000 0 17400.0 34800 ## X7985 3240 48400 63000 22000 118700.0 158440 ## X1300 24910 15000 215000 0 0.0 254910 ## X4760 144300 33300 259000 20000 181800.0 460800 ## X16305 60 2600 0 0 5300.0 -2640 ## X21035 216000 4800 70000 0 80800.0 290000 ## X2905 43000 0 0 0 0.0 43000 ## X1610 100500 14000 50000 0 2000.0 162500 ## X3490 31500 17000 37000 0 93000.0 335500 ## X16585 50 0 0 0 2300.0 -2250 ## X4145 30000 4900 30000 0 0.0 64900 ## X3135 0 14000 94000 0 56000.0 108000 ## X6000 37500 4300 0 0 0.0 41800 ## X10420 10000 15000 0 0 0.0 25000 ## X1655 0 10200 32000 0 46900.0 43300 ## X10705 4000 0 0 0 0.0 4000 ## X11735 388900 48800 230000 0 70000.0 667700 ## X6720 1700 4700 0 0 0.0 6400 ## X12680 6150 11700 84000 0 36000.0 71850 ## X7530 7200 29200 30000 0 12000.0 54400 ## X7795 1421 9100 0 0 7800.0 2721 ## X1480 23850 14000 0 0 0.0 37850 ## X21575 227540 29000 100000 0 153000.0 333540 ## X2585 47000 28100 30000 0 80350.0 261050 ## X16595 0 0 0 0 0.0 0 ## X4040 100 3600 35000 0 150.0 38550 ## X1630542 60 2600 0 0 5300.0 -2640 ## X16330 12840 4800 0 0 300.0 17340 ## X15665 72640 24000 60000 0 26900.0 269740 ## X690043 540000 48800 130000 0 0.0 1018800 ## X6795 400 17200 55000 0 109900.0 222700 ## X21350 3400 3200 113000 1500 17220.0 119880 ## X6700 658000 21000 155000 0 95000.0 2343000 ## X18665 103000 20300 57000 0 63000.0 180300 ## X19580 192800 44000 37000 1200 72900.0 240100 ## X20130 16200 26600 6000 0 101100.0 36700 ## X10325 5720 6600 0 0 6500.0 5820 ## X4130 798780 23000 72000 0 166000.0 1403780 ## X5475 36990 37900 175000 0 8220.0 316170 ## X1790 1500 22900 150000 0 0.0 196900 ## X17480 31650 11600 172000 0 102240.0 210010 ## X12830 52600 21000 0 0 10100.0 63500 ## X1865 25500 12800 44000 0 32300.0 81000 ## X768044 12700 4200 8000 0 92000.0 24900 ## X457045 50 17400 32000 0 40100.0 37350 ## X11490 28720 25000 200000 0 0.0 253720 ## X1146546 1600 12800 0 0 800.0 13600 ## X10180 3100 2700 0 0 17500.0 -11700 ## X3910 647000 7100 0 0 0.0 786600 ## X11565 33750 35000 91000 0 147370.0 291380 ## X21825 2100 28000 0 0 19600.0 10500 ## X4525 1000 2300 0 0 0.0 3300 ## X3060 1300 1800 48000 0 0.0 51100 ## X9250 269930 11000 10000 0 80000.0 290930 ## X17500 65000 34500 175000 0 450.0 299050 ## X1100 0 17700 0 0 7000.0 10700 ## X16025 2451 19200 200000 0 150000.0 221651 ## X12380 163500 23000 79000 0 147700.0 263800 ## X753547 31251 16100 0 0 24700.0 24951 ## X12850 9100 18000 0 0 11240.0 15860 ## X1955548 18650 30300 64000 0 125900.0 383050 ## X19775 2750 3000 0 0 0.0 30750 ## X11525 391 0 0 0 810.0 -419 ## X2975 600 5000 55000 0 770.0 60830 ## X18895 2600 6900 125000 0 23600.0 130900 ## X1602549 2451 19200 200000 0 150000.0 221651 ## X345 9620 14000 9000 0 50000.0 32620 ## X490 18200 29000 41000 0 259000.0 88200 ## X14580 22970000 89000 700000 0 2000.0 26208000 ## X10875 102500 38000 100000 0 78150.0 222350 ## X5270 7390 9800 0 0 31900.0 -14710 ## X9400 230 3400 40000 0 2360.0 41270 ## X12900 51400 15400 90000 0 380.0 168420 ## X4530 43500 2400 75000 0 0.0 120900 ## X17670 0 0 130000 0 0.0 130000 ## X5440 4330 26700 28000 0 97080.0 33950 ## X8875 138530 41600 41000 0 137250.0 232880 ## X2060 147940 2800 180000 0 0.0 330740 ## X2153550 71150 4000 39000 0 11000.0 114150 ## X5080 11500 25100 78000 0 43740.0 81860 ## X12500 61300 43000 133000 0 119000.0 235300 ## X830 100 0 0 0 0.0 100 ## X495051 79300 9400 89000 0 0.0 206700 ## X1304052 19880 13500 0 0 4400.0 28980 ## X16685 500 6600 0 0 4800.0 2300 ## X5695 145900 14000 55000 0 10180.0 204720 ## X2026553 19220 4300 0 0 0.0 23520 ## X215 101050 4700 70000 0 55000.0 175750 ## X7460 77300 24100 5000 0 154000.0 57400 ## X21060 100500 0 0 0 0.0 100500 ## X3770 160600 19500 87000 0 0.0 267100 ## X940054 230 3400 40000 0 2360.0 41270 ## X15320 7600 9900 0 0 7940.0 9560 ## X96555 49600 3400 0 0 20.0 52980 ## X19340 36000 12000 100000 0 0.0 148000 ## X1395 1800 20600 0 0 7810.0 14590 ## X939056 50000 2500 59000 0 0.0 111500 ## X5245 1430 8400 32000 0 3080.0 41750 ## X18830 2650 6000 24000 0 33710.0 30940 ## X15215 72000 19000 58000 0 132400.0 133600 ## X496557 0 3600 0 0 15000.0 -11400 ## X12210 23500 5200 0 6000 0.0 34700 ## X17560 17700 24000 69000 0 101930.0 99770 ## X19625 38700 37200 124000 0 21000.0 199900 ## X2530 50000 6100 50000 0 590.0 105510 ## X9075 16000 20000 17000 0 80400.0 37600 ## X1925 41450 2200 30000 0 0.0 73650 ## X21010 140000 23100 92000 0 0.0 263100 ## X1745058 0 2600 0 0 0.0 2600 ## X17555 5700 4200 38000 0 81000.0 40900 ## X2018059 3350 49700 10000 0 156300.0 39750 ## X5330 1574000 61000 1200000 0 0.0 4275000 ## X2150560 0 10100 0 0 1200.0 8900 ## X2970 12600 0 0 0 0.0 12600 ## X19190 51900 10000 76000 2000 179200.0 134700 ## X12570 5450 5300 10000 0 28480.0 20270 ## X1325 6600 16600 56000 0 90700.0 481500 ## X4195 70000 26600 67000 0 112000.0 139600 ## X20915 3000 5000 105000 0 52000.0 106000 ## X14145 749300 52900 47000 0 84000.0 848200 ## X13090 300 4500 0 0 10100.0 37200 ## X2211061 41450 9800 20000 0 111800.0 62450 ## X13062 67800 10000 185000 0 40080.0 497720 ## X7190 10 1400 0 0 900.0 510 ## X10690 200 0 41000 0 35000.0 41200 ## X21495 2000 10700 26000 0 24000.0 38700 ## X3745 11370 22900 38000 0 83650.0 60620 ## X2315 58310 5100 0 0 500.0 62910 ## X3170 160 3000 0 0 0.0 3160 ## X10940 293900 9200 250000 0 0.0 553100 ## X2116563 426300 79200 60000 70000 143000.0 632500 ## X2109564 386100 24300 70000 0 142560.0 454840 ## X233065 109400 14000 95000 0 100110.0 288290 ## X17530 37000 13800 65000 0 100280.0 115520 ## X12410 3084500 59000 876000 0 0.0 4569500 ## X1694566 413300 20800 90000 0 180.0 553920 ## X1250 205280 48000 115000 0 21550.0 346730 ## X1323567 25000 0 0 0 0.0 525000 ## X17190 100950 31000 271000 0 134675.0 397275 ## X103068 2000 0 0 0 0.0 2000 ## X1018069 3100 2700 0 0 17500.0 -11700 ## X18295 300 3800 0 0 6000.0 -1900 ## X8770 11300 7400 220000 0 0.0 238700 ## X585 65300 17000 48000 0 172250.0 120050 ## X8750 2000 28100 21000 0 90800.0 1094300 ## X13955 7900 15500 222000 0 29400.0 254000 ## X18825 8551000 4500 1500000 0 55300.0 35953200 ## X12280 41500 46700 23000 0 118500.0 95700 ## X21780 161000 10250 12000 0 113200.0 183050 ## X17810 246000 41600 132000 0 83000.0 444600 ## X2535 226000 3300 90000 0 50050.0 319250 ## X1614070 226800 12000 100000 0 0.0 338800 ## X17290 32510 0 75700 0 9300.0 108210 ## X15400 605700 31000 240000 0 2400.0 2897300 ## X1842071 24450 23100 17000 21000 93000.0 70550 ## X1650 5000 2700 82000 0 54700.0 438000 ## X2050072 0 0 163000 0 0.0 163000 ## X360 18900 0 -4000 0 64200.0 13700 ## X1895 185850 24700 98000 0 102000.0 308550 ## X7350 1020 12300 10000 0 6720.0 16600 ## X107573 158090 14300 37000 2500 0.0 223890 ## X2204574 42000 12000 101000 0 42000.0 137000 ## X10345 20 4000 0 0 64960.0 -60940 ## X90575 700 0 0 0 0.0 700 ## X75 60000 24900 50000 0 123000.0 112900 ## X1550 176860 7300 77000 0 29210.0 251950 ## X6040 500 2300 7000 0 750.0 9050 ## X3525 820750 17700 320000 0 800.0 1219650 ## X6980 169940 34900 220000 0 31660.0 418180 ## X178576 17300 5400 95000 0 26000.0 118700 ## X16935 9200 0 0 2500 0.0 11700 ## X928577 1805000 14000 750000 0 0.0 2569000 ## X9615 0 3000 600 0 81400.0 -25400 ## X17940 2030 15500 32000 0 9750.0 39780 ## X2855 0 0 0 0 0.0 0 ## X17900 22590 14100 25000 0 119900.0 -58210 ## X1222078 992100 88000 250000 60000 250000.0 2390100 ## X2020 37550 34000 98600 0 29870.0 141680 ## X10380 0 6700 0 0 9300.0 -2600 ## X4000 29100 67400 135000 0 0.0 231500 ## X90 5900 24100 20000 0 92800.0 26200 ## X19145 124100 6800 62000 0 48000.0 192900 ## X9140 3000 2500 0 0 0.0 14500 ## X2300 35 5000 0 0 3200.0 1835 ## X13560 730 3900 0 0 1000.0 3630 ## X1767079 0 0 130000 0 0.0 130000 ## X9665 67520 28000 21000 0 96300.0 99220 ## X9065 5000 2200 40000 0 6200.0 41000 ## X12715 12380 55800 0 0 23430.0 44750 ## X1069080 200 0 41000 0 35000.0 41200 ## X15150 48000 14000 0 0 16180.0 45820 ## X14780 3447000 22000 123000 0 102000.0 3592000 ## X3080 123750 24200 67000 0 146500.0 148450 ## X1023081 10000 10000 159000 0 41000.0 179000 ## X9725 38700 18900 77000 0 56300.0 116300 ## X1330082 5220 5200 30000 0 93340.0 37080 ## X3215 339000 15300 127000 0 98000.0 481300 ## X1069083 200 0 41000 0 35000.0 41200 ## X19635 0 0 0 0 601.0 -601 ## X14800 3300 15200 26000 0 88000.0 30500 ## X2105584 104220 3900 135000 0 7000.0 254120 ## X21380 0 0 0 0 1190.0 -1190 ## X2024585 2000 22700 3000 0 36490.0 15210 ## X11040 10250 11400 38000 0 35600.0 58050 ## X12070 23220 16000 69000 0 6400.0 101820 ## X3465 70410 36000 64000 0 117000.0 149410 ## X20725 0 0 52000 0 0.0 52000 ## X15730 49100 33600 84000 0 274440.0 137260 ## X17005 603500 22000 300000 0 615000.0 3060500 ## X4065 124440 8700 54000 0 6000.0 198140 ## X620 300 0 40000 0 0.0 40300 ## X1776586 12000 4500 0 0 0.0 16500 ## X2715 1900 7200 55000 0 111000.0 58100 ## X5210 27300 14000 40000 0 136000.0 75300 ## X1303587 449800 19600 175000 0 0.0 644400 ## X18625 280210 26000 93000 0 72050.0 392160 ## X20460 300 6400 0 0 2100.0 4600 ## X4700 20000 12200 96000 0 11400.0 120800 ## X256588 430000 26600 70000 0 5000.0 526600 ## X4180 2499000 49000 773000 0 302000.0 3259200 ## X5760 196450 4900 134000 0 231500.0 354850 ## X286589 167500 9300 75000 0 0.0 251800 ## X5745 31700 9900 0 0 54500.0 30100 ## X5175 360 8300 0 0 2200.0 6460 ## X15105 47010 9900 23000 0 30150.0 76760 ## X19895 370 18000 0 0 0.0 68370 ## X1210 111000 42000 71000 0 81000.0 212000 ## X1998090 45000 13000 100000 0 0.0 158000 ## X202091 37550 34000 98600 0 29870.0 141680 ## X14570 12300 15800 23000 15000 82400.0 49700 ## X2100 20480 5300 0 0 0.0 25780 ## X1208092 760 6100 0 0 11770.0 -4910 ## X21340 28640 22200 0 0 490.0 50350 ## X14250 200 0 0 0 46400.0 -7200 ## X1719093 100950 31000 271000 0 134675.0 397275 ## X2105594 104220 3900 135000 0 7000.0 254120 ## X11425 386330 34100 105000 0 125000.0 525430 ## X2135095 3400 3200 113000 1500 17220.0 119880 ## X13565 40050 17900 55000 0 35000.0 112950 ## X17540 691800 31400 425000 0 0.0 1148200 ## X2985 19000 4100 0 0 63000.0 23100 ## X15070 7680 0 0 0 19260.0 -11580 ## X3505 22800 18200 140000 0 0.0 249000 ## X15015 1530 13000 30000 0 50930.0 43600 ## X16815 500 0 0 0 9600.0 -9100 ## X7485 27300 3200 20000 0 64000.0 50500 ## X18460 154500 13000 50000 0 53450.0 264050 ## X9465 10000 0 0 0 12000.0 60000 ## X10825 2550 5500 0 0 0.0 8050 ## X8105 197150 10000 66000 6000 189000.0 274150 ## X5820 147000 31000 39000 0 92900.0 208100 ## X14765 417500 46000 90000 0 1200.0 552300 ## X5340 1168010 13000 0 25000 0.0 1286010 ## X3720 0 0 0 0 0.0 0 ## X4475 47350 0 150000 0 0.0 197350 ## X15185 8420 5600 16000 0 55500.0 28520 ## X68096 327800 43000 220000 0 0.0 590800 ## X13745 0 2900 -6000 0 26000.0 -3100 ## X125097 205280 48000 115000 0 21550.0 346730 ## X8345 3600 17700 0 0 6830.0 14470 ## X2435 285 10000 0 0 1590.0 8695 ## X1900 650200 22000 80000 0 0.0 752200 ## X11670 40200 0 0 0 0.0 40200 ## X18465 63500 25500 91000 0 28000.0 161000 ## X20605 38100 2500 0 3000 600.0 43000 ## X1127598 351300 12500 200000 0 0.0 563800 ## X15815 2100 26960 95000 0 75300.0 267460 ## X4465 99600 8500 160000 0 100000.0 319100 ## X14585 137500 17600 90000 2000 15700.0 241400 ## X12930 24850 10300 10000 0 126000.0 39150 ## X3875 57000 10000 0 0 27560.0 39440 ## X11340 848300 103100 100000 0 1400.0 1990000 ## X4985 116880 20200 106000 0 0.0 243080 ## X21245 1 6300 20000 0 1000.0 25301 ## X1203599 1170 8900 28000 0 108800.0 26270 ## X18640 3150 4500 9000 0 10260.0 6390 ## X3875100 57000 10000 0 0 27560.0 39440 ## X7570 3650 9100 0 0 9200.0 3550 ## X16115 359100 14100 107000 0 50000.0 473200 ## X6355 0 6600 0 0 3300.0 3300 ## X17615 430 11500 31000 0 117450.0 34480 ## X6920 0 0 0 0 0.0 0 ## X8960 7370 0 0 0 2500.0 59870 ## X2325101 214900 23700 215000 0 151600.0 527000 ## X255 43000 8800 345000 0 80000.0 424800 ## X19515 58110 19000 0 0 9900.0 74210 ## X19205102 57550 13000 136000 0 60200.0 200350 ## X15825 1000 14900 60000 0 115000.0 115900 ## X9090 19080 26200 68000 0 32200.0 113080 ## X4540 9400 0 64000 0 0.0 73400 ## X15225 2061430 85000 1130000 0 677900.0 3268530 ## X10300 120 0 0 0 0.0 120 ## X21650 2300 11400 13332 0 83930.5 103100 ## X3780 10 0 0 0 0.0 10 ## X13835 2500 27000 20000 0 72300.0 27200 ## X5100 139000 13000 273000 0 99900.0 422100 ## X11025 22700 24000 40000 0 96000.0 70700 ## X13740 37700 9000 19000 0 41000.0 65700 ## X17905 7060 9800 3000 0 142140.0 -35280 ## X925 2500 13000 0 0 0.0 15500 ## X11925 510 0 164900 0 5350.0 165160 ## X5210103 27300 14000 40000 0 136000.0 75300 ## X14005 0 0 0 0 890.0 -890 ## X17815 18600 0 65000 0 0.0 83600 ## X14200 23500 14200 89000 0 8500.0 118200 ## X2855104 0 0 0 0 0.0 0 ## X310105 2500 2000 0 0 600.0 3900 ## X15355 108200 16300 10000 0 0.0 134500 ## X15135 23800 20900 32000 0 43800.0 71400 ## X9020 19950 12000 80000 0 129900.0 102050 ## X18630 32820 13000 34000 0 150800.0 50020 ## X17315 2505 12500 0 0 4430.0 10575 ## X19685 111300 13000 -2000 0 24900.0 112400 ## X7100 620 11700 9200 0 4550.0 17770 ## X12945106 4000 9400 0 0 1500.0 11900 ## X7655 468000 0 353000 0 364000.0 8743300 ## X20875 5450 0 0 0 0.0 5450 ## X9300 1910 6700 22000 75000 63880.0 101730 ## X16905107 680430 9900 200000 0 0.0 1020330 ## X155 7100 13900 0 0 104400.0 7640 ## X15280 1700 0 0 0 250.0 1450 ## X17385 1200 0 0 0 0.0 1200 ## X12880 6210 12600 0 0 23780.0 -4970 ## X1595 310090 33100 113000 0 102000.0 456190 ## X5720 5200 0 26000 0 60100.0 30100 ## X17375 339500 28900 170000 0 70000.0 538400 ## X11495108 182500 17100 47000 0 97100.0 222500 ## X7680109 12700 4200 8000 0 92000.0 24900 ## X2590 10170 9900 0 5000 13780.0 11290 ## X7200 9700 2400 76000 0 410.0 87690 ## X1575 81610 30000 23000 0 88280.0 108330 ## X12065 0 0 0 0 0.0 0 ## X9715 86500 29000 25000 0 158650.0 136850 ## X5065 0 0 0 0 340.0 -340 ## X9520 3300 0 0 0 4000.0 -700 ## X20565110 384360 30900 148000 0 187000.0 478260 ## X2100111 20480 5300 0 0 0.0 25780 ## X5175112 360 8300 0 0 2200.0 6460 ## X15880 21300 33800 121000 0 161300.0 143800 ## X615 1000 7500 0 0 4400.0 4100 ## X19490 201000 27900 47000 0 184900.0 244000 ## X13850 2400 0 0 0 1100.0 1300 ## X14070 21920 5800 78000 0 1600.0 104120 ## X16555113 13000 1800 15000 0 0.0 29800 ## X21750 1500 12000 40000 0 57000.0 46500 ## X1305 105300 21000 75900 0 4100.0 202200 ## X5210114 27300 14000 40000 0 136000.0 75300 ## X14400115 0 0 50000 0 0.0 50000 ## X4120 125500 6700 983400 0 0.0 1819700 ## X13600 57500 12000 147000 0 129000.0 205500 ## X1670 800 7800 63000 0 157000.0 51600 ## X8790 111900 30000 159000 0 133000.0 278900 ## X8150 87750 3800 45000 0 51300.0 120250 ## X14155 42500 16700 150000 0 0.0 439200 ## X17905116 7060 9800 3000 0 142140.0 -35280 ## X16735 34700 11000 0 0 3000.0 83700 ## X21095117 386100 24300 70000 0 142560.0 454840 ## X10280 12700 7000 6200 10000 2100.0 33800 ## X8695 5770 6900 0 0 2160.0 10510 ## X15485 12000 0 60000 0 195000.0 67000 ## X920118 93000 7700 59000 0 66000.0 159700 ## X525 10300 12000 50000 0 0.0 72300 ## X10740119 0 0 102000 0 28600.0 101400 ## X8885 490 13000 0 0 12800.0 690 ## X20200 2300 6500 0 0 38200.0 -29400 ## X2295 720 6200 112000 0 39000.0 112920 ## X14855 800 0 0 0 0.0 800 ## X20390 105000 1600 60000 0 160000.0 166600 ## X13895 12340 26000 43000 0 68940.0 62400 ## X12335 271000 17500 200000 0 2900.0 485600 ## X11880 62000 23700 65000 0 1500.0 149200 ## X3750 17500 3800 0 0 3000.0 24300 ## X16305120 60 2600 0 0 5300.0 -2640 ## X11875 42100 1900 50000 0 26300.0 67700 ## X7670121 12200 34000 22000 0 60600.0 45600 ## X6130 0 950 0 0 0.0 950 ## X8050 150 22900 21000 0 55600.0 17450 ## X17970 0 0 0 0 0.0 0 ## X18735 50 4100 0 0 0.0 4150 ## X6920122 0 0 0 0 0.0 0 ## X235 20000 12700 0 0 63740.0 -31040 ## X5530 231500 20500 0 0 16000.0 236000 ## X18720 15400 12120 0 0 0.0 97520 ## X2330123 109400 14000 95000 0 100110.0 288290 ## X335 11800 5000 0 0 2350.0 14450 ## X19405 4840 23500 15000 0 146950.0 36390 ## X2615 4100 13400 63000 0 76900.0 78600 ## X8060 437200 36100 400000 0 0.0 873300 ## X7990 630 5000 0 0 8300.0 -2670 ## X17205 1600 0 17000 0 133480.0 18120 ## X110 25800 0 11000 0 58000.0 34800 ## X16470124 400060 26000 380000 0 0.0 2856060 ## X8690 36000 14100 34000 0 85960.0 80140 ## X3350 198400 23000 80000 30000 40000.0 331400 ## X2440 11070 17000 0 0 23260.0 4810 ## X9570 100 3300 0 0 0.0 3400 ## X19650 79650 7700 23000 0 113200.0 109150 ## X12680125 6150 11700 84000 0 36000.0 71850 ## X6175 4800 9800 80000 0 220.0 94380 ## X2860126 4830 2800 0 0 650.0 6980 ## X21470 600 1900 0 0 2000.0 500 ## X9360 13400 10000 0 0 11400.0 12000 ## X3235127 6200 20100 70000 0 24670.0 91630 ## X10540 1370300 20800 122000 0 0.0 1523100 ## X18595 293500 37800 202000 0 218000.0 819300 ## X13935 1038000 24800 370000 15000 30000.0 1447800 ## X16950 263700 17700 115000 0 4400.0 392000 ## X9715128 86500 29000 25000 0 158650.0 136850 ## X11980 200 5800 3000 0 17000.0 9000 ## X2345 79020 13000 25000 10000 70700.0 117320 ## X21130129 310 4400 83000 0 0.0 87710 ## X12600 3100 4200 0 0 0.0 7300 ## X3470130 19800 18900 75000 0 13000.0 100700 ## X9425 19800 2500 0 0 0.0 22300 ## X21625 1000 21000 0 0 13000.0 9000 ## X13110 32660 0 46000 0 61900.0 689760 ## X10765 115770 9400 85000 8000 60000.0 218170 ## X10290 163300 12000 72000 0 116000.0 262300 ## X20650 276200 1800 400000 0 0.0 678000 ## X20680 87500 0 0 0 40110.0 47390 ## X20325 130300 18800 17000 0 58260.0 160840 ## X15740 750 1900 0 0 0.0 2650 ## X10040131 0 0 88000 0 0.0 88000 ## X2085 780 17600 0 0 0.0 18380 ## X18375132 3600 11000 0 0 11220.0 3380 ## X15970 1273000 19000 300000 0 0.0 1592000 ## X15490 65850 10850 68568 0 0.0 157900 ## X9805 200 0 36000 0 8000.0 28200 ## X19805 12070 15700 0 1000 26950.0 1820 ## X6710133 125000 65200 60000 0 115700.0 194500 ## X20265134 19220 4300 0 0 0.0 23520 ## X16850 20 4100 0 0 100100.0 -95980 ## X10875135 102500 38000 100000 0 78150.0 222350 ## X10560136 660 3700 0 0 0.0 4360 ## X19625137 38700 37200 124000 0 21000.0 199900 ## X13545 5300 1900 70000 0 15000.0 77200 ## X13725138 24220 10000 70000 0 21000.0 103220 ## X13385 10000 20000 65000 0 147000.0 474000 ## X16935139 9200 0 0 2500 0.0 11700 ## X4930 63000 0 65000 0 0.0 308000 ## X20930 50830 21200 41000 0 28960.0 103070 ## X17100 29300 46400 179000 35000 15000.0 354700 ## X18795 42200 14300 57000 0 3000.0 145500 ## X1315 1550 0 0 0 170.0 1380 ## X3990140 0 0 0 0 0.0 0 ## X6590141 134930 26900 55000 0 141070.0 195760 ## X10940142 293900 9200 250000 0 0.0 553100 ## X17560143 17700 24000 69000 0 101930.0 99770 ## X300 16260 7500 0 0 2000.0 21760 ## X11475 9610 26600 0 0 54880.0 -18670 ## X15370 6640 1700 0 0 1800.0 6540 ## X12230 130 2000 0 0 0.0 2130 ## X6570 400 9200 50000 0 27550.0 157050 ## X13610 90 29600 9200 0 19760.0 29930 ## X1940 14500 4000 0 0 19800.0 -1300 Next, assign to income the column INCOME in the cfb data frame, and determine the mean and median income values. income &lt;- cfb$INCOME mean(income) ## [1] 63402.66 median(income) ## [1] 38032.7 The first output is the mean income and the second is the median income. Mean income is greater than median income. This indicates there are more small income values than large income values, but some of the large income values are very large. This ‘skewness’ in the distribution of values can be seen on a histogram. A histogram is a plot that displays the frequency of the values using intervals that divide the values into equal bins. This is done with the hist() function. Here you specify the number of intervals with the breaks = argument. hist(income, breaks = 25) The distribution is said to be right skewed. It has a long right tail. Note: Some packages come with data sets. To see what data is available in a package, type data(package = &quot;UsingR&quot;) Spread A simple measure of the spread of data values is the range. The range is given by the minimum and maximum value or by the difference between them. range(income) ## [1] 0 1541866 diff(range(income)) ## [1] 1541866 Or using the central tendency as the center of a set of values, you can define spread in terms of deviations from the center. The sum of the squared deviations from the center divided by sample length minus one is the sample variance. var(income) ## [1] 13070833215 sqrt(var(income)) ## [1] 114327.7 sd(income) ## [1] 114327.7 To illustrate consider two sets of test scores. ts1 &lt;- c(80, 85, 75, 77, 87, 82, 88) ts2 &lt;- c(100, 90, 50, 57, 82, 100, 86) Some test score statistics are mean(ts1) ## [1] 82 mean(ts2) ## [1] 80.71429 var(ts1) ## [1] 24.66667 var(ts2) ## [1] 394.2381 All the elements of a vector must have the same type. That is you can’t mix numbers with character strings. Consider the following character strings. simpsons &lt;- c(&quot;Homer&quot;, &quot;Marge&quot;, &quot;Bart&quot;, &quot;Lisa&quot;, &quot;Maggie&quot;) simpsons ## [1] &quot;Homer&quot; &quot;Marge&quot; &quot;Bart&quot; &quot;Lisa&quot; &quot;Maggie&quot; Note that character strings are made with matching quotes, either double, “, or single, ’. If you mix element types within a data vector, all elements will change into the ‘lowest’ common type, which is usually a character. Arithmetic does not work on character elements. Returning to the land falling hurricane counts. cD1 &lt;- c(2, 3, 0, 3, 1, 0, 0, 1, 2, 1) cD2 &lt;- c(0, 5, 4, 2, 3, 0, 3, 3, 2, 1) Now suppose the National Hurricane Center (NHC) reanalyzes a storm, and that the 6th year of the 2nd decade is a 1 rather than a 0 for the number of landfalls. In this case you type cD2[6] &lt;- 1 The assignment to the 6th element in the vector cD2 is done by referencing the 6th element of the vector with square brackets []. It’s important to keep this in mind: Parentheses () are used for functions and square brackets [] are used to get values from vectors (and arrays, lists, etc). REPEAT: [] are used to extract or subset values from vectors, data frames, matrices, etc. Print out all the elements of a data vector, print the 2nd element, the 4th element, all but the 4th element, all odd number elements. cD2 ## [1] 0 5 4 2 3 1 3 3 2 1 cD2[2] ## [1] 5 cD2[4] ## [1] 2 cD2[-4] ## [1] 0 5 4 3 1 3 3 2 1 cD2[c(1, 3, 5, 7, 9)] ## [1] 0 4 3 3 2 R’s console keeps a history of our commands. The previous commands are accessed using the up and down arrow keys. Repeatedly pushing the up arrow will scroll backward through the history so you can reuse previous commands. Many times you wish to change only a small part of a previous command, such as when a typo is made. With the arrow keys you access the previous command then edit it as desired. Structured data When data are in a pattern; for instance the integers 1 through 99. The colon : function is used for creating simple sequences. 1:100 ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 rev(1:100) ## [1] 100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 ## [19] 82 81 80 79 78 77 76 75 74 73 72 71 70 69 68 67 66 65 ## [37] 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 ## [55] 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 ## [73] 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 ## [91] 10 9 8 7 6 5 4 3 2 1 100:1 ## [1] 100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 83 ## [19] 82 81 80 79 78 77 76 75 74 73 72 71 70 69 68 67 66 65 ## [37] 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 ## [55] 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 ## [73] 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 ## [91] 10 9 8 7 6 5 4 3 2 1 It’s often necessary to specify either the step size and the starting and ending points or the starting and ending points and the length of the sequence. The seq() function does this. seq(from = 1, to = 9, by = 2) ## [1] 1 3 5 7 9 seq(from = 1, to = 10, by = 2) ## [1] 1 3 5 7 9 seq(from = 1, to = 9, length = 5) ## [1] 1 3 5 7 9 To create a vector with each element having the same value use the rep() function (replicate). The simplest usage is to replicate the first argument a specified number of times. rep(1, times = 10) ## [1] 1 1 1 1 1 1 1 1 1 1 rep(1:3, times = 3) ## [1] 1 2 3 1 2 3 1 2 3 More complicated patterns can be repeated by specifying pairs of equal-sized vectors. In this case, each element of the first vector is repeated the corresponding number of times specified by the element in the second vector. rep(c(&quot;long&quot;, &quot;short&quot;), times = c(1, 2)) ## [1] &quot;long&quot; &quot;short&quot; &quot;short&quot; To find the most landfalls in the first decade, type: max(cD1) ## [1] 3 Which years had the most? cD1 == 3 ## [1] FALSE TRUE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE Notice the double equals signs (==). This tests each value (element) in cD1 to see if it is equal to 3. The 2nd and 4th values are equal to 3 so TRUEs are returned. Think of this as asking R a question. Is the value equal to 3? R answers all at once with a vector of TRUEs and FALSEs. How do you get the vector element corresponding to the TRUE values? That is, which years have 3 landfalls? which(cD1 == 3) ## [1] 2 4 The function which.max() can be used to get the first maximum. which.max(cD1) ## [1] 2 You might also want to know the total number of landfalls in each decade and the number of years in a decade without a landfall. Or how about the ratio of the mean number of landfalls over the two decades. sum(cD1) ## [1] 13 sum(cD2) ## [1] 24 sum(cD1 == 0) ## [1] 3 sum(cD2 == 0) ## [1] 1 mean(cD2) / mean(cD1) ## [1] 1.846154 There are 85% more landfalls during the second decade. Is this increase statistically significant? To remove an object from the current environment you use the rm() function. Usually not needed unless you have very large objects (e.g., million cases). rm(cD1, cD2) Tables and summaries All elements of a vector must be of the same type. For example, the vectors A, B, and C below are constructed as numeric, logical, and character, respectively. First create the vectors then check the class. A &lt;- c(1, 2.2, 3.6, -2.8) B &lt;- c(TRUE, TRUE, FALSE, TRUE) C &lt;- c(&quot;Cat 1&quot;, &quot;Cat 2&quot;, &quot;Cat 3&quot;) class(A) ## [1] &quot;numeric&quot; class(B) ## [1] &quot;logical&quot; class(C) ## [1] &quot;character&quot; With logical and character vectors the table() function indicates how many occurrences for each element type. For instance, let the vector wx denote the weather conditions for five forecast periods as character data. wx &lt;- c(&quot;sunny&quot;, &quot;clear&quot;, &quot;cloudy&quot;, &quot;cloudy&quot;, &quot;rain&quot;) class(wx) ## [1] &quot;character&quot; table(wx) ## wx ## clear cloudy rain sunny ## 1 2 1 1 The output is a list of the character strings and the corresponding number of occurrences of each string. As another example, let the vector ss denote the Saffir-Simpson category for a set of five hurricanes. ss &lt;- c(&quot;Cat 3&quot;, &quot;Cat 2&quot;, &quot;Cat 1&quot;, &quot;Cat 3&quot;, &quot;Cat 3&quot;) table(ss) ## ss ## Cat 1 Cat 2 Cat 3 ## 1 1 3 Here the character strings correspond to different intensity levels as ordered categories with Cat 1 &lt; Cat 2 &lt; Cat 3. In this case convert the character vector to an ordered factor with levels. This is done with the function factor(). ss &lt;- factor(ss, order = TRUE) class(ss) ## [1] &quot;ordered&quot; &quot;factor&quot; ss ## [1] Cat 3 Cat 2 Cat 1 Cat 3 Cat 3 ## Levels: Cat 1 &lt; Cat 2 &lt; Cat 3 The vector object is now an ordered factor. Printing the object results in a list of the elements in the vector and a list of the levels in order. Note: if you do the same for the wx object, the order is alphabetical by default. Try it. "],["tuesday-september-6-2022.html", "Tuesday, September 6, 2022 Getting your data into R Data frames", " Tuesday, September 6, 2022 Today Getting your data into R Data frames Pipes More information about how to use RStudio and markdown files is available here: https://www.pipinghotdata.com/posts/2020-09-07-introducing-the-rstudio-ide-and-r-markdown/ Getting your data into R You need to know two thing: (1) where the data are located, and (2) what type of data file is it. Consider the file US.txt located in your project folder. It is in the same folder as this file (05-Lesson.Rmd). Click on the file name. It opens a file tab that shows a portion of the file. It is a file with the column headings Year, All, MUS, G, FL, E. Each row is a year and the count is the number of hurricanes making landfall in the United States. All indicates anywhere in the continental U.S, MUS indicates at major hurricane intensity (at least 33 m/s). Each column is separated by a space. To create a data object you use the readr::read_table() function. The only required argument is file =. You put the name of the file in quotes. And set the header argument to TRUE since the first row in the file is not data. LH.df &lt;- readr::read_table(file = &quot;data/US.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## All = col_double(), ## MUS = col_double(), ## G = col_double(), ## FL = col_double(), ## E = col_double() ## ) An data object called LH.df is now in your Environment under Data. In this case the file name is simple because US.txt is in the same directory as your Rmd file. Data files for an analysis are often kept somewhere else. Here for example note the folder called data? Click on the folder name. To read the data from that location you need to change file string name to \"data/US.txt\". LH.df &lt;- readr::read_table(file = &quot;data/US.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## All = col_double(), ## MUS = col_double(), ## G = col_double(), ## FL = col_double(), ## E = col_double() ## ) The file = argument is where R looks for your data. If you get an error message it is likely because the data file is not where you think it is. Note: No changes are made to your original data file. If there are missing values in the data file they should be coded as NA. If they are coded as something else then you specify the coding with the na = argument. For example, if the missing value character in our file is coded as 99, you specify na = \"99\". The readr::read_csv() has settings that are suitable for comma delimited (csv) files that have been exported from a spreadsheet. A work flow might include exporting data from a spreadsheet using the csv file format then importing it to R using the readr::read_csv() function. You import data from the web by specifying the URL instead of the local file name. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/US.txt&quot; LH.df &lt;- readr::read_table(file = loc) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## All = col_double(), ## MUS = col_double(), ## G = col_double(), ## FL = col_double(), ## E = col_double() ## ) Recall that you reference the columns using the $ syntax. For example, type LH.df$FL ## [1] 1 2 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 2 0 2 1 0 1 2 1 0 3 0 2 0 0 0 3 1 ## [38] 2 0 0 0 0 1 2 0 3 1 1 1 0 1 0 1 0 0 2 0 0 1 1 1 0 0 0 1 2 1 0 1 0 1 0 0 2 ## [75] 1 2 0 2 1 0 0 0 2 2 2 1 0 0 1 0 1 2 0 1 2 1 2 2 1 2 0 0 1 0 0 1 0 0 0 1 0 ## [112] 0 0 3 1 2 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 2 0 1 0 1 0 0 1 0 0 2 0 0 2 ## [149] 1 0 0 0 0 4 3 0 0 0 0 0 0 0 0 0 0 1 sum(LH.df$FL) ## [1] 110 The number of years with 0, 1, 2, … Florida hurricanes is obtained by typing table(LH.df$FL) ## ## 0 1 2 3 4 ## 93 43 24 5 1 There are 93 years without a FL hurricane, 43 years with one hurricanes, 24 years with two hurricanes, and so on. Creating structured data files: https://environmentalcomputing.net/getting-started-with-r/ Golden rules of data entry. Convert unstructured data files (e.g., data stored in PDF forms) to structured data. https://www.youtube.com/watch?v=yBkHfIO8YJk Data frames The functions readr::read_table() and readr::read_csv() import data into our environment as a data frame. For example, LH.df is a data frame. You see the data object is a data frame in your Environment. A data frame is like a spreadsheet. Values are arranged in rows and columns. Rows are the cases (observations) and columns are the variables. The dim() function returns the size of the data frame in terms of how many rows (first number) and how many columns. dim(LH.df) ## [1] 166 6 There are 166 rows and 6 columns in the data frame. Note the use of inline code. Open with a single back tick (grave accent) followed by the letter r and close with a single back tick. Inline code allows content in your report to be dynamic. There is no need to retype values when the data changes. Open 05-Lesson.html in a browser. To list the first six lines of the data object, type head(LH.df) ## # A tibble: 6 × 6 ## Year All MUS G FL E ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1851 1 1 0 1 0 ## 2 1852 3 1 1 2 0 ## 3 1853 0 0 0 0 0 ## 4 1854 2 1 1 0 1 ## 5 1855 1 1 1 0 0 ## 6 1856 2 1 1 1 0 The columns include year, number of hurricanes, number of major hurricanes, number of Gulf coast hurricanes, number of Florida hurricanes, and number of East coast hurricanes in order. Column names are printed as well. The last six lines of the data frame are listed similarly using the tail() function. The number of lines listed is changed using the argument n =. tail(LH.df, n = 3) ## # A tibble: 3 × 6 ## Year All MUS G FL E ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2014 1 0 0 0 1 ## 2 2015 0 0 0 0 0 ## 3 2016 2 0 0 1 1 The number of years in the record is assigned to the object nY and the annual average number of hurricanes (rate) is assigned to the object rate. nY &lt;- length(LH.df$All) rate &lt;- mean(LH.df$All) By typing the names of the saved objects, the values are printed. nY ## [1] 166 rate ## [1] 1.668675 Thus over the 166 years of data the average number of hurricanes per year is 1.67. If you want to change the names of the columns in the data frame, type names(LH.df)[4] &lt;- &quot;GC&quot; names(LH.df) ## [1] &quot;Year&quot; &quot;All&quot; &quot;MUS&quot; &quot;GC&quot; &quot;FL&quot; &quot;E&quot; This changes the 4th column name from G to GC. Note that this change occurs to the data frame in R and not to your original data file. You will work almost exclusively with data frames. A data frame has rows and columns. Columns have names Columns are vectors Columns must be of the same length Columns must be of the same data type Each element is indexed by a row number and a column number in that order and separated by a comma. So if df is a data frame then df[2, 3] is the second row of the third column. To print the second row of the first column of the data frame LH.df you type LH.df[2, 1] ## # A tibble: 1 × 1 ## Year ## &lt;dbl&gt; ## 1 1852 If you want all the values in a column, you leave the row number blank. LH.df[ , 1] ## # A tibble: 166 × 1 ## Year ## &lt;dbl&gt; ## 1 1851 ## 2 1852 ## 3 1853 ## 4 1854 ## 5 1855 ## 6 1856 ## 7 1857 ## 8 1858 ## 9 1859 ## 10 1860 ## # … with 156 more rows ## # ℹ Use `print(n = ...)` to see more rows You can also reference the column by name LH.df$Year. Data frames have two indexes indicating the rows and columns in that order. LH.df[10, 4] ## # A tibble: 1 × 1 ## GC ## &lt;dbl&gt; ## 1 3 To a statistician a data frame is a table of observations. Each row contains one observation. Each observation must contain the same variables. These variables are called columns, and you can refer to them by name. You can also refer to the contents of the data frame by row number and column number (like a matrix). To an Excel user a data frame is a worksheet (or a range within a worksheet). A data frame is more restrictive in that each column can only be of one data type (e.g., character, numeric, etc). As an example, consider monthly precipitation from the state of Florida. Source: Monthly climate series. http://www.esrl.noaa.gov/psd/data/timeseries/. Get monthly precipitation values for the state back to the year 1895. Copy/paste into a text editor (notepad) then import using the readr::read_table() function. Here I did it for Florida and put the file on my website. Missing values are coded as -9.900 so you add the argument na = \"-9.900\" to the function. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot; FLp.df &lt;- readr::read_table(loc, na = &quot;-9.900&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## Jan = col_double(), ## Feb = col_double(), ## Mar = col_double(), ## Apr = col_double(), ## May = col_double(), ## Jun = col_double(), ## Jul = col_double(), ## Aug = col_double(), ## Sep = col_double(), ## Oct = col_double(), ## Nov = col_double(), ## Dec = col_double() ## ) Plot a time series graph. library(ggplot2) ggplot(data = FLp.df, aes(x = Year, y = Jan)) + geom_line() + ylab(&quot;Inches&quot;) + ggtitle(label = &quot;January Precipitation in Florida&quot;, subtitle = &quot;1895-2012&quot;) A minimal, complete, reproducible example. Quantiles The median value cuts a set of ordered data values into two equal parts. Values larger than the median and values less than the median. The ordering comes from arranging the data from lowest to highest. Quantiles cut a set of ordered data into arbitrary number of equal-sized parts. The quantile corresponding to cutting the data into two halves is called the median. Fifty percent of the data have values less than or equal to the median value. The median is the 50th percentile (.5 quantile). Quantiles corresponding to cutting the ordered data into quarters are called quartiles. The lower (first) quartile cuts the data into the lower 25% and upper 75% of the data. The lower quartile is the .25 quantile or the 25th percentile indicating that 25% of the data have values less than this quantile value. Correspondingly, the upper (third) quartile corresponding to the .75 quantile (75th percentile), indicates that 75% of the data have values less than this quantile value. The quantile() function calculates quantiles on a vector of data. For example, consider Florida precipitation for the month of June. First apply the sort() function on the June values (column indicated by the label Jun). sort(FLp.df$Jun) ## [1] 2.303 2.445 3.292 3.643 3.673 3.898 3.908 4.089 4.202 4.401 ## [11] 4.500 4.598 4.739 4.747 4.820 4.838 4.965 5.098 5.099 5.160 ## [21] 5.182 5.221 5.321 5.349 5.362 5.422 5.440 5.531 5.588 5.602 ## [31] 5.607 5.614 5.696 5.718 5.724 5.752 5.803 5.866 5.887 5.896 ## [41] 5.931 5.971 5.998 6.142 6.147 6.171 6.220 6.258 6.269 6.281 ## [51] 6.351 6.392 6.392 6.470 6.540 6.541 6.591 6.739 6.789 6.900 ## [61] 6.991 6.998 7.002 7.009 7.012 7.049 7.057 7.098 7.118 7.208 ## [71] 7.306 7.348 7.450 7.451 7.481 7.666 7.707 7.748 7.876 8.000 ## [81] 8.040 8.158 8.168 8.243 8.317 8.378 8.389 8.432 8.488 8.578 ## [91] 8.663 8.874 8.880 8.940 8.969 8.976 9.106 9.308 9.349 9.481 ## [101] 9.734 9.865 9.939 9.993 10.032 10.276 10.280 10.288 10.309 10.360 ## [111] 10.529 10.858 11.014 11.228 11.824 12.034 12.379 Again, note the use of the dollar sign to indicate the column in the data frame. To find the 50th percentile you use the median() function directly or the quantile() function and specify the quantile with the probs = argument. median(FLp.df$Jun) ## [1] 6.789 quantile(FLp.df$Jun, probs = .5) ## 50% ## 6.789 To retrieve the 25th and 75th percentile values quantile(FLp.df$Jun, probs = c(.25, .75)) ## 25% 75% ## 5.602 8.432 Of the 117 monthly precipitation values, 25% of them are less than 5.6 inches, 50% are less than 6.79 inches. Thus there are an equal number of years with June precipitation between 5.6 and 6.79 inches. The difference between the first and third quartile values is called the interquartile range (IQR). Fifty percent of the data values lie within the IQR. The IQR is obtained using the IQR() function. Another example: Consider the set of North Atlantic Oscillation (NAO) index values for the month of June from the period 1851–2010. The NAO is a variation in the climate over the North Atlantic Ocean featuring fluctuations in the difference of atmospheric pressure at sea level between the Iceland and the Azores. The index is computed as the difference in standardized sea-level pressures. The standardization is done by subtracting the mean and dividing by the standard deviation. The index has units of standard deviation. First read the data consisting of monthly NAO values, then list the column names and the first few data lines. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/NAO.txt&quot; NAO.df &lt;- read.table(loc, header = TRUE) head(NAO.df) ## Year Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 1 1851 3.29 1.03 1.50 -1.66 -1.53 -1.62 -5.39 4.68 1.85 0.78 -1.77 1.74 ## 2 1852 1.46 0.41 -2.50 -1.60 0.25 0.09 -1.13 2.94 -2.02 -1.65 -0.93 1.03 ## 3 1853 1.31 -4.04 -0.32 0.76 -3.17 1.09 1.76 -2.36 -0.22 -0.47 0.51 -4.28 ## 4 1854 1.28 1.72 2.67 0.88 0.04 -0.06 -1.92 -0.03 2.62 1.11 -1.56 2.42 ## 5 1855 -1.84 -3.80 -0.05 0.99 -2.28 0.78 -2.61 3.81 0.79 -1.09 -2.42 -1.66 ## 6 1856 -1.25 -0.10 -2.27 2.00 -0.70 2.03 -0.16 -0.44 -0.50 1.12 -1.69 -0.23 Determine the 5th and 95th percentile values for the month of June. quantile(NAO.df$Jun, prob = c(.05, .95)) ## 5% 95% ## -2.808 1.891 The summary() function provides summary statistics for each column in your data frame. The statistics include output the mean, median, minimum, maximum, along with the first quartile and third quartile values. summary(FLp.df) ## Year Jan Feb Mar Apr ## Min. :1895 Min. :0.340 Min. :0.288 Min. :0.496 Min. :0.408 ## 1st Qu.:1924 1st Qu.:1.798 1st Qu.:2.009 1st Qu.:2.142 1st Qu.:1.659 ## Median :1953 Median :2.696 Median :3.099 Median :3.349 Median :2.677 ## Mean :1953 Mean :2.916 Mean :3.164 Mean :3.663 Mean :2.926 ## 3rd Qu.:1982 3rd Qu.:4.010 3rd Qu.:4.171 3rd Qu.:5.097 3rd Qu.:4.163 ## Max. :2011 Max. :8.361 Max. :8.577 Max. :8.701 Max. :7.457 ## May Jun Jul Aug ## Min. :0.900 Min. : 2.303 Min. : 4.050 Min. : 4.053 ## 1st Qu.:2.483 1st Qu.: 5.602 1st Qu.: 6.427 1st Qu.: 6.164 ## Median :3.758 Median : 6.789 Median : 7.522 Median : 7.102 ## Mean :3.845 Mean : 7.046 Mean : 7.505 Mean : 7.345 ## 3rd Qu.:4.765 3rd Qu.: 8.432 3rd Qu.: 8.358 3rd Qu.: 8.310 ## Max. :9.848 Max. :12.379 Max. :11.263 Max. :13.090 ## Sep Oct Nov Dec ## Min. : 2.126 Min. :0.471 Min. :0.370 Min. :0.610 ## 1st Qu.: 4.930 1st Qu.:2.479 1st Qu.:1.370 1st Qu.:1.549 ## Median : 6.680 Median :3.541 Median :2.139 Median :2.558 ## Mean : 6.704 Mean :3.803 Mean :2.308 Mean :2.718 ## 3rd Qu.: 7.955 3rd Qu.:4.899 3rd Qu.:3.110 3rd Qu.:3.521 ## Max. :12.978 Max. :9.556 Max. :6.236 Max. :7.668 Columns with missing values get a row output from the summary() function indicating the number of them (NA’s). Creating a data frame The data.frame() function creates a data frame from a set of vectors. Consider ice volume (10\\(^3\\) km\\(^3\\)) measurements from the arctic from 2002 to 2012. The measurements are taken on January 1st each year and are available from http://psc.apl.washington.edu/wordpress/research/projects/arctic-sea-ice-volume-anomaly/data/ Volume &lt;- c(20.233, 19.659, 18.597, 18.948, 17.820, 16.736, 16.648, 17.068, 15.916, 14.455, 14.569) Since the data have a sequential order you create a data frame with year in the first column and volume in the second. Year &lt;- 2002:2012 Ice.df &lt;- data.frame(Year, Volume) head(Ice.df) ## Year Volume ## 1 2002 20.233 ## 2 2003 19.659 ## 3 2004 18.597 ## 4 2005 18.948 ## 5 2006 17.820 ## 6 2007 16.736 What year had the minimum ice volume? which.min(Ice.df$Volume) ## [1] 10 Ice.df[10, ] ## Year Volume ## 10 2011 14.455 Ice.df$Year[which.min(Ice.df$Volume)] ## [1] 2011 To change a vector to a data frame use the function as.data.frame(). For example, let counts be a vector of integers. counts &lt;- rpois(n = 100, lambda = 1.66) head(counts) ## [1] 1 2 2 3 0 3 H.df &lt;- as.data.frame(counts) head(H.df) ## counts ## 1 1 ## 2 2 ## 3 2 ## 4 3 ## 5 0 ## 6 3 The column name in the data frame is the name of the vector. So far you have computed statistics on data stored as vectors (mean, median, quantiles, etc). But you often import data as data frames so you need to know how to manipulate them. The {dplyr} package has functions (‘verbs’) that manipulate data frames in a friendly and logical way. Manipulations include, selecting columns, filtering rows, re-ordering rows, adding new columns, and summarizing data. library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:Hmisc&#39;: ## ## src, summarize ## The following object is masked from &#39;package:MASS&#39;: ## ## select ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union Let’s look at these using the airquality data frame. Recall the object airquality is a data frame containing New York air quality measurements from May to September 1973. (?airquality). head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 dim(airquality) ## [1] 153 6 The columns include Ozone (ozone concentration in ppb), Solar.R (solar radiation in langleys), Wind (wind speed in mph), Temp (air temperature in degrees F), Month, and Day. You summarize the values in each column with the summary() method. summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Note that columns that have missing values are tabulated. For example, there are 37 missing ozone measurements and 7 missing radiation measurements. Importantly you can apply the summary() function using the pipe operator (|&gt; or %&gt;%). The pipe operator is part of the {dplyr} package. airquality |&gt; summary() ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## You read the pipe as THEN. “airquality data frame THEN summarize”. The pipe operator allows you to string together a bunch of functions that makes it easy for humans to understand what was done. This is a key point. You want your code to be readable by a computer (correct syntax) but also readable to other humans. For example, suppose the object of interest is called me and suppose there is a function called wake_up(). I could apply the function in two ways. wake_up(me) me |&gt; wake_up() The second way involves a bit more typing but it is easier for a human to read and thus it is easier to understand. This becomes clear when stringing together many functions. Consider again the FLp.df. How would you use the above syntax to compute the mean value of June precipitation? You ask three questions: what function, applied to what variable, from what data frame? Answers: mean(), Jun, FLp.df. You then write the code starting with the answer to the last question first. FLp.df |&gt; pull(Jun) ## [1] 4.500 11.228 5.221 3.292 5.803 9.993 10.360 6.220 7.012 6.591 ## [11] 5.160 8.040 6.392 6.351 6.739 10.288 4.820 12.379 5.531 4.202 ## [21] 5.321 6.541 5.362 5.349 7.481 6.258 3.673 6.540 9.308 6.470 ## [31] 6.281 8.168 7.450 7.057 8.158 10.858 2.303 8.378 5.182 9.865 ## [41] 5.099 8.940 5.931 6.998 9.734 7.049 7.707 10.529 7.348 5.607 ## [51] 8.578 7.098 9.106 3.908 8.000 4.089 4.747 3.643 7.876 5.588 ## [61] 6.392 5.422 7.748 6.147 8.389 6.789 5.896 8.317 7.118 5.614 ## [71] 10.032 8.880 8.488 9.939 6.142 5.866 5.602 8.432 5.887 10.276 ## [81] 6.269 7.002 4.401 6.900 3.898 4.838 5.718 10.280 8.969 5.098 ## [91] 7.009 7.451 5.696 4.739 8.976 5.724 7.666 12.034 4.598 9.349 ## [101] 8.874 7.306 7.208 2.445 9.481 5.971 8.663 10.309 11.014 8.243 ## [111] 11.824 5.752 5.998 6.991 6.171 5.440 4.965 The function pull() from the {dplyr} packages pulls out the column named Jun as a vector. Then the mean() function takes these 118 values and computes the average. FLp.df |&gt; pull(Jun) |&gt; mean() ## [1] 7.045692 Note that the next function in the sequence receives the output from the previous function as its FIRST argument so the function mean() has nothing inside the parentheses. Your turn Use the piping operator and compute the average wind speed in the airquality data frame. Use the piping operator and compute the 10th and 90th percentiles (lower and upper decile values) of May precipitation in Florida. "],["thursday-september-8-2022.html", "Thursday, September 8, 2022 Using pipes Wrangling data frames", " Thursday, September 8, 2022 Today Using pipes Wrangling data Using pipes Data wrangling (munging) is the process of transforming data from one format into another to make it easier to interpret it. The {dplyr} package includes functions that wrangle data frames in a logical way. Key idea: The functions operate on data frames and return data frames. Operations include selecting columns, filtering rows, re-ordering rows, adding new columns, and summarizing data. library(dplyr) Recall the object airquality is a data frame containing New York air quality measurements from May to September 1973. (?airquality). You get a statistical summary of the values in each column with the summary() method. summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Importantly you can apply the summary() function using the pipe operator (|&gt;). The pipe operator is part of the {dplyr} package and when used together with the wrangling functions, it provides a easy way to make code easy to read. For example, you read the pipe as THEN. “airquality data frame THEN summarize”. airquality |&gt; summary() ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## The pipe operator allows you to string together functions while keeping the code readable. You want your code to be machine readable (correct syntax) but also human readable. For example, suppose the object of interest is called me and suppose there is a function called wake_up(). I can apply the function in two ways. wake_up(me) me |&gt; wake_up() The second way involves a bit more typing but it is easier for someone to read and thus it is easier to understand. This becomes clear when stringing together many functions. For example, what happens to the result of me after the function wake_up() has been applied? How about get_out_of_bed() and then get_dressed()? I can apply these functions in two ways. get_dressed(get_out_of_bed(wake_up(me))) me |&gt; wake_up() |&gt; get_out_of_bed() |&gt; get_dressed() Continuing me |&gt; wake_up() |&gt; get_out_of_bed() |&gt; get_dressed() |&gt; make_coffee() |&gt; drink_coffee() |&gt; leave_house() Which is much better in terms of ‘readability’ then leave_house(drink_coffee(make_coffee(get_dressed(get_out_of_bed(wake_up(me)))))). Consider again the FLp.df. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot; FLp.df &lt;- read.table(loc, header = TRUE, na.string = &quot;-9.900&quot;) How would you use the above readable syntax to compute the mean value of June precipitation? You ask three questions: what function, applied to what variable, from what data frame? Answers: mean(), Jun, FLp.df. You then write the code starting with the answer to the last question first. FLp.df |&gt; pull(Jun) ## [1] 4.500 11.228 5.221 3.292 5.803 9.993 10.360 6.220 7.012 6.591 ## [11] 5.160 8.040 6.392 6.351 6.739 10.288 4.820 12.379 5.531 4.202 ## [21] 5.321 6.541 5.362 5.349 7.481 6.258 3.673 6.540 9.308 6.470 ## [31] 6.281 8.168 7.450 7.057 8.158 10.858 2.303 8.378 5.182 9.865 ## [41] 5.099 8.940 5.931 6.998 9.734 7.049 7.707 10.529 7.348 5.607 ## [51] 8.578 7.098 9.106 3.908 8.000 4.089 4.747 3.643 7.876 5.588 ## [61] 6.392 5.422 7.748 6.147 8.389 6.789 5.896 8.317 7.118 5.614 ## [71] 10.032 8.880 8.488 9.939 6.142 5.866 5.602 8.432 5.887 10.276 ## [81] 6.269 7.002 4.401 6.900 3.898 4.838 5.718 10.280 8.969 5.098 ## [91] 7.009 7.451 5.696 4.739 8.976 5.724 7.666 12.034 4.598 9.349 ## [101] 8.874 7.306 7.208 2.445 9.481 5.971 8.663 10.309 11.014 8.243 ## [111] 11.824 5.752 5.998 6.991 6.171 5.440 4.965 The function pull() from the {dplyr} packages pulls out the column named Jun and returns a vector of the values. Then the mean() function takes these 118 values and computes the average. FLp.df |&gt; pull(Jun) |&gt; mean() ## [1] 7.045692 IMPORTANT: the next function in the sequence receives the output from the previous function as its FIRST argument so the function mean() has nothing inside the parentheses. Use the piping operator and compute the average wind speed in the airquality data frame. airquality |&gt; pull(Wind) |&gt; mean() ## [1] 9.957516 Use the piping operator and compute the 10th and 90th percentiles (lower and upper decile values) of May precipitation in Florida. FLp.df |&gt; pull(May) |&gt; quantile(probs = c(.1, .9)) ## 10% 90% ## 1.7954 6.0828 Wrangling data frames You will wrangle data with functions from the {dplyr} package. The functions work on data frames but they work better if the data frame is a tibble. Tibbles are data frames that make life a little easier. R is an old language, and some things that were useful 10 or 20 years ago now get in the way. To make a data frame a tibble (tabular data frame) type airquality &lt;- as_tibble(airquality) class(airquality) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Click on airquality in the environment. It is a data frame. Selecting and filtering The function select() chooses variables by name to create a data frame with fewer columns. For example, choose the month, day, and temperature columns from the airquality data frame. airquality |&gt; dplyr::select(Month, Day, Temp) ## # A tibble: 153 × 3 ## Month Day Temp ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 5 1 67 ## 2 5 2 72 ## 3 5 3 74 ## 4 5 4 62 ## 5 5 5 56 ## 6 5 6 66 ## 7 5 7 65 ## 8 5 8 59 ## 9 5 9 61 ## 10 5 10 69 ## # … with 143 more rows ## # ℹ Use `print(n = ...)` to see more rows Suppose you want a new data frame with only the temperature and ozone concentrations. df &lt;- airquality |&gt; dplyr::select(Temp, Ozone) df ## # A tibble: 153 × 2 ## Temp Ozone ## &lt;int&gt; &lt;int&gt; ## 1 67 41 ## 2 72 36 ## 3 74 12 ## 4 62 18 ## 5 56 NA ## 6 66 28 ## 7 65 23 ## 8 59 19 ## 9 61 8 ## 10 69 NA ## # … with 143 more rows ## # ℹ Use `print(n = ...)` to see more rows You include an assignment operator (&lt;-, left pointing arrow) and an object name (here df). Note: The result of applying most {dplyr} verbs is a data frame. The take only data frames and return only data frames. The function filter() chooses observations based on specific values. Suppose you want only the observations where the temperature is at or above 80F. airquality |&gt; dplyr::filter(Temp &gt;= 80) ## # A tibble: 73 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 45 252 14.9 81 5 29 ## 2 NA 186 9.2 84 6 4 ## 3 NA 220 8.6 85 6 5 ## 4 29 127 9.7 82 6 7 ## 5 NA 273 6.9 87 6 8 ## 6 71 291 13.8 90 6 9 ## 7 39 323 11.5 87 6 10 ## 8 NA 259 10.9 93 6 11 ## 9 NA 250 9.2 92 6 12 ## 10 23 148 8 82 6 13 ## # … with 63 more rows ## # ℹ Use `print(n = ...)` to see more rows The result is a data frame with the same 6 columns but now only 73 observations. Each of the observations has a temperature of at least 80F. Suppose you want a new data frame keeping only observations where temperature is at least 80F AND winds less than 5 mph. df &lt;- airquality |&gt; dplyr::filter(Temp &gt;= 80 &amp; Wind &lt; 5) df ## # A tibble: 8 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 135 269 4.1 84 7 1 ## 2 64 175 4.6 83 7 5 ## 3 66 NA 4.6 87 8 6 ## 4 122 255 4 89 8 7 ## 5 168 238 3.4 81 8 25 ## 6 118 225 2.3 94 8 29 ## 7 73 183 2.8 93 9 3 ## 8 91 189 4.6 93 9 4 Example: Palmer penguins Let’s return to the penguins data set. The data set is located on the web, and you import it as a data frame using the readr::read_csv() function. loc &lt;- &quot;https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv&quot; penguins &lt;- readr::read_csv(loc) ## Rows: 344 Columns: 8 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): species, island, sex ## dbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. penguins ## # A tibble: 344 × 8 ## species island bill_length_mm bill_depth_mm flipper_…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## 7 Adelie Torgersen 38.9 17.8 181 3625 fema… 2007 ## 8 Adelie Torgersen 39.2 19.6 195 4675 male 2007 ## 9 Adelie Torgersen 34.1 18.1 193 3475 &lt;NA&gt; 2007 ## 10 Adelie Torgersen 42 20.2 190 4250 &lt;NA&gt; 2007 ## # … with 334 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows To keep only the penguins labeled in the column sex as female type penguins |&gt; dplyr::filter(sex == &quot;female&quot;) ## # A tibble: 165 × 8 ## species island bill_length_mm bill_depth_mm flipper_…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 2 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 3 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 4 Adelie Torgersen 38.9 17.8 181 3625 fema… 2007 ## 5 Adelie Torgersen 41.1 17.6 182 3200 fema… 2007 ## 6 Adelie Torgersen 36.6 17.8 185 3700 fema… 2007 ## 7 Adelie Torgersen 38.7 19 195 3450 fema… 2007 ## 8 Adelie Torgersen 34.4 18.4 184 3325 fema… 2007 ## 9 Adelie Biscoe 37.8 18.3 174 3400 fema… 2007 ## 10 Adelie Biscoe 35.9 19.2 189 3800 fema… 2007 ## # … with 155 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows To filter rows keeping only species that are not Adalie penguins. penguins |&gt; dplyr::filter(species != &quot;Adelie&quot;) ## # A tibble: 192 × 8 ## species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gentoo Biscoe 46.1 13.2 211 4500 fema… 2007 ## 2 Gentoo Biscoe 50 16.3 230 5700 male 2007 ## 3 Gentoo Biscoe 48.7 14.1 210 4450 fema… 2007 ## 4 Gentoo Biscoe 50 15.2 218 5700 male 2007 ## 5 Gentoo Biscoe 47.6 14.5 215 5400 male 2007 ## 6 Gentoo Biscoe 46.5 13.5 210 4550 fema… 2007 ## 7 Gentoo Biscoe 45.4 14.6 211 4800 fema… 2007 ## 8 Gentoo Biscoe 46.7 15.3 219 5200 male 2007 ## 9 Gentoo Biscoe 43.3 13.4 209 4400 fema… 2007 ## 10 Gentoo Biscoe 46.8 15.4 215 5150 male 2007 ## # … with 182 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows When the column of interest is a numerical, you can filter rows by using greater than condition. For example, to create a data frame containing the heaviest penguins you filter keeping only rows with body mass greater than 6000 g. penguins |&gt; dplyr::filter(body_mass_g &gt; 6000) ## # A tibble: 2 × 8 ## species island bill_length_mm bill_depth_mm flipper_leng…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 ## 2 Gentoo Biscoe 59.6 17 230 6050 male 2007 ## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g You can also filter rows of a data frame with less than condition. For example, to create a data frame containing only penguins with short flippers you filter keeping only rows with flipper length less than 175 mm. penguins |&gt; dplyr::filter(flipper_length_mm &lt; 175) ## # A tibble: 2 × 8 ## species island bill_length_mm bill_depth_mm flipper_leng…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Biscoe 37.8 18.3 174 3400 fema… 2007 ## 2 Adelie Biscoe 37.9 18.6 172 3150 fema… 2007 ## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g You can also specify more than one conditions. For example to create a data frame with female penguins that have larger flippers you filter keeping only rows with flipper length greater than 220 mm and with sex equal to female. penguins |&gt; dplyr::filter(flipper_length_mm &gt; 220 &amp; sex == &quot;female&quot;) ## # A tibble: 1 × 8 ## species island bill_length_mm bill_depth_mm flipper_leng…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gentoo Biscoe 46.9 14.6 222 4875 fema… 2009 ## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g You can also filter a data frame for rows satisfying one of the two conditions using OR. For example to create a data frame with penguins have large flippers or short bills you filter keeping rows with flipper length of at least 220 mm or with bill depth less than 10 mm. penguins |&gt; dplyr::filter(flipper_length_mm &gt; 220 | bill_depth_mm &lt; 10) ## # A tibble: 35 × 8 ## species island bill_length_mm bill_depth_mm flipper_len…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Gentoo Biscoe 50 16.3 230 5700 male 2007 ## 2 Gentoo Biscoe 49.2 15.2 221 6300 male 2007 ## 3 Gentoo Biscoe 48.7 15.1 222 5350 male 2007 ## 4 Gentoo Biscoe 47.3 15.3 222 5250 male 2007 ## 5 Gentoo Biscoe 59.6 17 230 6050 male 2007 ## 6 Gentoo Biscoe 49.6 16 225 5700 male 2008 ## 7 Gentoo Biscoe 50.5 15.9 222 5550 male 2008 ## 8 Gentoo Biscoe 50.5 15.9 225 5400 male 2008 ## 9 Gentoo Biscoe 50.1 15 225 5000 male 2008 ## 10 Gentoo Biscoe 50.4 15.3 224 5550 male 2008 ## # … with 25 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows Often you want to remove rows if one of the columns has a missing value. With is.na() on the column of interest, you can filter rows based on whether or not a column value is missing. Note the is.na() function returns a vector of TRUEs and FALSEs is.na(airquality$Ozone) ## [1] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE ## [37] TRUE FALSE TRUE FALSE FALSE TRUE TRUE FALSE TRUE TRUE FALSE FALSE ## [49] FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [61] TRUE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [73] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [97] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE TRUE FALSE ## [109] FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE TRUE FALSE ## [121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [145] FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE The first four rows of the vector Ozone in the airquality data frame are not missing so the function is.na() returns four FALSEs. When you combine that with the filter() function you get a data frame containing all the rows where is.na() returns a TRUE. For example, create a data frame containing rows where the bill length value is missing. penguins |&gt; dplyr::filter(is.na(bill_length_mm)) ## # A tibble: 2 × 8 ## species island bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 2 Gentoo Biscoe NA NA NA NA &lt;NA&gt; 2009 ## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g Usually you will want to do the reverse of this. That is keep all the rows where the column value is not missing. In this case use negation symbol ! to reverse the selection. In this example, filter rows with no missing values for sex column. penguins |&gt; dplyr::filter(!is.na(sex)) ## # A tibble: 333 × 8 ## species island bill_length_mm bill_depth_mm flipper_…¹ body_…² sex year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 4 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 5 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## 6 Adelie Torgersen 38.9 17.8 181 3625 fema… 2007 ## 7 Adelie Torgersen 39.2 19.6 195 4675 male 2007 ## 8 Adelie Torgersen 41.1 17.6 182 3200 fema… 2007 ## 9 Adelie Torgersen 38.6 21.2 191 3800 male 2007 ## 10 Adelie Torgersen 34.6 21.1 198 4400 male 2007 ## # … with 323 more rows, and abbreviated variable names ¹​flipper_length_mm, ## # ²​body_mass_g ## # ℹ Use `print(n = ...)` to see more rows Note that this filtering will keep rows with other column values that are missing values but there will be no penguins where the sex value is NA. Stringing functions together The function arrange() orders the rows by values given in a particular column. airquality |&gt; dplyr::arrange(Solar.R) ## # A tibble: 153 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 16 7 6.9 74 7 21 ## 2 1 8 9.7 59 5 21 ## 3 23 13 12 67 5 28 ## 4 23 14 9.2 71 9 22 ## 5 8 19 20.1 61 5 9 ## 6 14 20 16.6 63 9 25 ## 7 9 24 13.8 81 8 2 ## 8 9 24 10.9 71 9 14 ## 9 4 25 9.7 61 5 23 ## 10 13 27 10.3 76 9 18 ## # … with 143 more rows ## # ℹ Use `print(n = ...)` to see more rows The ordering is from lowest value to highest value. Here the first 10 rows. Note Month and Day are no longer chronological. Repeat but order by the value of air temperature. airquality |&gt; dplyr::arrange(Temp) ## # A tibble: 153 × 6 ## Ozone Solar.R Wind Temp Month Day ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 NA NA 14.3 56 5 5 ## 2 6 78 18.4 57 5 18 ## 3 NA 66 16.6 57 5 25 ## 4 NA NA 8 57 5 27 ## 5 18 65 13.2 58 5 15 ## 6 NA 266 14.9 58 5 26 ## 7 19 99 13.8 59 5 8 ## 8 1 8 9.7 59 5 21 ## 9 8 19 20.1 61 5 9 ## 10 4 25 9.7 61 5 23 ## # … with 143 more rows ## # ℹ Use `print(n = ...)` to see more rows Importantly you can string the functions together. For example select the variables radiation, wind, and temperature then filter by temperatures above 90F and arrange from coolest to warmest by temperature. airquality |&gt; dplyr::select(Solar.R, Wind, Temp) |&gt; dplyr::filter(Temp &gt; 90) |&gt; dplyr::arrange(Temp) ## # A tibble: 14 × 3 ## Solar.R Wind Temp ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 291 14.9 91 ## 2 167 6.9 91 ## 3 250 9.2 92 ## 4 267 6.3 92 ## 5 272 5.7 92 ## 6 222 8.6 92 ## 7 197 5.1 92 ## 8 259 10.9 93 ## 9 183 2.8 93 ## 10 189 4.6 93 ## 11 225 2.3 94 ## 12 188 6.3 94 ## 13 237 6.3 96 ## 14 203 9.7 97 The result is a data frame with three columns and 14 rows arranged by increasing temperatures above 90F. The mutate() function adds new columns to the data frame. For example, create a new column called TempC as the temperature in degrees Celcius. Also create a column called WindMS as the wind speed in meters per second. airquality |&gt; dplyr::mutate(TempC = (Temp - 32) * 5/9, WindMS = Wind * .44704) ## # A tibble: 153 × 8 ## Ozone Solar.R Wind Temp Month Day TempC WindMS ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 41 190 7.4 67 5 1 19.4 3.31 ## 2 36 118 8 72 5 2 22.2 3.58 ## 3 12 149 12.6 74 5 3 23.3 5.63 ## 4 18 313 11.5 62 5 4 16.7 5.14 ## 5 NA NA 14.3 56 5 5 13.3 6.39 ## 6 28 NA 14.9 66 5 6 18.9 6.66 ## 7 23 299 8.6 65 5 7 18.3 3.84 ## 8 19 99 13.8 59 5 8 15 6.17 ## 9 8 19 20.1 61 5 9 16.1 8.99 ## 10 NA 194 8.6 69 5 10 20.6 3.84 ## # … with 143 more rows ## # ℹ Use `print(n = ...)` to see more rows The resulting data frame has 8 columns (two new ones) labeled TempC and WindMS. On days when the temperature is below 60 F add a column giving the apparent temperature based on the cooling effect of the wind (wind chill) and then arrange from coldest to warmest apparent temperature. airquality |&gt; dplyr::filter(Temp &lt; 60) |&gt; dplyr::mutate(TempAp = 35.74 + .6215 * Temp - 35.75 * Wind^.16 + .4275 * Temp * Wind^.16) |&gt; dplyr::arrange(TempAp) ## # A tibble: 8 × 7 ## Ozone Solar.R Wind Temp Month Day TempAp ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 NA NA 14.3 56 5 5 52.5 ## 2 6 78 18.4 57 5 18 53.0 ## 3 NA 66 16.6 57 5 25 53.3 ## 4 NA 266 14.9 58 5 26 54.9 ## 5 18 65 13.2 58 5 15 55.2 ## 6 NA NA 8 57 5 27 55.3 ## 7 19 99 13.8 59 5 8 56.4 ## 8 1 8 9.7 59 5 21 57.3 Summarize The summarize() function reduces (flattens) the data frame based on a function that computes a statistic. For example, to compute the average wind speed during July type airquality |&gt; dplyr::filter(Month == 7) |&gt; dplyr::summarize(Wavg = mean(Wind)) ## # A tibble: 1 × 1 ## Wavg ## &lt;dbl&gt; ## 1 8.94 airquality |&gt; dplyr::filter(Month == 6) |&gt; dplyr::summarize(Tavg = mean(Temp)) ## # A tibble: 1 × 1 ## Tavg ## &lt;dbl&gt; ## 1 79.1 We have seen functions that compute statistics on vectors including sum(), sd(), min(), max(), var(), range(), median(). Others include Summary function Description dplyr::n() Length of the column dplyr::first() First value of the column dplyr::last() Last value of the column dplyr::n_distinct() Number of distinct values Find the maximum and median wind speed and maximum ozone concentration values during the month of May. Also determine the number of observations during May. airquality |&gt; dplyr::filter(Month == 5) |&gt; dplyr::summarize(Wmax = max(Wind), Wmed = median(Wind), OzoneMax = max(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 1 × 4 ## Wmax Wmed OzoneMax NumDays ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 20.1 11.5 115 31 Why do you get an NA for OzoneMax? Fix this by including the argument na.rm = TRUE inside the max() function. airquality |&gt; dplyr::filter(Month == 5) |&gt; dplyr::summarize(Wmax = max(Wind), Wmed = median(Wind), OzoneMax = max(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 1 × 4 ## Wmax Wmed OzoneMax NumDays ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 20.1 11.5 115 31 Grouping by column name If you want to summarize separately for each month you use the group_by() function. You split the data frame by some variable (e.g., Month), apply a function to the individual data frames, and then combine the output. Find the highest ozone concentration by month. Include the number of observations (days) in the month. airquality |&gt; dplyr::group_by(Month) |&gt; dplyr::summarize(OzoneMax = max(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 5 × 3 ## Month OzoneMax NumDays ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 5 115 31 ## 2 6 71 30 ## 3 7 135 31 ## 4 8 168 31 ## 5 9 96 30 Find the average ozone concentration when temperatures are above and below 70 F. Include the number of observations (days) in the two groups. airquality |&gt; dplyr::group_by(Temp &gt;= 70) |&gt; dplyr::summarize(OzoneAvg = mean(Ozone, na.rm = TRUE), NumDays = dplyr::n()) ## # A tibble: 2 × 3 ## `Temp &gt;= 70` OzoneAvg NumDays ## &lt;lgl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 FALSE 18.0 32 ## 2 TRUE 49.1 121 On average ozone concentration is higher on warm days (Temp &gt;= 70 F) days. Said another way; mean ozone concentration statistically depends on temperature. The mean is a model for the data. The statistical dependency of the mean implies that a model for ozone concentration will likely be improved by including temperature as an explanatory variable. To summarize, the important verbs are Verb Description dplyr::select() selects columns; pick variables by their names dplyr::filter() filters rows; pick observations by their values dplyr::mutate() creates new columns; create new variables with functions of existing variables dplyr::summarize() summarizes values; collapse many values down to a single summary dplyr::group_by() allows operations to be grouped The syntax of the verb functions are all the same: Properties * The first argument is a data frame. This argument is implicit when using the |&gt; operator. * The subsequent arguments describe what to do with the data frame. You refer to columns in the data frame directly (without using $). * The result is a new data frame These properties make it easy to chain together many simple lines of code to do something complex. The five functions form the basis of a grammar for data. At the most basic level, you can only alter a data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise()). Your turn Consider again the Florida precipitation data set (http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt). Import the data as a data frame, select the columns April and Year, group by years &gt; 1960, then compute the mean and variance of the April rainfall with the summarize() function. "],["tuesday-september-13-2022.html", "Tuesday, September 13, 2022 0.1 Data munging examples Putting things together", " Tuesday, September 13, 2022 Today Examples of data munging Putting things together 0.1 Data munging examples You work with data frames. The functions are verbs. The verbs include: Verb Description dplyr::select() selects columns; pick variables by their names dplyr::filter() filters rows; pick observations by their values dplyr::arrange() reorders rows dplyr::mutate() creates new columns; create new variables with functions of existing variables dplyr::summarize() summarizes values; collapse many values down to a single summary dplyr::group_by() allows operations to be grouped Syntax for the verb functions are the same: Properties * The first argument is a data frame. This argument is implied when using the |&gt; (pipe) operator (also %&gt;%). * The subsequent arguments describe what to do with the data frame. You refer to columns in the data frame directly (without using $). * The result is a new data frame The properties make it easy to chain together simple lines of code to do something complex. The five functions form the basis of a grammar for data. At the most basic level, you can alter a data frame in five useful ways: you can reorder the rows (arrange()), pick observations and variables of interest (filter() and select()), add new variables that are functions of existing variables (mutate()), or collapse many values to a summary (summarise()). As a review consider again the Florida precipitation data set (http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt). Import the data as a data frame, select the columns April and Year, group by years &gt; 1960, then summarize by computing the mean and variance of the April rainfall. FLp.df &lt;- readr::read_table(file = &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## Jan = col_double(), ## Feb = col_double(), ## Mar = col_double(), ## Apr = col_double(), ## May = col_double(), ## Jun = col_double(), ## Jul = col_double(), ## Aug = col_double(), ## Sep = col_double(), ## Oct = col_double(), ## Nov = col_double(), ## Dec = col_double() ## ) FLp.df |&gt; dplyr::select(Apr, Year) |&gt; dplyr::group_by(Year &gt; 1960) |&gt; dplyr::summarize(Avg = mean(Apr), Var = var(Apr)) ## # A tibble: 2 × 3 ## `Year &gt; 1960` Avg Var ## &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 FALSE 3.14 2.61 ## 2 TRUE 2.66 2.07 Example: New York City flight data Let’s consider the flights data frame from the package {nycflights13}. library(nycflights13) dim(flights) ## [1] 336776 19 The data contains all 336,776 flights that departed NYC in 2013 and comes from the U.S. Bureau of Transportation Statistics. More information is available by typing ?nycflights13. The object flights is a tibble (tabled data frame). When we have a large data frame it is useful to make it a tibble. head(flights) ## # A tibble: 6 × 19 ## year month day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 1 517 515 2 830 819 11 UA ## 2 2013 1 1 533 529 4 850 830 20 UA ## 3 2013 1 1 542 540 2 923 850 33 AA ## 4 2013 1 1 544 545 -1 1004 1022 -18 B6 ## 5 2013 1 1 554 600 -6 812 837 -25 DL ## 6 2013 1 1 554 558 -4 740 728 12 UA ## # … with 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt;, and abbreviated variable names ¹​sched_dep_time, ## # ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay ## # ℹ Use `colnames()` to see all variable names The function filter() selects a set of rows in a data frame. How would you select all flights occurring on February 1st? flights |&gt; dplyr::filter(month == 2 &amp; day == 1) ## # A tibble: 926 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 2 1 456 500 -4 652 648 4 US ## 2 2013 2 1 520 525 -5 816 820 -4 UA ## 3 2013 2 1 527 530 -3 837 829 8 UA ## 4 2013 2 1 532 540 -8 1007 1017 -10 B6 ## 5 2013 2 1 540 540 0 859 850 9 AA ## 6 2013 2 1 552 600 -8 714 715 -1 EV ## 7 2013 2 1 552 600 -8 919 910 9 AA ## 8 2013 2 1 552 600 -8 655 709 -14 B6 ## 9 2013 2 1 553 600 -7 833 815 18 FL ## 10 2013 2 1 553 600 -7 821 825 -4 MQ ## # … with 916 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names The function arrange() reorders the rows. If you provide more than one column name as arguments, each additional column is used to break ties in the values of the preceding columns. How would you arrange all flights in descending order of departure delay? flights |&gt; dplyr::arrange(desc(dep_delay)) ## # A tibble: 336,776 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 1 9 641 900 1301 1242 1530 1272 HA ## 2 2013 6 15 1432 1935 1137 1607 2120 1127 MQ ## 3 2013 1 10 1121 1635 1126 1239 1810 1109 MQ ## 4 2013 9 20 1139 1845 1014 1457 2210 1007 AA ## 5 2013 7 22 845 1600 1005 1044 1815 989 MQ ## 6 2013 4 10 1100 1900 960 1342 2211 931 DL ## 7 2013 3 17 2321 810 911 135 1020 915 DL ## 8 2013 6 27 959 1900 899 1236 2226 850 DL ## 9 2013 7 22 2257 759 898 121 1026 895 DL ## 10 2013 12 5 756 1700 896 1058 2020 878 AA ## # … with 336,766 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Often you work with large data sets with many columns but only a few are of interest. The function select() allows us to zoom in on an interesting subset of the columns. How would you create a data frame containing only the dates, carrier, and flight numbers? df &lt;- flights |&gt; dplyr::select(year:day, carrier, flight) df ## # A tibble: 336,776 × 5 ## year month day carrier flight ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2013 1 1 UA 1545 ## 2 2013 1 1 UA 1714 ## 3 2013 1 1 AA 1141 ## 4 2013 1 1 B6 725 ## 5 2013 1 1 DL 461 ## 6 2013 1 1 UA 1696 ## 7 2013 1 1 B6 507 ## 8 2013 1 1 EV 5708 ## 9 2013 1 1 B6 79 ## 10 2013 1 1 AA 301 ## # … with 336,766 more rows ## # ℹ Use `print(n = ...)` to see more rows Note here the sequence operator : to get all columns between the column labeled year and the column labeled day. How many distinct carriers are there? df |&gt; dplyr::distinct(carrier) |&gt; nrow() ## [1] 16 You include new columns with the function mutate(). Compute the time gained during flight by subtracting the departure delay (minutes) from the arrival delay. flights |&gt; dplyr::mutate(gain = arr_delay - dep_delay) |&gt; dplyr::select(year:day, carrier, flight, gain) |&gt; dplyr::arrange(desc(gain)) ## # A tibble: 336,776 × 6 ## year month day carrier flight gain ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2013 11 1 VX 399 196 ## 2 2013 4 18 AA 707 181 ## 3 2013 8 8 UA 996 165 ## 4 2013 7 10 DL 1465 161 ## 5 2013 6 27 MQ 3199 157 ## 6 2013 7 22 DL 1619 154 ## 7 2013 7 1 DL 2395 153 ## 8 2013 7 10 EV 4580 150 ## 9 2013 7 22 MQ 2793 150 ## 10 2013 4 18 AA 2083 148 ## # … with 336,766 more rows ## # ℹ Use `print(n = ...)` to see more rows Determine the average departure delay. flights |&gt; dplyr::summarize(avgDelay = mean(dep_delay, na.rm = TRUE)) ## # A tibble: 1 × 1 ## avgDelay ## &lt;dbl&gt; ## 1 12.6 Note that if there are missing values in a vector the function mean() needs to include the argument na.rm = TRUE otherwise the output will be NA. y &lt;- c(5, 6, 7, NA) mean(y) ## [1] NA mean(y, na.rm = TRUE) ## [1] 6 You use sample_n() and sample_frac() to take random sample of rows from the data frame. Take a random sample of five rows from the flights data frame. flights |&gt; dplyr::sample_n(5) ## # A tibble: 5 × 19 ## year month day dep_time sched_dep…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 11 14 1547 1550 -3 1733 1745 -12 9E ## 2 2013 9 11 1229 1238 -9 1319 1354 -35 EV ## 3 2013 7 31 1451 1452 -1 1725 1747 -22 UA ## 4 2013 1 10 1145 1145 0 1322 1321 1 FL ## 5 2013 4 27 941 950 -9 1230 1252 -22 B6 ## # … with 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, origin &lt;chr&gt;, ## # dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, ## # time_hour &lt;dttm&gt;, and abbreviated variable names ¹​sched_dep_time, ## # ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay ## # ℹ Use `colnames()` to see all variable names Take a random sample of 1% of the rows. flights |&gt; dplyr::sample_frac(.01) ## # A tibble: 3,368 × 19 ## year month day dep_time sched_de…¹ dep_d…² arr_t…³ sched…⁴ arr_d…⁵ carrier ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2013 11 27 1854 1900 -6 2132 2131 1 DL ## 2 2013 1 25 552 600 -8 644 709 -25 B6 ## 3 2013 1 9 658 700 -2 834 839 -5 UA ## 4 2013 4 19 1805 1800 5 1914 1919 -5 US ## 5 2013 4 23 1600 1545 15 1805 1745 20 MQ ## 6 2013 6 14 1708 1715 -7 1820 1829 -9 B6 ## 7 2013 7 27 2358 2359 -1 336 344 -8 B6 ## 8 2013 9 10 1512 1453 19 1750 1811 -21 UA ## 9 2013 3 30 1901 1905 -4 2039 2114 -35 EV ## 10 2013 3 15 1910 1905 5 2011 2028 -17 UA ## # … with 3,358 more rows, 9 more variables: flight &lt;int&gt;, tailnum &lt;chr&gt;, ## # origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, ## # minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, and abbreviated variable names ## # ¹​sched_dep_time, ²​dep_delay, ³​arr_time, ⁴​sched_arr_time, ⁵​arr_delay ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names Use the argument replace = TRUE to perform a bootstrap sample. More on this later. Random samples are important to modern data science. The verbs are powerful when you apply them to groups of observations within a data frame. This is done with the function group_by(). Determine the average arrival delay by airplane (tail number). flights |&gt; dplyr::group_by(tailnum) |&gt; dplyr::summarize(delayAvg = mean(arr_delay, na.rm = TRUE)) |&gt; dplyr::arrange(desc(delayAvg)) ## # A tibble: 4,044 × 2 ## tailnum delayAvg ## &lt;chr&gt; &lt;dbl&gt; ## 1 N844MH 320 ## 2 N911DA 294 ## 3 N922EV 276 ## 4 N587NW 264 ## 5 N851NW 219 ## 6 N928DN 201 ## 7 N7715E 188 ## 8 N654UA 185 ## 9 N665MQ 175. ## 10 N427SW 157 ## # … with 4,034 more rows ## # ℹ Use `print(n = ...)` to see more rows Determine the number of distinct planes and flights by destination location. flights |&gt; dplyr::group_by(dest) |&gt; dplyr::summarize(planes = dplyr::n_distinct(tailnum), flights = dplyr::n()) ## # A tibble: 105 × 3 ## dest planes flights ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 ABQ 108 254 ## 2 ACK 58 265 ## 3 ALB 172 439 ## 4 ANC 6 8 ## 5 ATL 1180 17215 ## 6 AUS 993 2439 ## 7 AVL 159 275 ## 8 BDL 186 443 ## 9 BGR 46 375 ## 10 BHM 45 297 ## # … with 95 more rows ## # ℹ Use `print(n = ...)` to see more rows Repeat but arrange from most to fewest planes. Example: Daily weather data from Tallahassee Let’s consider another set of data. Daily high and low temperatures and precipitation in Tallahassee. The file (TLH_SOD1892.csv) is available in this project in the folder data). Import the data as a data frame. TLH.df &lt;- readr::read_csv(file = &quot;data/TLH_SOD1892.csv&quot;) ## Rows: 47056 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (10): LATITUDE, LONGITUDE, ELEVATION, PRCP, TAVG, TMAX, TMIN, WDF1, WSF... ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. The data frame contains daily high (TMAX) and low (TMIN) temperatures and total precipitation (PRCP) from two stations: Airport with STATION identification USW00093805 and downtown with STATION identification USC00088754. Use the select() function to create a new data frame with only STATION, DATE, PRCP, TMAX and TMIN. TLH.df &lt;- TLH.df |&gt; dplyr::select(STATION, DATE, PRCP, TMAX, TMIN) TLH.df ## # A tibble: 47,056 × 5 ## STATION DATE PRCP TMAX TMIN ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 USW00093805 1940-03-01 0 72 56 ## 2 USW00093805 1940-03-02 0 77 53 ## 3 USW00093805 1940-03-03 0.05 73 56 ## 4 USW00093805 1940-03-04 0 72 44 ## 5 USW00093805 1940-03-05 0 61 45 ## 6 USW00093805 1940-03-06 0 66 40 ## 7 USW00093805 1940-03-07 0 72 36 ## 8 USW00093805 1940-03-08 0 56 41 ## 9 USW00093805 1940-03-09 0 60 33 ## 10 USW00093805 1940-03-10 0 72 32 ## # … with 47,046 more rows ## # ℹ Use `print(n = ...)` to see more rows Note that you’ve recycled the name of the data frame. You started with TLH.df containing all the columns and we ended with TLH.df with only the columns selected. Then use the filter() function to keep only days at or above 90F. Similarly you recycle the name of the data frame. Use the glimpse() function to take a look at the resulting data frame. TLH.df &lt;- TLH.df |&gt; dplyr::filter(TMAX &gt;= 90) |&gt; dplyr::glimpse() ## Rows: 10,632 ## Columns: 5 ## $ STATION &lt;chr&gt; &quot;USW00093805&quot;, &quot;USW00093805&quot;, &quot;USW00093805&quot;, &quot;USW00093805&quot;, &quot;U… ## $ DATE &lt;date&gt; 1940-05-18, 1940-05-20, 1940-05-21, 1940-05-22, 1940-05-23, 1… ## $ PRCP &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.45, 0.00, 0.… ## $ TMAX &lt;dbl&gt; 91, 92, 94, 93, 93, 90, 90, 91, 91, 91, 92, 95, 95, 95, 93, 91… ## $ TMIN &lt;dbl&gt; 53, 60, 67, 64, 71, 60, 58, 62, 68, 73, 71, 72, 72, 70, 72, 70… Note that the DATE column is a vector of dates having class date. Note if this were a character string you convert the character string into a date with the as.Date() function. Functions from the {lubridate} package are used to extract information from dates. Here you add columns labeled Year, Month, and Day using the extractor functions year(), month(), etc. library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union TLH.df &lt;- TLH.df |&gt; dplyr::mutate(Year = year(DATE), Month = month(DATE), Day = day(DATE), DoW = weekdays(DATE)) TLH.df ## # A tibble: 10,632 × 9 ## STATION DATE PRCP TMAX TMIN Year Month Day DoW ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 USW00093805 1940-05-18 0 91 53 1940 5 18 Saturday ## 2 USW00093805 1940-05-20 0 92 60 1940 5 20 Monday ## 3 USW00093805 1940-05-21 0 94 67 1940 5 21 Tuesday ## 4 USW00093805 1940-05-22 0 93 64 1940 5 22 Wednesday ## 5 USW00093805 1940-05-23 0 93 71 1940 5 23 Thursday ## 6 USW00093805 1940-05-27 0 90 60 1940 5 27 Monday ## 7 USW00093805 1940-05-28 0 90 58 1940 5 28 Tuesday ## 8 USW00093805 1940-06-02 0 91 62 1940 6 2 Sunday ## 9 USW00093805 1940-06-14 0.45 91 68 1940 6 14 Friday ## 10 USW00093805 1940-06-17 0 91 73 1940 6 17 Monday ## # … with 10,622 more rows ## # ℹ Use `print(n = ...)` to see more rows Next you keep only the temperature record from the airport. You use the filter() function on the column labeled STATION. TLH.df &lt;- TLH.df |&gt; dplyr::filter(STATION == &quot;USW00093805&quot;) Now what if you want to know how many hot days (90F or higher) by year? You use the group_by() function and count using the n() function. TLH90.df &lt;- TLH.df |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(nHotDays = dplyr::n()) TLH90.df ## # A tibble: 79 × 2 ## Year nHotDays ## &lt;dbl&gt; &lt;int&gt; ## 1 1940 63 ## 2 1941 96 ## 3 1942 75 ## 4 1943 101 ## 5 1944 95 ## 6 1945 83 ## 7 1946 71 ## 8 1947 94 ## 9 1948 97 ## 10 1949 70 ## # … with 69 more rows ## # ℹ Use `print(n = ...)` to see more rows Note that the group_by() function results in a data frame with the first column the variable used inside the function. In this case it is Year. The next columns are defined by what is in the summarize() function. Repeat but this time group by Month. TLH.df |&gt; dplyr::group_by(Month) |&gt; dplyr::summarize(nHotDays = dplyr::n()) ## # A tibble: 8 × 2 ## Month nHotDays ## &lt;dbl&gt; &lt;int&gt; ## 1 3 2 ## 2 4 102 ## 3 5 778 ## 4 6 1523 ## 5 7 1794 ## 6 8 1746 ## 7 9 1119 ## 8 10 157 As expected the number of 90F+ days is highest in July and August. Note that you’ve had 90F+ days in October. Would you expect there to be more hot days on the weekend? How would you check this? TLH.df |&gt; dplyr::group_by(Year, DoW) |&gt; dplyr::summarize(nHotDays = dplyr::n()) ## `summarise()` has grouped output by &#39;Year&#39;. You can override using the ## `.groups` argument. ## # A tibble: 553 × 3 ## # Groups: Year [79] ## Year DoW nHotDays ## &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1940 Friday 10 ## 2 1940 Monday 10 ## 3 1940 Saturday 7 ## 4 1940 Sunday 8 ## 5 1940 Thursday 9 ## 6 1940 Tuesday 11 ## 7 1940 Wednesday 8 ## 8 1941 Friday 17 ## 9 1941 Monday 12 ## 10 1941 Saturday 13 ## # … with 543 more rows ## # ℹ Use `print(n = ...)` to see more rows You can group by more than one variable. For example, add the variable Year to the group_by() function above. Recall that you can also arrange() the data frame ordered according to the values in a particular column. TLH90.df |&gt; dplyr::arrange(desc(nHotDays)) ## # A tibble: 79 × 2 ## Year nHotDays ## &lt;dbl&gt; &lt;int&gt; ## 1 2016 134 ## 2 1990 129 ## 3 2011 125 ## 4 1993 119 ## 5 2010 118 ## 6 2015 118 ## 7 2018 118 ## 8 1986 116 ## 9 2007 116 ## 10 2000 115 ## # … with 69 more rows ## # ℹ Use `print(n = ...)` to see more rows Putting things together Let’s put together your first piece of original research. You know how to import a data file, you know how to manipulate the data frame to compute something of interest, and you know how to make a graph. Let’s do this for the number of hot days. Let’s say you want a plot of the annual number of hot days in Tallahassee since 1950. Let’s define a hot day as one where the high temperature is at least 90F. library(ggplot2) readr::read_csv(file = &quot;data/TLH_SOD1892.csv&quot;) |&gt; dplyr::filter(STATION == &quot;USW00093805&quot;, TMAX &gt;= 90) |&gt; dplyr::mutate(Year = year(DATE)) |&gt; dplyr::filter(Year &gt;= 1950) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(nHotDays = dplyr::n()) |&gt; ggplot(aes(x = Year, y = nHotDays)) + geom_point() + geom_smooth() + scale_y_continuous(limits = c(0, NA)) + ylab(&quot;Number of Days&quot;) + ggtitle(&quot;Number of Hot Days in Tallahassee Since 1950&quot;, subtitle = &quot;High Temperature &gt;= 90F&quot;) + theme_minimal() ## Rows: 47056 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (10): LATITUDE, LONGITUDE, ELEVATION, PRCP, TAVG, TMAX, TMIN, WDF1, WSF... ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; You go from data in a file to a plot of interest with a set of functions that are logically ordered and easy to read. What would you change to make a similar plot for the number of hot nights (say where the minimum temperature fails to drop below 74)? readr::read_csv(file = &quot;data/TLH_SOD1892.csv&quot;) |&gt; dplyr::filter(STATION == &quot;USW00093805&quot;, TMIN &gt;= 74) |&gt; dplyr::mutate(Year = year(DATE)) |&gt; dplyr::filter(Year &gt;= 1950) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(nHotNights = dplyr::n()) |&gt; ggplot(aes(x = Year, y = nHotNights)) + geom_point() + geom_smooth() + scale_y_continuous(limits = c(0, NA)) + ylab(&quot;Number of Nights&quot;) + ggtitle(&quot;Number of Hot Nights in Tallahassee Since 1950&quot;, subtitle = &quot;Low Temperature &gt;= 74F&quot;) + theme_minimal() ## Rows: 47056 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (10): LATITUDE, LONGITUDE, ELEVATION, PRCP, TAVG, TMAX, TMIN, WDF1, WSF... ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Make a similar plot showing the total precipitation by year. readr::read_csv(file = &quot;data/TLH_SOD1892.csv&quot;) |&gt; dplyr::filter(STATION == &quot;USW00093805&quot;) |&gt; dplyr::mutate(Year = year(DATE)) |&gt; dplyr::filter(Year &gt;= 1950) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(TotalPrecip = sum(PRCP)) |&gt; ggplot(aes(x = Year, y = TotalPrecip)) + geom_point() + geom_smooth() + scale_y_continuous(limits = c(0, NA)) + ylab(&quot;Total Precipitation by Year&quot;) + theme_minimal() ## Rows: 47056 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (10): LATITUDE, LONGITUDE, ELEVATION, PRCP, TAVG, TMAX, TMIN, WDF1, WSF... ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 1 rows containing non-finite values (stat_smooth). ## Warning: Removed 1 rows containing missing values (geom_point). Example: Food consumption and CO2 emissions Source: https://www.nu3.de/blogs/nutrition/food-carbon-footprint-index-2018 fc.df &lt;- readr::read_csv(&#39;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-02-18/food_consumption.csv&#39;) ## Rows: 1430 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): country, food_category ## dbl (2): consumption, co2_emmission ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(fc.df) ## # A tibble: 6 × 4 ## country food_category consumption co2_emmission ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Argentina Pork 10.5 37.2 ## 2 Argentina Poultry 38.7 41.5 ## 3 Argentina Beef 55.5 1712 ## 4 Argentina Lamb &amp; Goat 1.56 54.6 ## 5 Argentina Fish 4.36 6.96 ## 6 Argentina Eggs 11.4 10.5 Consumption is kg/person/year and CO2 emission is kg CO2/person/year. How many different countries are in the data frame? fc.df |&gt; dplyr::distinct(country) |&gt; nrow() ## [1] 130 Arrange the countries from most pork consumption per person to the least pork consumption. fc.df |&gt; dplyr::filter(food_category == &quot;Pork&quot;) |&gt; dplyr::select(country, consumption) |&gt; dplyr::arrange(desc(consumption)) ## # A tibble: 130 × 2 ## country consumption ## &lt;chr&gt; &lt;dbl&gt; ## 1 Hong Kong SAR. China 67.1 ## 2 Austria 52.6 ## 3 Germany 51.8 ## 4 Spain 48.9 ## 5 Poland 46.2 ## 6 Lithuania 45.7 ## 7 Luxembourg 43.6 ## 8 Croatia 42.8 ## 9 Czech Republic 41.2 ## 10 Belarus 40.4 ## # … with 120 more rows ## # ℹ Use `print(n = ...)` to see more rows Arrange the countries from the largest carbon footprint with respect to eating habits to the smallest carbon footprint. fc.df |&gt; dplyr::rename(co2_emission = co2_emmission) |&gt; dplyr::group_by(country) |&gt; dplyr::summarize(totalEmission = sum(co2_emission)) |&gt; dplyr::arrange(desc(totalEmission)) ## # A tibble: 130 × 2 ## country totalEmission ## &lt;chr&gt; &lt;dbl&gt; ## 1 Argentina 2172. ## 2 Australia 1939. ## 3 Albania 1778. ## 4 New Zealand 1751. ## 5 Iceland 1731. ## 6 USA 1719. ## 7 Uruguay 1635. ## 8 Brazil 1617. ## 9 Luxembourg 1598. ## 10 Kazakhstan 1575. ## # … with 120 more rows ## # ℹ Use `print(n = ...)` to see more rows Summary Data munging is a big part of data science. Data science is an iterative cycle: Generate questions about our data. Search for answers by transforming, visualizing, and modeling the data. Use what you learn to refine our questions and/or ask new ones. You use questions as tools to guide our investigation. When you ask a question, the question focuses our attention on a specific part of our data set and helps us decide what to do. For additional practice please check out http://r4ds.had.co.nz/index.html. Cheat sheets http://rstudio.com/cheatsheets "],["thursday-september-15-2022.html", "Thursday, September 15, 2022 Data visualization Transforming the response variable", " Thursday, September 15, 2022 Today Making graphs Data visualization Data visualization is a cornerstone of data science. It gives insights into your data that are not accessible by looking at a spreadsheet or data frame of values. The {ggplot2} package provides functions to make plots efficiently. The functions are an application of the grammar of graphics theory (Leland Wilkinson) of data visualization. At a basic level, graphics/plots/charts (all interchangeable terms) provide a way to explore the patterns in data; the presence of extreme values, distributions of individual variables, and relationships between groups of variables. Graphics should emphasize the findings and insights you want your audience to understand. This requires a balance. On the one hand, you want to highlight as many interesting findings as possible. On the other hand, you don’t want to include so much information that it overwhelms the audience. The grammar of graphics specifies how a plot translates data to attributes and geometric objects. - Attributes are things like location on along an axes, color, shape, and size. - Geometric objects are things like points, lines, bars, and polygons. The type of plot depends on the geometric object, which is specified as a function. Function names for geometric objects begin with geom_. For example, to create a scatter plot of points the geom_point() function is used. Make the functions from the {ggplot2} package available in your current session. library(ggplot2) Bar chart A simple graph is the bar chart showing the number of cases within each group. Consider again the annual hurricane counts. Import the data from the file on my website and print the first six rows. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/US.txt&quot; LH.df &lt;- readr::read_table(loc) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## All = col_double(), ## MUS = col_double(), ## G = col_double(), ## FL = col_double(), ## E = col_double() ## ) dplyr::glimpse(LH.df) ## Rows: 166 ## Columns: 6 ## $ Year &lt;dbl&gt; 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861,… ## $ All &lt;dbl&gt; 1, 3, 0, 2, 1, 2, 1, 1, 1, 3, 2, 0, 0, 0, 2, 1, 1, 0, 4, 2, 3, 0,… ## $ MUS &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ G &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 0, 1, 3, 0, 0, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0,… ## $ FL &lt;dbl&gt; 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0,… ## $ E &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0,… Recall that each case is a year and that the function table() returns the number of years for each landfall count. table(LH.df$All) ## ## 0 1 2 3 4 5 6 7 ## 36 50 40 27 6 1 5 1 The number of cases for each count is tallied and displayed below the count. There were 36 cases of 0 hurricanes. The function geom_bar() creates a bar chart of this frequency table. ggplot(data = LH.df) + geom_bar(mapping = aes(x = All)) You begin a plot with the function ggplot() that creates a coordinate system that you add layers to. The first argument of ggplot() is the data frame to use in the graph. So ggplot(data = LH.df) creates an empty graph. You complete the graph by adding one or more layers. The function geom_bar() adds a layer of bars to our plot, which creates a bar chart. Each geom_ function takes a mapping argument. This defines how variables in our data frame are mapped to visual properties. The mapping argument is always paired with aes() function, and the x argument of aes() specifies which variables to map to the x axes, in this case All. ggplot() looks for the mapped variable in the data argument, in this case, LH.df. The function geom_bar() tables the counts and then maps the number of cases to bars with the bar height proportional to the number of cases. Here the number of cases is the number of years with that many hurricanes. The functions are applied in order (ggplot() comes before geom_bar()) and are linked with the addition + symbol. In this way you can think of the functions as layers in a GIS. The bar chart contains the same information as displayed by the function table(). The y-axis label is ‘count’ and x-axis label is the column name. Repeat this time using Florida hurricane counts. The annual number of Florida hurricanes by year is given in column FL in the data frame LH.df. LH.df$FL ## [1] 1 2 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 2 0 2 1 0 1 2 1 0 3 0 2 0 0 0 3 1 ## [38] 2 0 0 0 0 1 2 0 3 1 1 1 0 1 0 1 0 0 2 0 0 1 1 1 0 0 0 1 2 1 0 1 0 1 0 0 2 ## [75] 1 2 0 2 1 0 0 0 2 2 2 1 0 0 1 0 1 2 0 1 2 1 2 2 1 2 0 0 1 0 0 1 0 0 0 1 0 ## [112] 0 0 3 1 2 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 2 0 1 0 1 0 0 1 0 0 2 0 0 2 ## [149] 1 0 0 0 0 4 3 0 0 0 0 0 0 0 0 0 0 1 The geom_bar() function tables these numbers and plots the frequency as a bar. ggplot(data = LH.df) + geom_bar(mapping = aes(x = FL)) + xlab(&quot;Number of Florida Hurricanes (1851-2016)&quot;) + ylab(&quot;Number of Years&quot;) Here axes labels are placed on the plot with the functions ylab() and xlab(). With this type of ‘layering’ it’s easy to go from data on the web to a publishable plot. Pie preference Thirty graduate students are surveyed about their favor pie. Categories are (1) chocolate cream, (2) coconut custard, (3) georgia peach, and (4) strawberry. To make a bar chart first create the data as a character vector and then change the vector to a data frame. pie &lt;- c(rep(&#39;chocolate cream&#39;, times = 4), rep(&#39;coconut custard&#39;, times = 12), rep(&#39;georgia peach&#39;, times = 5), rep(&#39;strawberry&#39;, times = 9)) piePref.df &lt;- as.data.frame(pie) Use the function str() to see the column type in the data frame. str(piePref.df) ## &#39;data.frame&#39;: 30 obs. of 1 variable: ## $ pie: chr &quot;chocolate cream&quot; &quot;chocolate cream&quot; &quot;chocolate cream&quot; &quot;chocolate cream&quot; ... There is a single column in the data frame with the name pie. It is a factor variable with 4 levels one for each type of pie. A factor is a categorical vector. It looks like a character but it can be ordered. This is important when factors are used in statistical models. Create a table. table(piePref.df$pie) ## ## chocolate cream coconut custard georgia peach strawberry ## 4 12 5 9 Create a bar chart and specify the axis labels. ggplot(data = piePref.df) + geom_bar(mapping = aes(x = pie)) + xlab(&quot;Pie Preference&quot;) + ylab(&quot;Number of Students&quot;) This is a good start. Improvements should be made. First, the bar order is alphabetical from left to right. This is the default ordering for character vectors or for factor variables created from character vectors. It is much easier to make comparisons if frequency determines the order. To change the order on the bar chart specify the order of the factor levels on the vector beer. pie &lt;- factor(pie, levels = c(&quot;coconut custard&quot;, &quot;strawberry&quot;, &quot;georgia peach&quot;, &quot;chocolate cream&quot;)) piePref.df &lt;- as.data.frame(pie) Or use forcats::fct_infreq() https://forcats.tidyverse.org/reference/fct_inorder.html Now remake the bar chart. ggplot(data = piePref.df) + geom_bar(mapping = aes(pie)) + xlab(&quot;Pie Preference&quot;) + ylab(&quot;Number of Students&quot;) Second, the vertical axis tic labels are fractions. Since the bar heights are counts (integers) the tic labels also should be integers. To override this default you add a new y-axis layer. The layer is the function scale_y_continuous() where you indicate the lower and upper limits of the axis with the concatenate (limits = c()) function. Now remake the bar chart. ggplot(data = piePref.df) + geom_bar(mapping = aes(pie)) + xlab(&quot;Beer Preference&quot;) + ylab(&quot;Number of Students&quot;) + scale_y_continuous(limits = c(0, 15)) Now the chart is publishable. Options exist for changing the look of the plot for digital media include, colors, orientation, background, etc. For example to change the bar color use the fill = argument in the function geom_bar(). To change the orientation of the bars use the layer function coord_flip, and to change the background use the layer function theme_minimal(). You make changes to the look of the plot with additional layers. ggplot(data = piePref.df) + geom_bar(mapping = aes(x = pie), fill = &quot;blue&quot;) + xlab(&quot;Pie Preference&quot;) + ylab(&quot;Number of Students&quot;) + scale_y_continuous(limits = c(0, 15)) + coord_flip() + theme_minimal() Recall: the fill = is used on the variable named in the aes() function but it is specified outside the aes() function. Available colors include colors() In the above example you manually reordered the levels in the factor vector pie according to preference. Let’s see how to do this automatically. Consider storm intensity of tropical cyclones during 2017. First create two vectors one numeric containing the minimum pressures (millibars) and the other character containing the storm names. minP &lt;- c(990, 1007, 992, 1007, 1005, 981, 967, 938, 914, 938, 972, 971) name &lt;- c(&quot;Arlene&quot;, &quot;Bret&quot;, &quot;Cindy&quot;, &quot;Don&quot;, &quot;Emily&quot;, &quot;Franklin&quot;, &quot;Gert&quot;, &quot;Harvey&quot;, &quot;Irma&quot;, &quot;Jose&quot;, &quot;Katia&quot;, &quot;Lee&quot;) The function reorder() takes a character vector as the first argument and returns an ordered factor with the order dictated by the numeric values in the second argument. reorder(name, minP) ## [1] Arlene Bret Cindy Don Emily Franklin Gert Harvey ## [9] Irma Jose Katia Lee ## attr(,&quot;scores&quot;) ## Arlene Bret Cindy Don Emily Franklin Gert Harvey ## 990 1007 992 1007 1005 981 967 938 ## Irma Jose Katia Lee ## 914 938 972 971 ## 12 Levels: Irma Harvey Jose Gert Lee Katia Franklin Arlene Cindy Emily ... Don The vector name is in alphabetically order but the factor levels indicate the order of storms from lowest pressure (Irma) to highest pressure (Don). Using the mutate() function you can add a column to a data frame where the column is an ordered factor. Note that it is the difference in pressure (deltaP for short) between the air outside the tropical cyclone and the air in the center that causes the winds. Cyclones with a large pressure difference are stronger in terms of their wind speed. Typically the air outside is about 1014 mb so you compute deltaP and then reorder the tropical cyclone names using this computed variable. df &lt;- data.frame(name, minP) |&gt; dplyr::mutate(deltaP = 1014 - minP, nameOrderedFactor = reorder(name, deltaP)) Finally you plot the bar chart. Since there is no tabulation of the values you use geom_col() instead of geom_bar(). ggplot(data = df) + geom_col(mapping = aes(x = nameOrderedFactor, y = deltaP)) + ylab(&quot;Pressure Difference [mb]&quot;) + xlab(&quot;Atlantic Tropical Cyclones of 2017&quot;) + coord_flip() Note: geom_bar() plots a bar chart AFTER tabulating a column. geom_col() plots a bar chart on a pre-tabulated column. Let’s return to the weather data from Tallahassee. df &lt;- readr::read_csv(file = &quot;data/TLH_SOD1892.csv&quot;) |&gt; dplyr::filter(STATION == &quot;USW00093805&quot;) |&gt; dplyr::mutate(Year = lubridate::year(DATE), Month = lubridate::month(DATE)) |&gt; dplyr::filter(Year &gt;= 1980 &amp; Month == 9) |&gt; dplyr::group_by(Year) |&gt; dplyr::summarize(TotalPrecip = sum(PRCP)) |&gt; dplyr::mutate(Year = reorder(as.factor(Year), TotalPrecip)) ## Rows: 47056 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): STATION, NAME ## dbl (10): LATITUDE, LONGITUDE, ELEVATION, PRCP, TAVG, TMAX, TMIN, WDF1, WSF... ## date (1): DATE ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ggplot(data = df) + geom_col(mapping = aes(x = Year, y = TotalPrecip)) + ylab(&quot;September Rainfall [in]&quot;) + coord_flip() Histogram The histogram is similar to the bar chart except it uses bars to indicate frequency (or proportion) over an interval of continuous values. For instance, with continuous values the function table() is not useful. x &lt;- rnorm(n = 10) table(x) ## x ## -1.6032276455739 -1.21105005355005 -0.930597500697453 -0.659731635004227 ## 1 1 1 1 ## -0.638249710440162 -0.499239870677882 -0.0277773938058075 0.277103773302114 ## 1 1 1 1 ## 0.526728668221472 0.545780025214686 ## 1 1 So neither is a bar plot. A histogram is made as follows: First a collection of disjoint intervals, called bins, covering the range of data points is chosen. “Disjoint” means no overlap, so the intervals look like (a,b] or [a,b). The interval (a,b] means the interval contains all the values from a to b including b but not a, whereas the interval [a,b) means the interval contains all the values from a to b including a but not b.]. Greater than 3.98 (indicated by () and less than or equal to 8.2 (indicated by ]). And so on. You include these cuts as a ordered factor variable to the original cars data frame. cars &lt;- cars |&gt; dplyr::mutate(x_bins = cut(speed, breaks = 5)) You then map the x_bins variable to the x aesthetic and draw box plots. ggplot(data = cars, mapping = aes(x = x_bins, y = dist)) + geom_boxplot() + xlab(&quot;Speed (mph)&quot;) + ylab(&quot;Breaking distance (ft)&quot;) By binning the explanatory variable you create sub-samples of the data. A sample of response values within a given interval of the explanatory variable. The number of intervals is the number of breaks specified by the cut() function. Here you see evidence against the assumption of linearity. Further you see that the box size is increasing. That is the IQR range of breaking distance (depth of the box) is larger for faster moving cars. This creates doubt that the assumption of constant variance is valid. What about the assumption of normality? Although the normality assumption about the residuals is that the conditional distribution of the residuals at each \\(x_i\\) is adequately described by a normal distribution, in practice the residuals are examined together. The residuals are obtained by using the resid() function. res &lt;- resid(model1) res ## 1 2 3 4 5 6 7 ## 3.849460 11.849460 -5.947766 12.052234 2.119825 -7.812584 -3.744993 ## 8 9 10 11 12 13 14 ## 4.255007 12.255007 -8.677401 2.322599 -15.609810 -9.609810 -5.609810 ## 15 16 17 18 19 20 21 ## -1.609810 -7.542219 0.457781 0.457781 12.457781 -11.474628 -1.474628 ## 22 23 24 25 26 27 28 ## 22.525372 42.525372 -21.407036 -15.407036 12.592964 -13.339445 -5.339445 ## 29 30 31 32 33 34 35 ## -17.271854 -9.271854 0.728146 -11.204263 2.795737 22.795737 30.795737 ## 36 37 38 39 40 41 42 ## -21.136672 -11.136672 10.863328 -29.069080 -13.069080 -9.069080 -5.069080 ## 43 44 45 46 47 48 49 ## 2.930920 -2.933898 -18.866307 -6.798715 15.201285 16.201285 43.201285 ## 50 ## 4.268876 There are other extractor functions (like coef()) that output information from the model as vectors or matrices. The fortify() function from the {ggplot2} package makes a data frame from the model object using several extractor functions. model.df &lt;- fortify(model1) head(model.df) ## dist speed .hat .sigma .cooksd .fitted .resid .stdresid ## 1 2 4 0.11486131 15.53088 0.0045923121 -1.849460 3.849460 0.2660415 ## 2 10 4 0.11486131 15.43338 0.0435139907 -1.849460 11.849460 0.8189327 ## 3 4 7 0.07150365 15.51624 0.0062023503 9.947766 -5.947766 -0.4013462 ## 4 22 7 0.07150365 15.43489 0.0254673384 9.947766 12.052234 0.8132663 ## 5 16 8 0.05997080 15.53907 0.0006446705 13.880175 2.119825 0.1421624 ## 6 10 9 0.04989781 15.49830 0.0071319931 17.812584 -7.812584 -0.5211526 The data frame resulting from the fortify() function has a column labeled .resid containing the vector of residuals. The residuals are the observed distance minus the predicted (fitted) distance (dist column minus .fitted column). Create a histogram and density of the model’s residuals by typing ggplot(data = model.df, mapping = aes(.resid)) + geom_histogram(bins = 8, color = &quot;white&quot;) + geom_freqpoly(bins = 8) You can see that the histogram is not symmetric. There are more values to the right of the central set of values than to the left. The validity of the normality assumption is therefore under question. Since departures from normality can occur simply because of sampling variation, the question arises as to whether that apparent skewness (asymmetry) you see in this set of residuals is significantly larger than expected by chance. One approach to visualizing the expected variation from a reference distribution is to add an uncertainty band on the density plot. The sm.density() function from the {sm} package provides a way to plot the uncertainty band. The first argument is a vector of residuals and the argument model = \"Normal\" draws a band around a normal distribution centered on zero with a variance equal to the variance of the residuals. sm::sm.density(res, model = &quot;Normal&quot;) The black curve representing the residuals is shifted left relative to a normal density and goes outside the blue ribbon in the right tail indicating that the residuals may not be adequately described by a normal distribution although deviation from normality is small. The blue ribbon is the uncertainty surrounding a normal density curve. In summary, the linear regression model may not be adequate. The assumptions of linearity, equal variance, and normally distributed residuals are not entirely reasonable for these data. What should you do? The relationship appears to be non-linear. So you use the square root of the breaking distance as the response variable instead. ggplot(data = cars, mapping = aes(x = speed, y = sqrt(dist))) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab(&quot;Speed (mph)&quot;) + ylab(expression(sqrt(&quot;Break Distance (ft)&quot;))) ## `geom_smooth()` using formula &#39;y ~ x&#39; Fit another model. First make a copy of the original data frame and add a column called distSR. Check the assumption of linearity. cars2 &lt;- cars cars2$distSR &lt;- sqrt(cars$dist) ggplot(data = cars2, mapping = aes(x = cut(speed, breaks = 5), y = distSR)) + geom_boxplot() + xlab(&quot;Speed (mph)&quot;) + ylab(&quot;Square Root of Break Distance (ft)&quot;) It looks good. Fit the new model and extract and make a histogram of the residuals. model2 &lt;- lm(distSR ~ speed, data = cars2) res2 &lt;- resid(model2) model2.df = fortify(model2) ggplot(data = model2.df, mapping = aes(.resid)) + geom_histogram(bins = 8, color = &quot;white&quot;) + geom_freqpoly(bins = 8) Distribution of residuals is still skewed but it is better. In fact now the black line is completely inside the uncertainty ribbon of a normal density plot. sm::sm.density(res2, model = &quot;Normal&quot;) Transforming the response variable The cars example uses a square-root transformation of the response variable. This is appropriate when the response variable has a physical connection to the explanatory variable (e.g., timber volume as a response variable to tree diameter at breast height). Limitations Cannot be applied to negative numbers Transforms numbers &lt; 1 and &gt; 1 in different ways A more common situation is to take logarithms (natural or common) of the response variable. Appropriate When the SD of the residuals is directly proportional to the fitted values (and not to some power of the fitted values) When the relationship is close to exponential Limitations How to transform zero values? Add a constant such as 1 or 0.00001 or remove zero values from analysis (not recommended) Family of transformations Box-Cox (Power) Transformation More about this later If your response variable is counts (e.g., the number of hurricanes in a season) then it is better to use a generalized linear regression model than to transform the counts to normality. "],["thursday-october-20-2022.html", "Thursday, October 20, 2022 More regression examples Outliers Simpson’s paradox", " Thursday, October 20, 2022 Today More regression examples Outliers Simpson’s paradox More regression examples Linear regression provides an adequate model for your data under the following four assumptions. Linearity: Average values of Y in ordered intervals of X are a straight-line function of X. Each interval creates a ‘sub-population’ of Y values. Constant variance: Sub-populations of Y have about the same standard deviation. Normality: Values from each sub-population are described by a normal distribution. Independence: Each observation is independent from the other observations. Key idea: A model can be statistically significant, but not adequate. Example: Average income vs percent college graduates at the state level The file Inc.txt on my website contains average annual household income vs percentage of college graduates by state. Fit a linear regression model to these data and check the model assumption. Does the linear model appear to be adequate? Inc.df &lt;- read.table(&quot;http://myweb.fsu.edu/jelsner/temp/data/Inc.txt&quot;, header = TRUE) head(Inc.df) ## State College Income ## 1 AL 20.4 20487 ## 2 AK 28.1 26171 ## 3 AZ 24.6 21947 ## 4 AR 18.4 19479 ## 5 CA 27.5 26808 ## 6 CO 34.6 27573 What is the type and strength of the relationship between percent college graduates and income? These are answered by a scatter plot and correlation, respectively. cor(Inc.df$College, Inc.df$Income) ## [1] 0.7773291 library(ggplot2) ggplot(data = Inc.df, mapping = aes(x = College, y = Income)) + geom_point() + geom_smooth(method = lm) + xlab(&quot;College Graduates (%)&quot;) + ylab(&quot;Average Income ($)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The plot shows that there is a linear relationship between percent college graduates and income. The relationship is positive indicating that states with higher percentage of college graduates also have higher incomes on average. The relationship is quite strong (the correlation value is .78), with few, if any, of the states deviating from the linear pattern. When the data have a spatial component it’s a good idea to make a map. Here you create a choropleth map using functions from the {tmap} package. library(tmap) library(USAboundaries) library(sf) ## Linking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE library(dplyr) states.sf &lt;- us_states() Inc.df &lt;- Inc.df |&gt; rename(stusps = State) states.sf &lt;- left_join(states.sf, Inc.df, by = &quot;stusps&quot;) |&gt; filter(!stusps %in% c(&quot;AK&quot;, &quot;HI&quot;, &quot;PR&quot;)) tm_shape(states.sf) + tm_polygons(col = c(&quot;Income&quot;, &quot;College&quot;)) + tm_legend(legend.position = c(&quot;left&quot;, &quot;bottom&quot;)) Regress annual household income on percent of college graduates. model1 &lt;- lm(Income ~ College, data = Inc.df) summary(model1) ## ## Call: ## lm(formula = Income ~ College, data = Inc.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4136.0 -1344.6 74.9 1275.6 5121.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10459.85 1614.70 6.478 4.27e-08 *** ## College 545.27 63.04 8.649 1.98e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2078 on 49 degrees of freedom ## Multiple R-squared: 0.6042, Adjusted R-squared: 0.5962 ## F-statistic: 74.81 on 1 and 49 DF, p-value: 1.975e-11 Results indicate a significant relationship (\\(p\\)-value &lt; .001). Percent college graduates explains 60% of the variation in average income by state. The significant effect implies it is unlikely that income and education have no relationship. Not only is the effect significant but it is large. For every 1 percentage point increase in graduates, average annual household income increases by $545. From the small standard error (relative to the estimate) on the slope extracted as summary(model1)$coefficients[2, 2] ## [1] 63.04127 You can say that the effect is quite precise. This precision shows in the narrow uncertainty interval for the slope estimate. confint(model1) ## 2.5 % 97.5 % ## (Intercept) 7214.9929 13704.7171 ## College 418.5846 671.9569 Is the model adequate? Check linearity and equal spread. ggplot(data = Inc.df, mapping = aes(x = cut(College, breaks = 6), y = Income)) + geom_boxplot() + xlab(&quot;Percentage of College Graduates&quot;) + ylab(&quot;Average Income ($)&quot;) It looks favorable for these two assumptions. Although the effect seems to level off for the highest percentage of graduates. Next look at the distribution of the residuals. Does the distribution look like a normal distribution? res &lt;- residuals(model1) sm::sm.density(res, model = &quot;Normal&quot;) The black curve falls within the blue envelope of a normal distribution, so you have no reason to suspect the assumption of normally distributed residuals. Another check is to use the quantile-quantile plot. A quantile-quantile plot (or Q-Q plot) is a graph of the quantiles of one distribution against the quantiles of another distribution. If the distributions have similar shapes, the points on the plot fall roughly along the straight line. To check the normality assumption of a regression model you want to compare the quantiles of the residuals against the quantiles of a normal distribution. You do that with the qqnorm() function. qqnorm(res) qqline(res) Departures from normality are seen as a systematic departure of the points from a straight line on the Q-Q plot. Interpreting Q-Q plots is somewhat subjective. Here are the common situations. Description: interpretation All but a few points fall on a line: few outliers in the data Left end of pattern is below the line; right end of pattern is above the line: long tails at both ends of the distribution Left end of pattern is above the line; right end of pattern is below the line: short tails at both ends of the data distribution Curved pattern with slope increasing from left to right: data is skewed to the right Curved pattern with slope decreasing from left to right: data is skewed to the left Staircase pattern (plateaus and gaps): data have been rounded or are discrete Outliers What state is most/least favorable with respect to income after graduating from college? These are the states where incomes for a given % graduates fall farthest from the regression line. The residuals are what is left over once percent graduated is in the model—or in statistical language, “after controlling for percent graduated.” The above assumptions concern the conditional distribution of the residuals. The residuals for each of the observations are computed when you use the lm() function to fit the model. In the code below, the fortify() function from the {ggplot2} package creates a data frame with elements computed from the fitted model. model1.df &lt;- fortify(model1) head(model1.df) ## Income College .hat .sigma .cooksd .fitted .resid ## 1 20487 20.4 0.04076339 2093.191 0.0061668998 21583.38 -1096.3784 ## 2 26171 28.1 0.02738038 2098.646 0.0005072964 25781.96 389.0367 ## 3 21947 24.6 0.01993274 2080.543 0.0089192327 23873.52 -1926.5156 ## 4 19479 18.4 0.06209651 2093.973 0.0084026089 20492.84 -1013.8369 ## 5 26808 27.5 0.02450204 2090.083 0.0054600928 25454.80 1353.1992 ## 6 27573 34.6 0.10104189 2082.383 0.0445065637 29326.22 -1753.2232 ## .stdresid ## 1 -0.5387362 ## 2 0.1898443 ## 3 -0.9365331 ## 4 -0.5038109 ## 5 0.6593668 ## 6 -0.8899099 model1.df$State &lt;- Inc.df$stusps The variables in the model are given in the first two columns with information about the regression that is specific to each of the cases (observations) given in the next six columns. The first three of those columns contain information that allows you to assess how influential that observation is to the model. If that particularly observation is removed and the regression model refit without it, how much difference would it make in terms of the coefficients? If removing an observation changes the coefficients a lot then that observation is said to be influential. A plot of the model residuals as a function of the explanatory variables lets us easily identify which states have the largest positive and negative residuals. To make the plot more readable use the geom_label_repel() function from the {ggrepel} package. This labels the points while avoiding overlapping labels. library(ggrepel) ggplot(data = model1.df, mapping = aes(x = College, y = .resid)) + geom_point() + geom_hline(yintercept = 0) + xlab(&quot;Percent College Graduates&quot;) + ylab( &quot;Model Residuals&quot;) + geom_label_repel(aes(label = State), color = &quot;darkblue&quot;) Nevada and Connecticut stand out as states where the model underestimates income from percent graduation rates. While Utah, New Mexico, and Montana are states where the model over estimates income. Statistically significant outliers are those that are outside +/- 2 standard deviations from the regression line. The standardized residuals are plotted and the corresponding significance lines drawn. ggplot(data = model1.df, mapping = aes(x = College, y = .stdresid)) + geom_point() + geom_hline(yintercept = 0) + geom_hline(yintercept = -2, lty = &quot;dotted&quot;) + geom_hline(yintercept = 2, lty = &quot;dotted&quot;) + xlab(&quot;Percent College Graduates&quot;) + ylab(&quot;Studentized Residuals&quot;) + geom_label_repel(aes(label = State), color = &quot;darkblue&quot;) Summary Regression modeling is statistical control. You often want to do more than just summarize the relationship between variables. That is go beyond reporting the correlation. Regression provides a strategy to control for effects of an explanatory variable to see what is left over. These left overs (residuals) are interpreted as “controlled observations” (e.g. percent income controlling for percent graduates). Observations that result in large residuals are called outliers. Outliers can distort regression results or they can be interesting on their own (e.g. unusually destructive tornadoes). Inspect scatter plots and plots of residuals to determine whether there are outliers that have a strong influence on the regression line. If there are you should re-fit the regression model without those observations and compare results. Regardless of how you decide to handle them, you need to let our readers know about these unusual cases. When interpreting a regression model fit to our data, you are making some implicit assumptions. Before accepting a model you need to examine those assumptions to make sure they are tenable. The model fit may be excellent, but you can’t be sure your conclusions are correct unless you can defend the assumptions. Examining the model residuals helps you defend (if warranted) the assumptions. Example: A regression model for trend The rate at which something is changing over time is called a trend. Trend analysis is common in climate change studies and it often involves fitting a linear regression model to quantify the trend where the “explanatory variable” is some index of time. Returning to the Florida precipitation data. Import the data and make a line plot showing March values each year. Add the best-fit line through these values. loc &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt&quot; FLp &lt;- readr::read_table(loc, na = &quot;-9.900&quot;) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Year = col_double(), ## Jan = col_double(), ## Feb = col_double(), ## Mar = col_double(), ## Apr = col_double(), ## May = col_double(), ## Jun = col_double(), ## Jul = col_double(), ## Aug = col_double(), ## Sep = col_double(), ## Oct = col_double(), ## Nov = col_double(), ## Dec = col_double() ## ) library(ggplot2) ggplot(FLp, aes(x = Year, y = Mar)) + geom_line() + geom_smooth(method = lm, color = &quot;blue&quot;) + ylab(&quot;Statewide March Precipitation (in)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; There is an upward trend (increasing precipitation) but you can see that the uncertainty ribbon would allow a horizontal line. Thus you do not anticipate a significant trend term in a regression model. To check on this expectation, you regress March precipitation onto year. model &lt;- lm(Mar ~ Year, data = FLp) summary(model) ## ## Call: ## lm(formula = Mar ~ Year, data = FLp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7877 -1.4772 -0.3558 1.0638 4.9677 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.206620 10.260642 -1.872 0.0638 . ## Year 0.011710 0.005253 2.229 0.0277 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.919 on 115 degrees of freedom ## Multiple R-squared: 0.04142, Adjusted R-squared: 0.03309 ## F-statistic: 4.969 on 1 and 115 DF, p-value: 0.02774 You see that the statewide annual average precipitation has been increasing by .01 inches per year. This upward trend has a \\(p\\)-value of .04 providing suggestive but inconclusive evidence against the null hypothesis of no trend. The larger the \\(p\\)-value the more evidence you have that the trend is not significant. You can fit regression models for all months. First convert the wide to a long data frame. FLpL &lt;- FLp |&gt; tidyr::pivot_longer(cols = Jan:Dec, names_to = &quot;Month&quot;, values_to = &quot;Precipitation&quot;) Then to fit trend models separately for each month you use the do() function from the {dplyr} package together with the tidy() function from the {broom} package. FLpL |&gt; dplyr::mutate(MonthF = factor(Month, levels = month.abb)) |&gt; dplyr::group_by(MonthF) |&gt; dplyr::do(broom::tidy(lm(Precipitation ~ Year, data = .))) ## # A tibble: 24 × 6 ## # Groups: MonthF [12] ## MonthF term estimate std.error statistic p.value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Jan (Intercept) -10.6 8.14 -1.30 0.197 ## 2 Jan Year 0.00691 0.00417 1.66 0.100 ## 3 Feb (Intercept) 4.46 8.22 0.543 0.588 ## 4 Feb Year -0.000664 0.00421 -0.158 0.875 ## 5 Mar (Intercept) -19.2 10.3 -1.87 0.0638 ## 6 Mar Year 0.0117 0.00525 2.23 0.0277 ## 7 Apr (Intercept) 2.61 8.34 0.313 0.755 ## 8 Apr Year 0.000162 0.00427 0.0380 0.970 ## 9 May (Intercept) 8.76 9.72 0.901 0.369 ## 10 May Year -0.00252 0.00498 -0.506 0.614 ## # … with 14 more rows ## # ℹ Use `print(n = ...)` to see more rows The table shows the intercept and slope coefficients for each month. Simpson’s paradox The four assumptions that under gird a linear regression model include (1) linearity, (2) equal variance, (3) normality of residuals, and (4) independence of observations. But even if those assumptions are valid, another issue is that of scale. Simpson’s paradox occurs when a trend appears in different groups of data but disappears or reverses when the groups are combined. It is encountered in social-science and medical-science statistics and is problematic when data are aggregated at various scales (spatial or otherwise). The paradox can be resolved when causal relations are appropriately addressed in the statistical modeling. As an example, consider again the relationship between bill length and bill depth in the Palmer penguins data without regards to species. You start with a scatter plot and a linear regression model. library(palmerpenguins) ( p &lt;- ggplot(data = penguins, mapping = aes(y = bill_depth_mm, x = bill_length_mm)) + geom_point() + geom_smooth(method = lm) ) ## `geom_smooth()` using formula &#39;y ~ x&#39; lm(bill_depth_mm ~ bill_length_mm, data = penguins) ## ## Call: ## lm(formula = bill_depth_mm ~ bill_length_mm, data = penguins) ## ## Coefficients: ## (Intercept) bill_length_mm ## 15.86202 0.05983 The plot shows an inverse relationship between bill depth and bill length. And the regression model shows a statistically significant relationship with bill depth decreasing by .85 cm for every one mm increase in bill length. However the relationship is across all species of penguins. If you group by species you see that for each species the relationship is the opposite. p + geom_point(mapping = aes(y = bill_depth_mm, x = bill_length_mm, color = species)) + geom_smooth(method = lm, mapping = aes(color = species)) + scale_color_manual(values = c(&quot;darkorange&quot;, &quot;purple&quot;, &quot;cyan4&quot;)) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; That is bill depth increases with increasing bill length. So you conclude that the negative relationship is an artifact of grouping the penguins with different body masses together. The paradox is resolved when the relations are appropriately addressed in the statistical modeling. See also Berkson’s paradox https://en.wikipedia.org/wiki/Berkson%27s_paradox "],["tuesday-october-25-2022.html", "Tuesday, October 25, 2022 A regression plane Fitting a multiple variable regression model Should you remove a variable from the model? Checking on the model assumptions Using the model to make predictions Collinearity Interactions", " Tuesday, October 25, 2022 Today Multiple variable linear regression A regression plane Fitting a multiple variable regression model Removing a variable from the model Checking model assumptions Using the model to make predictions Collinearity Interactions A regression plane Multiple variable (multi-variable) linear regression is used to model a dependent variable when there is more than one explanatory variable. Everything from simple (single explanatory variable) regression carries over to multiple variable linear regression. There is a slope coefficient for each explanatory variable. Note: You regress the response variable onto the explanatory variables. You do not perform a regression of the variables or regress y AND x or regress y with respect to x. These are ambiguous at best and misleading at worse. With one explanatory variable the regression model is described with a straight line. With two explanatory variables the regression model is described with a flat surface. To see this consider a three dimensional scatter plot created with the scatterplot3d() from the {scatterplot3d} package and add the regression plane. The data comes from the trees data frame. You start with a (simple) linear regression. You regress timber volume (Volume) on tree girth (Girth) and add the model as a line on the scatter plot. Here I use the plot() method and abline() are base R graphics functions. model1 &lt;- lm(Volume ~ Girth, data = trees) plot(trees$Volume ~ trees$Girth, pch = 16, xlab = &quot;Tree diameter (in)&quot;, ylab = &quot;Timber volume (cubic ft)&quot;) abline(model1) Next you regress timber volume (Volume) on girth (Girth) and height (Height) and use the scatterplot3d() function to plot the scatter of points. The resulting plane3d() function takes the regression model and adds the surface. model2 &lt;- lm(Volume ~ Girth + Height, data = trees) s3d &lt;- scatterplot3d::scatterplot3d(trees, angle = 55, scale.y = .7, pch = 16, xlab = &quot;Tree diameter (in)&quot;, zlab = &quot;Timber volume (cubic ft)&quot;, ylab = &quot;Tree height (ft)&quot;) s3d$plane3d(model2) The graph shows that timber volume increases with tree diameter and tree height. By changing the view angle (angle =) you can see that some observations are above the surface and some are below. s3d &lt;- scatterplot3d(trees, angle = 32, scale.y = .7, pch = 16, xlab = &quot;Tree diameter (in)&quot;, zlab = &quot;Timber volume (cubic ft)&quot;, ylab = &quot;Tree height (ft)&quot;) s3d$plane3d(model2) The distance along the vertical axis from the observation to the model surface is the residual. So even though there are two explanatory variables there is only one set of residuals as with simple regression. The multiple linear regression model is given by: \\[ \\hat y = \\hat \\beta_0 + \\hat \\beta_1 x_{i1} + \\hat \\beta_2 x_{i2} + \\cdots + \\hat \\beta_p x_{ip} \\] The explanatory variables are written with a double subscript (there are \\(p\\) of them) to indicate the observation number (1st subscript) and the variable number (2nd subscript). Each explanatory variable gets a coefficient, so there are \\(p + 1\\) of them (including \\(\\hat \\beta_0\\), the y-intercept). Now for each observation \\(i\\) there is an observed response (\\(y_i\\)) and a predicted response (\\(\\hat y_i\\)) and the difference is called the residual. The residuals are given by the equation \\[ y_i - \\hat y_i = \\varepsilon_i \\] and are assumed to be described by a set of normal distributions each centered on zero and having the same variance (\\(\\sigma^2\\)). A grid of scatter plots As with simple linear regression, before you model the data you should make a scatter plot. With more than one explanatory variable you rely on a series of scatter plots. Consider the data set PetrolConsumption.txt that has gasoline consumption by state for a given year. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt&quot; PC.df &lt;- readr::read_table(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Petrol.Tax = col_double(), ## Avg.Inc = col_double(), ## Pavement = col_double(), ## Prop.DL = col_double(), ## Petrol.Consumption = col_double() ## ) head(PC.df) ## # A tibble: 6 × 5 ## Petrol.Tax Avg.Inc Pavement Prop.DL Petrol.Consumption ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 3571 1976 0.525 541 ## 2 9 4092 1250 0.572 524 ## 3 9 3865 1586 0.58 561 ## 4 7.5 4870 2351 0.529 414 ## 5 8 4399 431 0.544 410 ## 6 10 5342 1333 0.571 457 Columns in the data frame include petrol (gas) tax (Petrol.Tax) [cents per gallon], per capita income (Avg.Inc) [$/10], miles of paved highway (Pavement), proportion of drivers (Prop.DL), and consumption of petrol (Petrol.Consumption) [millions of gallons]. First create a panel of scatter plots using all the variables in the data frame with the GGally::ggpairs() function from the {GGally} package. GGally::ggpairs(PC.df) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 The plots are arranged in a grid. The rows correspond to the variables as given from left to right in the data frame. The first column in the data set is Petrol.Tax then Avg.Inc, etc. The plot in row 2, column 1 is shows Avg.Inc on the vertical axis and Petrol.Tax on the horizontal axis. The value in the diagonal panel (row 1, column 2) is the Pearson correlation coefficient between Avg.Inc and Petrol.Tax. The panel in row 3 column 1 shows Pavement on the vertical axis and Petrol.Tax on the horizontal axis. You want to quantify how much of the variation in gas consumption at the state level is statistically explained by the variables taxes, income, amount of pavement, and proportional of drivers. So gas consumption (the variable Petrol.Consumption) is your response variable and you focus on the subset of scatter plots (in row 5) where Petrol.Consumption is on the vertical axis in each plot. By eye, which of the variables appears to you to be the most (least) related to gas consumption? The variable with the highest correlation with gas consumption is Prop.DL, the proportion of people with a drivers license. The correlation between these two variables is .7. With more people driving, gas consumption increases. Gas taxes have an inverse relationship with gas consumption. This is seen in the scatter plot in the lower left corner of the grid and by the negative correlation (-.45) in the upper right corner. Plots along the diagonal are density plots of each of the variables individually. Correlations arranged in a matrix like the grid of scatter plots is printed using the cor() function. cor(PC.df) ## Petrol.Tax Avg.Inc Pavement Prop.DL ## Petrol.Tax 1.00000000 0.01266516 -0.52213014 -0.2880372 ## Avg.Inc 0.01266516 1.00000000 0.05016279 0.1570701 ## Pavement -0.52213014 0.05016279 1.00000000 -0.0641295 ## Prop.DL -0.28803717 0.15707008 -0.06412950 1.0000000 ## Petrol.Consumption -0.45128028 -0.24486207 0.01904194 0.6989654 ## Petrol.Consumption ## Petrol.Tax -0.45128028 ## Avg.Inc -0.24486207 ## Pavement 0.01904194 ## Prop.DL 0.69896542 ## Petrol.Consumption 1.00000000 The correlations between the explanatory variables and gas consumption are located in the last column of the matrix under Petrol.Consumption. From the plots and the correlation matrix you anticipate the proportion of people with drivers licenses to be the most important variable in a regression model and the coefficient on this term will be a positive number. Further you anticipate that the gas tax will be an important variable but the coefficient on this term will be a negative number. Fitting a multiple variable regression model You fit a multiple variable regression model to the gas data using the lm() function. You put the name of the response variable (Petrol.Consumption) to the left of the ~ and then the names of the explanatory variables to the right. After each variable you include a + sign (except for the last variable). Here you assign the model to an object called model1. model1 &lt;- lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax, data = PC.df) model1 ## ## Call: ## lm(formula = Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + ## Petrol.Tax, data = PC.df) ## ## Coefficients: ## (Intercept) Prop.DL Pavement Avg.Inc Petrol.Tax ## 3.773e+02 1.336e+03 -2.426e-03 -6.659e-02 -3.479e+01 You have four explanatory variables so the model has five coefficients. There is a coefficient for the y-intercept (Intercept). Looking only at the signs on the numbers under the variable names you see that gas consumption increases with the proportion of drivers and decreases with the amount of pavement, average income and gas tax. Why might gas consumption decrease with increasing income? The model is interpreted as follows: Average gas consumption [millions of gallons] = 377.3 + 1336 * Prop.DL – 0.0024 * Pavement – 0.06659 * Avg.Inc – 34.79 * Petrol.Tax The model says that for every 1 percentage point increase in the proportion of drivers (to overall population), the mean gas consumption increases by 1336 million gallons (1.336 billion gallons) assuming Pavement, Avg.Inc and Petrol.Tax are constant. For every 1 cent/gallon increase in taxes, mean gas consumption decreases by 34.79 million gallons assuming the three other variables are held constant. The statistically significant variables in the model for explaining gas consumption are identified by looking at the table of coefficients from the summary() function. summary(model1) ## ## Call: ## lm(formula = Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + ## Petrol.Tax, data = PC.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -122.03 -45.57 -10.66 31.53 234.95 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.773e+02 1.855e+02 2.033 0.048207 * ## Prop.DL 1.336e+03 1.923e+02 6.950 1.52e-08 *** ## Pavement -2.426e-03 3.389e-03 -0.716 0.477999 ## Avg.Inc -6.659e-02 1.722e-02 -3.867 0.000368 *** ## Petrol.Tax -3.479e+01 1.297e+01 -2.682 0.010332 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66.31 on 43 degrees of freedom ## Multiple R-squared: 0.6787, Adjusted R-squared: 0.6488 ## F-statistic: 22.71 on 4 and 43 DF, p-value: 3.907e-10 You ignore the Intercept term. It represents the value of the response variable where the model line intersects the \\(y\\)-axis. You see that three of the four coefficients have a \\(p\\)-value at or less than .01. Only the coefficient on the variable Pavement has a \\(p\\)-value greater than .15. Therefore this variable is not significant in explaining the variance in the response variable (the coefficient is not statistically significant from a value of zero). The null hypothesis is that the variable is NOT important to the model. The \\(p\\)-value on the variable’s coefficient is evidence in support of the null hypothesis. The larger the \\(p\\)-value the more evidence you have that the variable is not important to the model. In determining which variables are most important in the regression model, the coefficient estimates (effect sizes) cannot be compared directly because they do not have the same units. Key idea: A regression coefficient has units of the response variable divided by the units of the explanatory variable. But the explanatory variables have different units. For example, Petrol.Tax has units of cents per gallon and Pavement has units of miles. So instead of comparing the values of the coefficients directly (i.e., instead of looking in the column labeled Estimate) you look at the column labeled t value. The magnitude of the \\(t\\) values are inter comparable. They are coefficients that have been standardized by dividing by the corresponding standard error. Accordingly, Avg.Inc is more more important than Petrol.Tax even though .0666 is much smaller than 34.79. Key point: The order of the explanatory variables does not change the magnitude or the sign of the corresponding coefficients. summary(lm(Petrol.Consumption ~ Pavement + Petrol.Tax + Avg.Inc + Prop.DL, data = PC.df)) ## ## Call: ## lm(formula = Petrol.Consumption ~ Pavement + Petrol.Tax + Avg.Inc + ## Prop.DL, data = PC.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -122.03 -45.57 -10.66 31.53 234.95 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.773e+02 1.855e+02 2.033 0.048207 * ## Pavement -2.426e-03 3.389e-03 -0.716 0.477999 ## Petrol.Tax -3.479e+01 1.297e+01 -2.682 0.010332 * ## Avg.Inc -6.659e-02 1.722e-02 -3.867 0.000368 *** ## Prop.DL 1.336e+03 1.923e+02 6.950 1.52e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66.31 on 43 degrees of freedom ## Multiple R-squared: 0.6787, Adjusted R-squared: 0.6488 ## F-statistic: 22.71 on 4 and 43 DF, p-value: 3.907e-10 Key point: The model does not change with a linear transformation of the variables. For example, if you multiply Pavement by 20, the coefficient and standard error change accordingly, but not the \\(t\\) value or the corresponding \\(p\\)-value. summary(lm(Petrol.Consumption ~ I(Pavement * 20) + Petrol.Tax + Avg.Inc + Prop.DL, data = PC.df)) ## ## Call: ## lm(formula = Petrol.Consumption ~ I(Pavement * 20) + Petrol.Tax + ## Avg.Inc + Prop.DL, data = PC.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -122.03 -45.57 -10.66 31.53 234.95 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.773e+02 1.855e+02 2.033 0.048207 * ## I(Pavement * 20) -1.213e-04 1.695e-04 -0.716 0.477999 ## Petrol.Tax -3.479e+01 1.297e+01 -2.682 0.010332 * ## Avg.Inc -6.659e-02 1.722e-02 -3.867 0.000368 *** ## Prop.DL 1.336e+03 1.923e+02 6.950 1.52e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 66.31 on 43 degrees of freedom ## Multiple R-squared: 0.6787, Adjusted R-squared: 0.6488 ## F-statistic: 22.71 on 4 and 43 DF, p-value: 3.907e-10 Side note: Here the multiplication is done inside the function I() to restrict the interpretation of the operator * to multiplication rather than interaction. The multiple R-squared value is .68 which means the model explains 68% of the variation in gas consumption. Note that the \\(p\\)-value on the model (given on the last line of the output) is small. The null hypothesis in this case is that none of the variables are important in explaining petrol consumption. You reject this null hypothesis. Should you remove a variable from the model? A model is said to be simpler if it has fewer explanatory variables. You try another model without the Pavement variable (the variable that was not significant in the previous model). model2 &lt;- lm(Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax, data = PC.df) summary(model2) ## ## Call: ## lm(formula = Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax, ## data = PC.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -110.10 -51.22 -12.89 24.49 238.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 307.32790 156.83067 1.960 0.05639 . ## Prop.DL 1374.76841 183.66954 7.485 2.24e-09 *** ## Avg.Inc -0.06802 0.01701 -3.999 0.00024 *** ## Petrol.Tax -29.48381 10.58358 -2.786 0.00785 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 65.94 on 44 degrees of freedom ## Multiple R-squared: 0.6749, Adjusted R-squared: 0.6527 ## F-statistic: 30.44 on 3 and 44 DF, p-value: 8.235e-11 The remaining explanatory variables are significant. The values are slightly different as the variance in the gas consumption attributed to Pavement is now spread across the remaining variables. Key point: Removing an explanatory variable changes the coefficient values on the remain variables in the model. The proportion of the population with licenses is the most important variable. It has the largest \\(t\\) value (in absolute value). By removing a variable the R-squared statistic DECREASES to .675. This is always the case with a simpler model (fewer explanatory variables). Thus the R-squared statistic should not be used to compare models unless the models have the same number of explanatory variables. One alternative for comparing models is the adjusted R squared. It is a modification of the R squared that accounts for the number of explanatory variables in the model. Key point: The adjusted R squared increases only if the new variable improves the model more than would be expected by chance. The adjusted R squared can be negative, and will always be less than or equal to R squared. To see the importance of the adjusted R squared, first note that Petrol.Tax has the smallest \\(t\\) value. Suppose you remove it. What happens? model3 &lt;- lm(Petrol.Consumption ~ Prop.DL + Avg.Inc, data = PC.df) summary(model3) ## ## Call: ## lm(formula = Petrol.Consumption ~ Prop.DL + Avg.Inc, data = PC.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -115.46 -45.93 -13.76 30.12 243.50 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.83709 122.46304 0.064 0.949257 ## Prop.DL 1525.04289 188.29687 8.099 2.47e-10 *** ## Avg.Inc -0.07092 0.01821 -3.895 0.000323 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 70.72 on 45 degrees of freedom ## Multiple R-squared: 0.6175, Adjusted R-squared: 0.6005 ## F-statistic: 36.33 on 2 and 45 DF, p-value: 4.064e-10 The adjusted R squared is smaller, so you conclude that this variable should be kept in the model. Thus you settle on a final model: Average gas consumption [millions of gallons] = 307.3 + 1375 x Prop.DL - .06802 x Avg.Inc - 29.48 x Petrol.Tax Checking on the model assumptions Next you need to check the model assumptions. Equal variance: You saw how to check this assumption by using the cut() function and creating side-by-side box plots. Another way is to plot the standardized residuals on the vertical axis and the fitted values along the horizontal axis. Adding a smoothed curve and the y equal zero line makes it easy to see whether there is a pattern to the residuals. library(ggplot2) model2.df &lt;- fortify(model2) ggplot(data = model2.df, mapping = aes(x = .fitted, y = .stdresid)) + geom_point() + geom_smooth() + geom_hline(yintercept = 0) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; In this case: no pattern in the residuals. Normality. First you use the sm.density() function. res &lt;- residuals(model2) sm::sm.density(res, xlab = &quot;Model Residuals&quot;, model = &quot;Normal&quot;) You also examine a quantile-normal plot of the residuals. qqnorm(model2$residuals) qqline(model2$residuals) These plots indicate some evidence against normality and constant variance. The fact that the residuals do not exactly follow a normal distribution decreases the confidence you can place on your inferences (e.g., stating a particular variable is significant in explaining variations in the response variable). To improve model adequacy you might transform the response variable or use a weighted regression model. We will return to this shortly. Using the model to make predictions https://easystats.github.io/performance/reference/check_predictions.html Posterior predictive checks mean simulating replicated data under the fitted model and then comparing these to the observed data. Posterior predictive checks are used to look for systematic discrepancies between real and simulated data. The {performance} package gives posterior predictive check methods for a variety of frequentist and Bayesian models. performance::check_predictions(model2) ## Warning: Maximum value of original data is not included in the ## replicated data. ## Model may not capture the variation of the data. The model predicted lines should resemble the observed data line (smoothed density of response variable). Here you see that it looks pretty good, but improvements could be made. Example: Predicting house prices A Realtor can use multiple variable regression to justify a selling price for a house based on a list of features the house has. Here you consider a data file (houseprice.txt) containing a random sample of 107 home sales in Albuquerque, New Mexico during the period February 15 through April 30, 1993 (Albuquerque Board of Realtors, 1993). Get the data. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/houseprice.txt&quot; hp.df &lt;- readr::read_table(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## price = col_double(), ## sqft = col_double(), ## custom = col_double(), ## corner = col_double(), ## taxes = col_double() ## ) head(hp.df) ## # A tibble: 6 × 5 ## price sqft custom corner taxes ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2050 2650 1 0 1639 ## 2 2080 2600 1 0 1088 ## 3 2150 2664 1 0 1193 ## 4 2150 2921 1 0 1635 ## 5 1999 2580 1 0 1732 ## 6 1900 2580 0 0 1534 The data include: price: Selling price in $100s sqft: Square feet of living space custom: Whether the house was built with custom features (1) or not (0) corner: Whether the house sits on a corner lot (1) or not (0) taxes: Annual taxes in $ Here you assume that taxes determine price. In some (many?) real estate contexts the causality would work in the opposite direction: selling price affects appraisals and hence taxes. Use the ggpairs() function from the {GGally} package to get a look at your data. GGally::ggpairs(hp.df) As expected selling prices increase with size of the living space and with taxes. Scatter plots are not very informative for binary variables (variables with only two values). The response variable is selling price (price). You begin with living space (sqft) and whether the house was custom built as the two explanatory variables. model1 &lt;- lm(price ~ sqft + custom, data = hp.df) summary(model1) ## ## Call: ## lm(formula = price ~ sqft + custom, data = hp.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1034.29 -105.79 10.53 81.55 648.11 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 143.45904 68.76297 2.086 0.03940 * ## sqft 0.53696 0.04299 12.489 &lt; 2e-16 *** ## custom 172.20996 54.51139 3.159 0.00207 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 199.3 on 104 degrees of freedom ## Multiple R-squared: 0.7357, Adjusted R-squared: 0.7306 ## F-statistic: 144.8 on 2 and 104 DF, p-value: &lt; 2.2e-16 The model indicates that for a one square foot increase in living space, selling prices increase by $54 controlling for whether or not it has custom features. You move the decimal place on the coefficient to the right two places because selling price is in 100s of dollars. The model also indicates that custom-featured houses (indicated with a value of 1 in the variable custom) sell for more by $17,211 on average controlling for living space. To predict the selling price of a house that has custom features and has 3500 square feet of living space type predict(model1, newdata = data.frame(sqft = 3500, custom = 1), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 2195.045 2057.235 2332.855 The model predicts a selling price of $219.5K with a 95% uncertainty interval between $205.7K and $233.3K. Collinearity With more than one explanatory variable in a regression model you need to consider the possibility that the model coefficients are imprecise (small changes in input can lead to large changes in the model) due to collinearity (multicollinearity). Collinearity is when explanatory variables are highly correlated. If collinearity exists then your ability to properly interpret the model is compromised. As an example, consider the data set called fat.txt containing measurements related to body fat from 47 individuals. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/fat.txt&quot; bf.df &lt;- readr::read_table(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## bodyfat = col_double(), ## abdomen = col_double(), ## biceps = col_double(), ## forearm = col_double(), ## wrist = col_double() ## ) head(bf.df) ## # A tibble: 6 × 5 ## bodyfat abdomen biceps forearm wrist ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 12.6 85.2 32 27.4 17.1 ## 2 6.9 83 30.5 28.9 18.2 ## 3 24.6 87.9 28.8 25.2 16.6 ## 4 10.9 86.4 32.4 29.4 18.2 ## 5 27.8 100 32.2 27.7 17.7 ## 6 20.6 94.4 35.7 30.6 18.8 The response variable bodyfat is a percentage measured using special equipment. The four explanatory variables are circumferences of different body parts and are easy to measure. So you are interested in a model to predict bodyfat from these easier to measure variables. As always, you start with a grid of scatter plots. GGally::ggpairs(bf.df) Here abdomen has the strongest linear relationship with bodyfat, but other variables have high direct (positive) correlation. Note the large correlation between the explanatory variables. Next you regress bodyfat onto the four explanatory variables and summarize the model. model1 &lt;- lm(bodyfat ~ abdomen + biceps + forearm + wrist, data = bf.df) summary(model1) ## ## Call: ## lm(formula = bodyfat ~ abdomen + biceps + forearm + wrist, data = bf.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.1250 -3.2277 0.0787 3.3325 7.7094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.02650 10.57861 1.042 0.30322 ## abdomen 0.78426 0.06758 11.606 1.1e-14 *** ## biceps -0.89571 0.31660 -2.829 0.00713 ** ## forearm 1.45378 0.46132 3.151 0.00299 ** ## wrist -4.29502 0.90214 -4.761 2.3e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.018 on 42 degrees of freedom ## Multiple R-squared: 0.8173, Adjusted R-squared: 0.7999 ## F-statistic: 46.98 on 4 and 42 DF, p-value: 5.68e-15 What is wrong here? The negative coefficient on the variable biceps is opposite of the sign on the correlation between bodyfat and biceps. cor(bf.df$bodyfat, bf.df$biceps) ## [1] 0.4720003 This is an indication of collinearity. The problem is that there is a large correlation between abdomen and biceps leading to a model that may not make physical sense. Key idea: The rule is that when the correlation between two explanatory variables exceeds .6, collinearity can be a problem. When explanatory variables have large correlation then estimates of the model parameters are not precise. A model with imprecise parameter estimates is not useful. The best way to proceed in this situation is to reduce the set of explanatory variables. Remove the explanatory variable that makes the least sense from physical arguments. For these data, it is probably best to remove all variables except abdomen. Interactions Sometimes the relationship between an explanatory variable and the response variable depends on another explanatory variable. In this case we say there is an interaction between the two explanatory variables. For instance, the relationship between selling price and living space might depend on whether the house was custom built. To see if the interaction might need to be included in the regression model first make a plot. ggplot(data = hp.df, mapping = aes(y = price, x = sqft, color = factor(custom))) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab(&quot;Living Space (sq ft)&quot;) + ylab(&quot;Selling Price ($100)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Here the conditioning variable must be a factor. You see both regression lines have about the same slope. This implies the relationship between selling price and living space does not depend on whether the house has custom features. In contrast, whether or not the house is on the corner influences the relationship between selling price and living space. ggplot(data = hp.df, mapping = aes(y = price, x = sqft, color = factor(corner))) + geom_point() + geom_smooth(method = lm, se = FALSE) + xlab(&quot;Living Space (sq ft)&quot;) + ylab(&quot;Selling Price ($100)&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; The slope of the regression line between selling price and house living space is different for corner and non-corner houses. The regression lines have different slopes and cross. This is indicates an interaction between the continuous variable of living space and the categorical variable of corner. To add an interaction term to the model, you use the : notation. You need to know how the categorical variable is coded to correctly interpret the model [On a corner lot (1), or not (0)] model1 &lt;- lm(price ~ sqft + corner + sqft:corner, data = hp.df) summary(model1) ## ## Call: ## lm(formula = price ~ sqft + corner + sqft:corner, data = hp.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -424.73 -87.14 0.13 106.33 658.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -149.04760 64.12010 -2.325 0.0221 * ## sqft 0.75172 0.03701 20.309 &lt; 2e-16 *** ## corner 629.41974 119.09627 5.285 7.07e-07 *** ## sqft:corner -0.45344 0.06651 -6.818 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 168.2 on 103 degrees of freedom ## Multiple R-squared: 0.8136, Adjusted R-squared: 0.8082 ## F-statistic: 149.8 on 3 and 103 DF, p-value: &lt; 2.2e-16 Or, you can use the notation * to add both the main effects and the interaction effect to the model. Models with interaction effects should also include the main effects, even if these main effects are not significant. model1b &lt;- lm(price ~ sqft * corner, data = hp.df) summary(model1b) ## ## Call: ## lm(formula = price ~ sqft * corner, data = hp.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -424.73 -87.14 0.13 106.33 658.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -149.04760 64.12010 -2.325 0.0221 * ## sqft 0.75172 0.03701 20.309 &lt; 2e-16 *** ## corner 629.41974 119.09627 5.285 7.07e-07 *** ## sqft:corner -0.45344 0.06651 -6.818 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 168.2 on 103 degrees of freedom ## Multiple R-squared: 0.8136, Adjusted R-squared: 0.8082 ## F-statistic: 149.8 on 3 and 103 DF, p-value: &lt; 2.2e-16 The model indicates that for a one square foot increase in living space, selling prices increase by $75 for the non-corner houses. The coefficient for the interaction is -.453 which is the difference in slope between the non-corner and corner houses, i.e., the slope for the corner houses is .752 - .453 = .299 (~$30). Lastly the coefficient on the corner term of 629 is the difference in the intercepts between non-corner and corner. Or the difference in the y-intercept when living space has a value of zero. The coefficient is significant when living space is zero but this is not realistic. You transform the variables and refit the model. model1c &lt;- lm(price ~ I(sqft - mean(sqft)) + corner + I(sqft - mean(sqft)):corner, data = hp.df) summary(model1c) ## ## Call: ## lm(formula = price ~ I(sqft - mean(sqft)) + corner + I(sqft - ## mean(sqft)):corner, data = hp.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -424.73 -87.14 0.13 106.33 658.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1104.25833 18.13734 60.883 &lt; 2e-16 *** ## I(sqft - mean(sqft)) 0.75172 0.03701 20.309 &lt; 2e-16 *** ## corner -126.57787 40.95871 -3.090 0.00257 ** ## I(sqft - mean(sqft)):corner -0.45344 0.06651 -6.818 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 168.2 on 103 degrees of freedom ## Multiple R-squared: 0.8136, Adjusted R-squared: 0.8082 ## F-statistic: 149.8 on 3 and 103 DF, p-value: &lt; 2.2e-16 model1d &lt;- lm(price ~ I(sqft - 1500) + corner + I(sqft - 1500):corner, data = hp.df) summary(model1d) ## ## Call: ## lm(formula = price ~ I(sqft - 1500) + corner + I(sqft - 1500):corner, ## data = hp.df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -424.73 -87.14 0.13 106.33 658.42 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 978.52518 19.09682 51.240 &lt; 2e-16 *** ## I(sqft - 1500) 0.75172 0.03701 20.309 &lt; 2e-16 *** ## corner -50.73529 42.69095 -1.188 0.237 ## I(sqft - 1500):corner -0.45344 0.06651 -6.818 6.44e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 168.2 on 103 degrees of freedom ## Multiple R-squared: 0.8136, Adjusted R-squared: 0.8082 ## F-statistic: 149.8 on 3 and 103 DF, p-value: &lt; 2.2e-16 In both cases the intercept difference is negative. In the second case it is insignificant. When interpreting the results from models that contain interaction terms, the rule is to not interpret the coefficients on the main effects (marginal coefficients). As demonstrated the presence of interactions implies that the meaning of coefficients for terms vary depending on the values of the other variables and thus are not easily interpreted. "],["thursday-october-27-2022.html", "Thursday, October 27, 2022 Removing variables from a regression model The stepwise selection procedure Transforming a response variable Regression trees", " Thursday, October 27, 2022 Today {-} Removing variable from a regression model The stepwise selection procedure Transforming a response variable Regression trees Removing variables from a regression model Often you start with a model that includes many explanatory variables. This is because you are not sure what variables are important in explaining the response variable. A model with all the possible explanatory variables is called a ‘full model’. The R squared value is largest when all explanatory variables are included in the model. But there is a cost. This cost is called ‘model bias’. The model is biased toward the particular set of data used to create the model. If there is too much bias then the model will make poor predictions when given another set of similar data. Model bias/not data bias. When you remove a variable from a model, you decrease the variance explained but you add a degree of freedom. This makes the model less biased. It is less tuned to the data you have. A balance is achieved in this trade-off between minimizing bias and maximizing variance explained by considering the adjusted R squared value. You choose a model that has the largest adjusted R squared. If the adjusted R squared gets smaller AFTER you remove a particular explanatory variable from the full model, then you should keep that variable in the model. The AIC (Akaike Information Criterion) is similar to the adjusted R squared. It helps achieve a balance between maximizing the variance explained and minimizing the bias. Using math, AIC = \\(2 k - 2 \\log(L)\\), where \\(k\\) is the number of model parameters (1 + number of explanatory variables) and \\(L\\) is the highest value from the likelihood function. The likelihood function (likelihood) is the probability of observing the data you have, given the model. The AIC rewards good fitting models with a penalty for the size of the model (number of explanatory variables). Put simply: given a set of candidate models for your data, you should choose the one with the lowest AIC. Let’s see how this works. Returning to the model for gas consumption at the state level. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt&quot; PC.df &lt;- readr::read_table(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Petrol.Tax = col_double(), ## Avg.Inc = col_double(), ## Pavement = col_double(), ## Prop.DL = col_double(), ## Petrol.Consumption = col_double() ## ) head(PC.df) ## # A tibble: 6 × 5 ## Petrol.Tax Avg.Inc Pavement Prop.DL Petrol.Consumption ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 3571 1976 0.525 541 ## 2 9 4092 1250 0.572 524 ## 3 9 3865 1586 0.58 561 ## 4 7.5 4870 2351 0.529 414 ## 5 8 4399 431 0.544 410 ## 6 10 5342 1333 0.571 457 First you fit a full model that includes all four explanatory variables for explaining the variation in gas consumption. They are Prop.DL, Pavement, Avg.Inc, and Petrol.Tax. You regress gas consumption onto these four variables and save the object as modelFull. modelFull &lt;- lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax, data = PC.df) Then use the drop1() function, which takes the regression model object and returns a table showing what happens when each variable is successively removed from the model. drop1(modelFull) ## Single term deletions ## ## Model: ## Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax ## Df Sum of Sq RSS AIC ## &lt;none&gt; 189050 407.37 ## Prop.DL 1 212355 401405 441.51 ## Pavement 1 2252 191302 405.94 ## Avg.Inc 1 65729 254779 419.69 ## Petrol.Tax 1 31632 220682 412.80 The table includes the degrees of freedom for each variable (Df) the sum of the squares, the residual sum of squares (RSS) and the AIC value (AIC). The first row of the table (labeled &lt;none&gt;) shows the RSS and the AIC value for the full model (all explanatory variables). The RSS is 189050. Recall that this value is computed as the sum of the residuals squared and is identical to the model deviance. sum(residuals(modelFull)^2) ## [1] 189050 deviance(modelFull) ## [1] 189050 Model deviance has units of the response variable squared and you want this value to be as small as possible, but without making the model too biased. The first row also gives the AIC value for the full model. The AIC has no units. Here the value for AIC from the full model is 407.37. Think of this as the goal line. Variables that, when removed from the model, have a value that exceeds this line should be kept in the model. Variables that when removed from the model have a value that falls short of this line should be removed from the model. After the first row, successive rows show what happens to the AIC value after the particular variable is removed from the full model. The row labeled Prop.DL tells us what happens when the explanatory variable Prop.DL is removed. The RSS increases to 401405 (from 189050) and you gain one degree of freedom (less bias). Said another way, the value of 212355 in the Sum of Sq column indicates how much the variable Prop.DL is worth to the model. By removing Prop.DL from the full model, the RSS increases from 189050 to 401405; a difference of 212355. But the AIC is 441.51, which exceeds 407.37 (the goal line) so Prop.DL should be kept in the model. The next row labeled Pavement gives the same information but for the Pavement variable. By removing Pavement from the full model, the RSS increases from 189050 to 19130; a difference of 2252. But the AIC is 405.04, which falls short of 407.37 (the goal line) so Pavement should be removed from the model. The AIC column keeps the score on which variables should be kept and which should be removed. The AIC for the model having all the explanatory variables is 407.37. A model without Prop.DL has an AIC of 441.51, which is LARGER than the AIC for the full model, so you keep Prop.DL in the model. In contrast, a model without Pavement has an AIC of 405.94, which is SMALLER than the AIC for the full model, so you remove Pavement from the model. The variable Avg.Inc when removed results in an AIC that exceeds the value of 407.37 so it should be kept in the model as does the variable Petrol.Tax. In summary, to simplify a multiple variable regression model you compare the AIC values for each variable against the AIC value for the full model. If the AIC value for a variable is less than the AIC for the full model then the trade-off is in favor of removing it. The table indicates that Pavement should be removed. The next step is to create a reduced model and then check the single-variable deletions again. That is you repeat the drop1() function on the reduced model. modelReduced &lt;- lm(Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax, data = PC.df) drop1(modelReduced) ## Single term deletions ## ## Model: ## Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax ## Df Sum of Sq RSS AIC ## &lt;none&gt; 191302 405.94 ## Prop.DL 1 243586 434889 443.36 ## Avg.Inc 1 69532 260834 418.82 ## Petrol.Tax 1 33742 225044 411.74 Now all the AIC values exceed the value given in the row labeled &lt;none&gt;, so you conclude there is no reason to remove any of the remaining variables so you are finished and this is your final model. The stepwise selection procedure With only a hand full of variables to check the drop1() function is recommended. With a large number of variables an automated procedure is preferred. Stepwise selection (sometimes called stepwise regression) is a procedure (not a model) that automates this. It finds the best model from a set of candidate models when the models are nested, and it can be useful for sifting through a large number of explanatory variables. Stepwise regression can be done by backward deletion of variables or by forward selection of the variables. Backward deletion amounts to automating the drop1() function and forward selection amounts to automating the add1() function. Both drop1() and add1() use the AIC as a criterion for choosing. To see it implemented you return to the full model and apply the step() function. step(modelFull) ## Start: AIC=407.37 ## Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax ## ## Df Sum of Sq RSS AIC ## - Pavement 1 2252 191302 405.94 ## &lt;none&gt; 189050 407.37 ## - Petrol.Tax 1 31632 220682 412.80 ## - Avg.Inc 1 65729 254779 419.69 ## - Prop.DL 1 212355 401405 441.51 ## ## Step: AIC=405.94 ## Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 191302 405.94 ## - Petrol.Tax 1 33742 225044 411.74 ## - Avg.Inc 1 69532 260834 418.82 ## - Prop.DL 1 243586 434889 443.36 ## ## Call: ## lm(formula = Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax, ## data = PC.df) ## ## Coefficients: ## (Intercept) Prop.DL Avg.Inc Petrol.Tax ## 307.32790 1374.76841 -0.06802 -29.48381 As another example, let’s consider the stackloss data frame. ?stackloss head(stackloss) ## Air.Flow Water.Temp Acid.Conc. stack.loss ## 1 80 27 89 42 ## 2 80 27 88 37 ## 3 75 25 90 37 ## 4 62 24 87 28 ## 5 62 22 87 18 ## 6 62 23 87 18 You are interested in regressing stack loss (stack.loss) onto air flow (Air.Flow), water temperature (Water.Temp), and acid concentration (Acid.Conc.). modelFull &lt;- lm(stack.loss ~ Water.Temp + Air.Flow + Acid.Conc., data = stackloss) Note that with many explanatory variables you can write the model formula as stack.loss ~ .. The period after the tilde indicates that you want all the remaining columns in the data frame as explanatory variables saving the effort of typing all the variable names. step(modelFull) ## Start: AIC=52.98 ## stack.loss ~ Water.Temp + Air.Flow + Acid.Conc. ## ## Df Sum of Sq RSS AIC ## - Acid.Conc. 1 9.965 188.80 52.119 ## &lt;none&gt; 178.83 52.980 ## - Water.Temp 1 130.308 309.14 62.475 ## - Air.Flow 1 296.228 475.06 71.497 ## ## Step: AIC=52.12 ## stack.loss ~ Water.Temp + Air.Flow ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 188.80 52.119 ## - Water.Temp 1 130.32 319.12 61.142 ## - Air.Flow 1 294.36 483.15 69.852 ## ## Call: ## lm(formula = stack.loss ~ Water.Temp + Air.Flow, data = stackloss) ## ## Coefficients: ## (Intercept) Water.Temp Air.Flow ## -50.3588 1.2954 0.6712 Backward deletion of variables is the default method with the step() function. That is a successive application of the drop1() function. In the case of having a large number of explanatory variables it’s a good idea to also try forward selection to see if the results are the same. This is done with the argument direction = \"forward\". Transforming a response variable When the response variable can’t be adequately described by a normal distribution often a simple transformation of the variable will make it approximately normal. Consider the following data set containing pollution levels in 41 cites. The data are in the file pollute.txt. The data set contains levels of pollution in 41 cites along with explanatory variables including average temperature, the amount of industry, the population, the average wind speed, the amount of rain, and the number of wet days. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/pollute.txt&quot; pollute.df &lt;- readr::read_table(url) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Pollution = col_double(), ## Temp = col_double(), ## Industry = col_double(), ## Population = col_double(), ## Wind = col_double(), ## Rain = col_double(), ## Wet.days = col_double() ## ) head(pollute.df) ## # A tibble: 6 × 7 ## Pollution Temp Industry Population Wind Rain Wet.days ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 24 61.5 368 497 9.1 48.3 115 ## 2 30 55.6 291 593 8.3 43.1 123 ## 3 56 55.9 775 622 9.5 35.9 105 ## 4 28 51 137 176 8.7 15.2 89 ## 5 14 68.4 136 529 8.8 54.5 116 ## 6 46 47.6 44 116 8.8 33.4 135 Here you want to fit a multiple regression model that can predict pollution levels (Pollution) from the explanatory variables including average temperature, the amount of industry, the population, the average wind speed, the amount of rain, and the number of wet days. You start with a histogram and a density plot of the response variable Pollution. library(ggplot2) ggplot(data = pollute.df, mapping = aes(x = Pollution)) + geom_histogram(binwidth = 10, color = &quot;white&quot;) sm::sm.density(pollute.df$Pollution, model = &quot;Normal&quot;) The response variable Pollution (SO2 concentration in ppm) can not be described by a normal distribution. You can still use regression but you should transform the response variable to make the distribution look more like a normal distribution. What transform should you use? Raising a variable \\(y\\) to some power is most common. Box and Cox (1964) suggested a family of transformations, called the power transformations, to make the data histogram look more like a normal distribution. \\[ y_\\lambda&#39; = \\frac{y^\\lambda-1}{\\lambda} \\] The particular transformation depends on the value of \\(\\lambda\\). The choice of which value of \\(\\lambda\\) to use is determined using the boxcox() function from the {MASS} package. The first argument is a lm() formula. Also needed are the data and a sequence of potential value for \\(\\lambda\\) (lambda). Here you are interested in the response variable Pollution without any explanatory variables so the model formula is Pollution ~ 1. MASS::boxcox(Pollution ~ 1, data = pollute.df, lambda = seq(-1, 1, length = 10)) The output is a plot of the likelihood as a function of possible values for \\(lambda\\). The likelihood represents the chance of observing the data given the transformation and the normal distribution as the model. You choose the lambda that maximizes the likelihood. In this case it is -.25. Note: \\(\\lambda\\) is a random variable and thus the 95% uncertainty interval provides a range of values. Here the log-likelihood is maximized between -.75 and .2. Create a new variable (Pollution2) that is the transformed Pollution variable (with lambda = -.25 in the above formula). pollute.df &lt;- pollute.df |&gt; dplyr::mutate(Pollution2 = (Pollution^(-.25) - 1) / -.25) sm::sm.density(pollute.df$Pollution2, model = &quot;Normal&quot;) The variable Pollution2 can be described by a normal distribution. More precisely the concern is the distribution of the model residuals. The distribution of the response variable is not of big concern. The normality assumption under girding accurate inferences made with the model is that the distribution of residuals are described by a normal distribution. To determine the appropriate Box-Cox transformation on your response variable so that the model residuals can be described as normal use the model formula inside the boxcox() function. MASS::boxcox(Pollution ~ Temp + Industry + Population + Wind + Rain, data = pollute.df, lambda = seq(-1, 1, length = 10)) Regression trees A regression tree (decision tree) is a type of machine learning algorithm. Machine learning focuses on prediction rather than on inference or causality. Linear regression is a prediction model for the conditional mean. The conditional mean is the average response for specific values of the explanatory variables. For example, the average maximum heart rate for specific values of age. A regression tree is also a prediction model for the conditional mean. But with a regression tree the conditioning is done by dividing the space defined by the explanatory variables into distinct groups. Given a set of data a regression tree is trained to predict what the average response will be for specific values of the explanatory variables within the data set. To train a regression tree you can use the tree() function from the {tree} package. As an example you train a regression tree on the response variable Pollution from the data frame pollute.df. library(tree) tree.model &lt;- tree(Pollution ~ Temp + Industry + Population + Wind + Rain, data = pollute.df) tree.model ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 41 22040.00 30.05 ## 2) Industry &lt; 748 36 11260.00 24.92 ## 4) Population &lt; 190 7 4096.00 43.43 * ## 5) Population &gt; 190 29 4187.00 20.45 ## 10) Temp &lt; 61.55 22 3217.00 23.68 ## 20) Rain &lt; 36.165 9 296.00 15.00 * ## 21) Rain &gt; 36.165 13 1773.00 29.69 ## 42) Population &lt; 513.5 6 179.30 22.67 * ## 43) Population &gt; 513.5 7 1043.00 35.71 * ## 11) Temp &gt; 61.55 7 17.43 10.29 * ## 3) Industry &gt; 748 5 3002.00 67.00 * The output is a table listing the explanatory variables, splits, number of cases, and deviances (RSS). A plot helps make sense of this output. Here you use the generic plot() method and add text with the text() function. plot(tree.model) text(tree.model) The plot looks like an upside down tree. The branches (called splits) are the rules. Rules (involve the ‘less than’ symbol) are applied to the explanatory variables. Branches end with leaves. The value at the leaf is the mean of the response variable for that subset of observations. In this case the first split of the response variable Pollution is on the explanatory variable Industry. The split is a rule on Industry. Is the value of Industry less than 748 units? If yes, branch to the left; if no branch to the right. The left branch leads to another split. The right branch leads to a leaf (terminal node). Following the left branch the next split is on Population. Is the value of Population less than 190 units? If yes, branch to the left; if no branch to the right. The left branch leads to a terminal node. The right branch leads to another split, this time on Wet.days. The model is fit using binary partitioning. The response variable is split along coordinate axes of the explanatory variables so that at any node, the split which maximally distinguishes the response variable is selected. Splitting continues until nodes cannot be split or until there are too few cases (less than 6 by default). The variable that “explains” the greatest amount of variation in the response variable is selected first. Since the first split is on the variable Industry at a value of 748, let’s look at how Pollution is partitioned on this variable. First create a logical vector called split containing TRUE and FALSE depending on whether the value is less than 748. Then use this variable to select the values of Pollution (from the pollute.df) to compute the mean. split &lt;- pollute.df$Industry &lt; 748 mean(pollute.df$Pollution[split]) ## [1] 24.91667 mean(pollute.df$Pollution[!split]) ## [1] 67 Or using functions from the {dplyr} package. pollute.df |&gt; dplyr::mutate(split = Industry &lt; 748) |&gt; dplyr::group_by(split) |&gt; dplyr::summarize(Avg = mean(Pollution)) ## # A tibble: 2 × 2 ## split Avg ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 67 ## 2 TRUE 24.9 Thus when Industry is less than 748 the mean value for Pollution is 24.9 and when it is greater or equal to 748 the mean value for Pollution is 67 and when it is less than 748, the mean value of Pollution is 24.9. Graphically you can illustrate this using the following lines of code (code chunk). library(ggplot2) ggplot(data = pollute.df, mapping = aes(x = Industry, y = Pollution)) + geom_point(size = 2) + geom_vline(xintercept = 748, linetype = &#39;dashed&#39;) + geom_segment(x = 748, xend = max(pollute.df$Industry), y = 67, yend = 67) + geom_segment(x = 0, xend = 748, y = 24.92, yend = 24.92) The graph shows that the split of Pollution on Industry &lt; 748 results in two means for Pollution that are different. The split results in less than 6 observations of Pollution for Industry &gt;= 748, so a terminal node results and the mean value of Pollution for these (5) observations is 67. To check if the split is statistically significant by typing t.test(Pollution ~ split, data = pollute.df) ## ## Welch Two Sample t-test ## ## data: Pollution by split ## t = 3.337, df = 4.4887, p-value = 0.02428 ## alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0 ## 95 percent confidence interval: ## 8.52294 75.64373 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 67.00000 24.91667 The relatively large \\(t\\) value (greater than 3) results in a small \\(p\\)-value (less than .05) indicating you can safely reject the null hypothesis. Let’s try another split. What do you find? split &lt;- pollute.df$Industry &lt; 720 t.test(Pollution ~ split, data = pollute.df) ## ## Welch Two Sample t-test ## ## data: Pollution by split ## t = 2.2762, df = 5.4971, p-value = 0.06712 ## alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0 ## 95 percent confidence interval: ## -3.193031 67.507317 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 57.50000 25.34286 The \\(t\\) value goes down and the \\(p\\)-value increases indicating that splitting the variable Industry at a value of 720 is not as good as splitting it at a value of 748. Can you find a better split of Pollution along Industry? Here is how you can answer that question. tvalue &lt;- numeric() be &lt;- quantile(pollute.df$Industry, c(.05, .95)) for(i in be[1]:be[2]){ split = pollute.df$Industry &lt; i tvalue = c(tvalue, t.test(pollute.df$Pollution ~ split)$statistic) } df &lt;- data.frame(Industry = be[1]:be[2], tvalue) ggplot(data = df, mapping = aes(x = Industry, y = tvalue)) + geom_line() + geom_vline(xintercept = 748, color = &quot;red&quot;) + xlab(&quot;Industry&quot;) + ylab(&quot;t statistic&quot;) The split value (748) found by the regression tree for the variable Industry maximizes the \\(t\\)-value over all such splits. How a regression tree is trained. For a given explanatory variable, the algorithm Selects a candidate split value. Computes the mean of the response above and below the value. Computes the deviance (residual sum of squares) from the two means. Goes through all possible splits. Looks to see which split gives the lowest deviance. This is the split that gives the largest \\(t\\) statistic. Splits the entire data set into high and low subsets based on this split value. Repeats this procedure on each subset of the data on either side of the first split. Keeps going until no additional reduction in deviance is obtained, or there are too few data values to merit further divisions. The importance of a split to the regression tree is the reduction in the deviance and the importance is represented on the tree plot as the length of the hanging branch. The reduction in deviance is larger for the split on Population than for the split on Wet.days. The two key components of the training algorithm are: (1) Which variables to use in the split, and (2) how to best achieve the split given a variable. Tree regression is similar to forward selection in the stepwise regression procedure. Include the most important variable (the one that explains the most variance in the response) first then see if additional variables decrease the deviance keeping track of the cost associated with the loss of the degree of freedom. Model output from a trained regression tree is printed by typing the name of the model object. tree.model The terminal nodes are denoted with an asterisk (6 of them). The node number is labeled on the left with the name of the variable on which the split is made. Next is the split criterion which shows the threshold value and an inequality sign. The number of observations going into the split (or terminal node) comes next. The deviance value and the mean over all observations are shown as the next two values. At the root node the mean is 30.05 (mean(Pollution)) with a deviance of 22040 from 41 observations. The split on Industry at a threshold of 748 reduces the deviance to 11260 for values of Pollution less than 748 (node 2) and to 3002 for values of Pollution greater than or equal to 748 (node 3). The reduction in deviance is 22040 - (11260 + 3002) Since node 3 has less than 6 observations, it becomes a terminal node. The remaining deviance from node 2 is split using Population at a value of 190. Note how the nodes are nested: within node 2, for example, node 4 is terminal but node 5 is not; within node 5 node 10 is terminal but node 11 is not; within node 11, node 23 is terminal but node 22 is not, and so on. The highest mean value of Pollution according to this tree regression is 67 (node 3) and the lowest is 12 (node 10). The most extreme observations of Industry stand alone in leading to a large level of pollution. For the rest, Population is the most important variable, but it is low populations that are associated with the highest levels of pollution. For high levels of population (Population &gt; 190), the number of wet days becomes a key determinant of pollution; places with the fewest wet days (less than 108 per year) have the lowest pollution levels. For those places with more than 108 wet days, it’s temperature that is most important in explaining variation in pollution levels; the warmest places have the lowest air pollution levels. For the cooler places with more wet days, it is wind speed that matters; windier places are less polluted. This kind of nuanced and contingent explanation of how the explanatory variables influence the response is made possible with a regression tree. The cost is the potential to interpret noise as signal. Regression trees are used mainly for predictions. Predicted values are easy to anticipate. For example it is obvious that, given the model, if Industry exceeds 748, then the Pollution level is predicted to be 67. If Industry is less than 748 and Population is less than 190, then the Pollution level is predicted to be 43.43. The predict() method works like it does for a linear regression model object. The first argument is the model object and the next is the data frame containing the variables. predict(tree.model, newdata = data.frame(Industry = 750, Population = 0, Wet.days = 0, Temp = 0, Wind = 0, Rain = 0)) ## 1 ## 67 predict(tree.model, newdata = data.frame(Industry = 740, Population = 180, Wet.days = 0, Temp = 0, Wind = 0, Rain = 0)) ## 1 ## 43.42857 Note that you need to specify all the explanatory variables even if they are not involved with the final predicted value. predict(tree.model, newdata = data.frame(Industry = 740, Population = 200, Wet.days = 120, Temp = 50, Wind = 8, Rain = 0)) ## 1 ## 15 Prediction amounts to following the rules defined by the regression tree. You start with the value of the variable Industry and work your way down until you come to a terminal node. The predicted value is the group mean of the response. The standard error on the predicted value is more difficult to work out. You could use the standard error on the subset mean for that leaf. But this ignores the uncertainty surrounding the other decisions (branches) that came before. There is no simple way to get an uncertainty estimate about the predicted level of pollution. Simplifying the model is a trade-off between minimizing the bias and maximizing the variance explained. If the regression tree has many explanatory variables with many branches then it will explain more variance, but it is biased toward the particular data set. If the tree is too simple it will not explain much of the variance. If the tree is too complex predictions on new data will be poor. Pruning the tree helps balance this trade-off. The prune.tree() function gives a sequence of smaller trees from the largest tree by removing the least important splits. The function returns the tree size (number of terminal nodes), the total deviance of each tree, and the increase in deviance by going from a larger tree to a smaller tree. prune.tree(tree.model) ## $size ## [1] 6 5 3 2 1 ## ## $dev ## [1] 8633.905 9183.912 11284.887 14262.750 22037.902 ## ## $k ## [1] -Inf 550.0073 1050.4873 2977.8633 7775.1524 ## ## $method ## [1] &quot;deviance&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; Here you see the largest tree (6 terminal nodes) results in the smallest deviance (8877). By pruning one branch the deviance increases (9240) but only by a relatively small amount (364 or 4%). Similarly by pruning a second branch the deviance increases, this time a bit more. It is helpful to plot the deviance as a function of tree size. plot(prune.tree(tree.model)) Here the plot is opposite of the table. Deviance declines with increasing complexity. Code challenge: Make this using the grammar of graphs. Hint: save the prune tree object and then create a data frame from the object lists. Then use scale_x_reverse(). You see that the amount of decline diminishes with each node (diminishing return). In fact, you can argue that a tree with 4 terminal nodes is best since the deviance from 4 to 5 nodes is relatively small. Suppose you want the best tree with four nodes. tree.model2 &lt;- prune.tree(tree.model, best = 4) plot(tree.model2) text(tree.model2) tree.model2 "],["tuesday-november-1-2022.html", "Tuesday, November 1, 2022 Predictive uncertainty with regression trees The random forest algorithm Cross validation Classification trees", " Tuesday, November 1, 2022 Today Predictive uncertainty with regression trees The random forest algorithm Cross validation Classification trees Example: Let’s return to the Palmer penguins data frame. You are interested in using machine learning (regression tree) to predict the body mass of male Gentoo penguins using the penguin’s bill and flipper dimensions. First you filter the data frame keeping only the male Gentoo penguins. Gentoo.df &lt;- palmerpenguins::penguins |&gt; dplyr::filter(species == &quot;Gentoo&quot;, sex == &quot;male&quot;) To train (fit) the model you use the tree() function from the {tree} package. The response variable is body_mass_g and the explanatory variables are bill_length_mm, bill_depth_mm, and flipper_length_mm. The first argument in the function is the model formula and the formula has the same format as that for fitting a linear regression model. library(tree) model &lt;- tree(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm, data = Gentoo.df) model ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 61 5884000 5485 ## 2) bill_length_mm &lt; 47.45 13 795800 5238 ## 4) bill_depth_mm &lt; 15.7 8 89690 5131 * ## 5) bill_depth_mm &gt; 15.7 5 467000 5410 * ## 3) bill_length_mm &gt; 47.45 48 4086000 5552 ## 6) flipper_length_mm &lt; 228.5 38 3300000 5498 ## 12) flipper_length_mm &lt; 216.5 9 942500 5375 * ## 13) flipper_length_mm &gt; 216.5 29 2179000 5536 ## 26) flipper_length_mm &lt; 224.5 20 1554000 5595 ## 52) bill_length_mm &lt; 49 5 458000 5430 * ## 53) bill_length_mm &gt; 49 15 915000 5650 ## 106) bill_length_mm &lt; 50.45 9 553900 5711 * ## 107) bill_length_mm &gt; 50.45 6 277100 5558 * ## 27) flipper_length_mm &gt; 224.5 9 402200 5406 * ## 7) flipper_length_mm &gt; 228.5 10 262200 5755 * A tree diagram helps make sense of this output. Use the generic plot() method and add text with the text() function. plot(model) text(model) The most important variable in explaining body mass of the male Gentoo penguins is bill length. It is the variable named on the top of the tree diagram. For penguins with bills less than 47.45 mm, the next most important variable is bill depth (if the condition is true, then branch to the left). For those penguins with shorter bills if bill depth is less than 15.7 mm then the average body mass is 5131 grams and if the bill depth is greater or equal to 15.7 mm then the average body mass is larger at 5410 g. Again, if the condition is tree, then branch left. For Gentoo penguins with a bills at least 47.45 mm in length, the next most important variable is flipper length. For these larger-billed penguins, if flippers are longer (at least 228.5 mm) then the average body mass is 5755 g. For these larger-bill length penguins, if flippers are shorter than 228.5 mm, then split on flipper length again. As we move down the tree the branches get smaller (the vertical lines get shorter) indicating the splits are less important in reducing the residual sum of squares (RSS). Interpreting a tree regression is easier than interpreting a linear regression because the splits have the same units as the explanatory variables. In a linear regression the coefficients have units of response variable divided by the units of the explanatory variable. Machine learning algorithms are often used solely for predictions. Given the branching structure of the tree, the predicted values are easy to anticipate. Prediction amounts to following the rules defined by the final model. You start with the value of the variable bill_length_mm and work your way down until you come to a terminal node (a branch that ends in a value rather than a split). The value is the average of the response variable conditional on the splits. For example, if bill_length_mm is less than 47.45 mm and bill_depth_mm is less than 15.7 mm, then the mass of the penguin is predicted to be 5131 grams. Said another way, 5131 is the average body mass for all male Gentoo penguins with bill length less than 47.45 and bill depth less than 15.7. You can check this as follows. Gentoo.df |&gt; dplyr::filter(bill_length_mm &lt; 47.45 &amp; bill_depth_mm &lt; 15.7) |&gt; dplyr::summarize(mean(body_mass_g)) ## # A tibble: 1 × 1 ## `mean(body_mass_g)` ## &lt;dbl&gt; ## 1 5131. The predict() method is used to make predictions with the model. The first argument is the model object and the next is the data frame containing the variables. predict(model, newdata = data.frame(bill_length_mm = 45, bill_depth_mm = 15, flipper_length_mm = 220)) ## 1 ## 5131.25 predict(model, newdata = data.frame(bill_length_mm = 45, bill_depth_mm = 16, flipper_length_mm = 220)) ## 1 ## 5410 Note that you need to specify values for all the explanatory variables (even if the variables are not involved with any of the splits). The uncertainty on the predicted value is more difficult to work out. You can use the standard error on the mean for that terminal node, but this error ignores the uncertainty surrounding what variable to split and at what value to make the split. Because of this limitation machine learning algorithms are not usually used for inference. Another question that arises is can the tree be simplified? The question is answered by considering the trade-off between minimizing the bias and maximizing the variance explained in the response variable. If the tree has many explanatory variables and lots of decision splits then it will explain more variance. But this will make the tree biased toward the particular data set used in the training and predictions made with the model on new data (data not used in the training) will be poor. On the other hand, if the tree is too simple (few variables and few splits), it will not explain much of the variance in the response. Pruning the tree solves for this trade-off. The prune.tree() function gives a sequence of smaller trees from the largest tree by removing the least important splits. The function returns the tree size (number of terminal nodes), the total deviance for each tree, and the increase in deviance by going from a larger tree to a smaller tree. prune.tree(model) ## $size ## [1] 8 7 6 4 3 2 1 ## ## $dev ## [1] 3452632 3536660 3718160 4119414 4358496 4881277 5884098 ## ## $k ## [1] -Inf 84027.78 181500.00 200627.38 239081.73 522780.84 1002821.32 ## ## $method ## [1] &quot;deviance&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;prune&quot; &quot;tree.sequence&quot; From the output you see the largest tree (8 terminal nodes) results in the smallest deviance ($dev) (3452632). By pruning one branch the deviance increases (3536660) but only by a relatively small amount (84028 which is an increase of about 2.4%). Similarly by pruning a second branch the deviance increases, this time a bit more. It is helpful to plot the deviance as a function of tree size. plot(prune.tree(model)) Deviance declines with increasing complexity. You see that the amount of decline (vertical line between the steps) generally diminishes with each node. You see that at a tree size of 6 branches the vertical lines are quite small. Code challenge: Recreate this plot using the grammar of graphs. Hint: save the prune tree object and then create a data frame from the object lists. Then use scale_x_reverse(). Suppose you want a tree with six terminal nodes. model2 &lt;- prune.tree(model, best = 6) plot(model2) text(model2) Predictive uncertainty with regression trees Let’s look at another example. Predicting Atlantic hurricanes using a regression tree. Import the annual hurricane data and filter on years since 1950. Create a data frame containing only the basin-wide hurricane counts and Southern Oscillation Index (El Nino) and SST as the two explanatory variables. The SOI has units of standard deviation and the SST has units of degrees C. df &lt;- readr::read_table(&quot;http://myweb.fsu.edu/jelsner/temp/data/AnnualData.txt&quot;) |&gt; dplyr::filter(Year &gt;= 1950) |&gt; dplyr::select(Year = Year, H = B.1, SOI = soi, SST = sst) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## RowID = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. head(df) ## # A tibble: 6 × 4 ## Year H SOI SST ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1950 11 3.43 -0.000737 ## 2 1951 8 -3.47 0.293 ## 3 1952 6 -0.567 0.399 ## 4 1953 6 -3.6 0.257 ## 5 1954 8 1.07 0.0113 ## 6 1955 9 4.23 0.252 Then fit a regression tree to these data with the annual count of hurricanes as the response variable and SOI and SST as the explanatory variables. model &lt;- tree(H ~ SOI + SST, data = df) plot(model) text(model) The predicted number of hurricanes when SOI is -2 s.d. and when SST is .2C is found using the predict() method. predict(model, newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 7.347826 So, on average, you can expect between 7 and 8 hurricanes in the Atlantic in years with the SOI is -2 s.d. and the SST is .2C above normal. The predicted value depends on the tree, which in turn depends exactly on what values of SOI and SST are used to “grow” it. To see this, you grow a new tree for hurricane count, but this time leaving the values of SOI and SST from the last year in the data frame out. Since there are 61 years, leaving the last year out amounts to sub-setting the data frame as df[-61, ]. Then you fit a new tree model (model2) and make a prediction for the same values of SOI and SST (SOI = -2 s.d. and SST = .2C). model2 &lt;- tree(H ~ SOI + SST, data = df[-61, ]) predict(model2, newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 5.714286 The predicted value is quite different even though you left out only one year of data! Predictive uncertainty occurs with any model, but the level of uncertainty is much larger with machine learning algorithms because they have many parameters. Each branch in a regression tree is a parameter and so is every split so with our two explanatory variables there are at least 10 parameters. For example, compare the predicted hurricane rates from a linear regression model with and without the last year. predict(lm(H ~ SOI + SST, data = df), newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 6.230173 predict(lm(H ~ SOI + SST, data = df[-61, ]), newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 6.219233 The difference in predicted values is very small with a linear regression model. On the order of .2% 0.1605136 compared with 22% ((7.35 - 5.71)/7.35 * 100) for the machine learning algorithm The random forest algorithm A random forest algorithm solves some of the problem of large predictive uncertainty by fitting many regression trees (a ‘forest’). The algorithm takes many samples (e.g., 100) from the set of all observations and then using these samples fits many regression trees. Given a set of new values for the explanatory variables, each regression tree provides a predicted value. The average over all predicted values is taken as the random forest prediction. The randomForest() function in the {randomForest} package provides a random forest algorithm. For comparison here you apply a random forest algorithm to the hurricane data frame. I use the word algorithm because there is no single model. The number of trees is set at 500 (by default). rfa &lt;- randomForest::randomForest(H ~ SOI + SST, data = df, ntree = 500) Then to make a prediction you use (again) the predict() method. predict(rfa, newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 4.752333 The predicted value is less sensitive to removing single observations. To see this you apply the random forest algorithm with the last observation removed. rfa &lt;- randomForest::randomForest(H ~ SOI + SST, data = df[-61, ], ntree = 500) predict(rfa, newdata = data.frame(SOI = -2, SST = .2)) ## 1 ## 4.558867 In summary: Regression trees tend to produce predicted values that are unstable, which means a small change in the data used to grow the tree results in a large difference in the predicted value. The in-sample error (the error made when using the model to predict the data that were used to fit the model) might be small, but the out-of-sample error (the error made when using the model to predict new data) can be large. A random forest algorithm solves some of the problem (but not all) of unstable predictions by growing many trees. Cross validation Why use a machine learning algorithm? They are useful when you have a large amount of data. They can find features in the data that are not accessible with linear regression. But how do you compare the predictive skill of a random forest algorithm against the predictive skill of another model? You use a method called cross validation. Cross validation helps you decide what model to use across the span of many models. Key idea: Cross validation is a method that removes noise specific to each observation and estimates how well the model/algorithm finds useful prediction rules when this coincident information is unavailable. For example, here you compare the skill of a random forest algorithm against the skill of a Poisson regression model for predicting hurricane counts. You arrange the cross validation as follows. n &lt;- length(df$H) rfx &lt;- numeric(n) prx &lt;- numeric(n) for(i in 1:n){ rfa &lt;- randomForest::randomForest(H ~ SOI + SST, data = df[-i, ]) prm &lt;- glm(H ~ SOI + SST, data = df[-i, ], family = &quot;poisson&quot;) new &lt;- df[i, ] rfx[i] &lt;- predict(rfa, newdata = new) prx[i] &lt;- predict(prm, newdata = new, type = &quot;response&quot;) } You compute the out-of-sample averaged prediction error squared (mean squared prediction error) for both models. mean((df$H - prx)^2) ## [1] 5.070468 mean((df$H - rfx)^2) ## [1] 5.36583 The Poisson regression model performs slightly better than the random forest model in this case although the difference is not large. Correlations between the actual and predicted value are cor(df$H, prx) ## [1] 0.5385559 cor(df$H, rfx) ## [1] 0.5010745 The influence each of the two explanatory variables has on the hurricane counts is seen by plotting the predictions on a grid. newdat &lt;- expand.grid(SST = seq(-.5, .7, .01), SOI = seq(-5, 4, .1)) z1 &lt;- predict(rfa, newdata = newdat) prm &lt;- glm(H ~ SOI + SST, data = df, family = &quot;poisson&quot;) z2 &lt;- predict(prm, newdata = newdat, type = &quot;response&quot;) newdat$Hrf &lt;- z1 newdat$Hpr &lt;- z2 library(ggplot2) p1 &lt;- ggplot(newdat, aes(x = SST, y = SOI, fill = Hrf)) + geom_tile() + scale_fill_viridis_c(limits = c(0, 12)) + labs(fill = &quot;Rate&quot;) + xlab(&quot;Atlantic Sea Surface Temperature (°C)&quot;) + ylab(&quot;Southern Oscillation Index (s.d.)&quot;) + ggtitle(subtitle = &quot;Random forest algorithm&quot;, label = &quot;Annual Atlantic hurricane rate&quot;) + theme_minimal() p2 &lt;- ggplot(newdat, aes(x = SST, y = SOI, fill = Hpr)) + geom_tile() + scale_fill_viridis_c(limits = c(0, 12)) + labs(fill = &quot;Rate&quot;) + xlab(&quot;Atlantic Sea Surface Temperature (°C)&quot;) + ylab(&quot;Southern Oscillation Index (s.d.)&quot;) + ggtitle(subtitle = &quot;Poisson regression model&quot;, label = &quot;Annual Atlantic hurricane rate&quot;) + theme_minimal() library(patchwork) p1 + p2 Hurricane counts increase with SST and SOI but for high values of SOI the influence of SST is stronger. For high values of SST the influence of the SOI is more pronounced. The random forest algorithm is able to capture more subtle features (plaid pattern) but at the expense of interpreting some noise as signal as seen by the relative high count with SOI values near -3 s.d. and SST values near -.1C. Classification trees When the response variable is categorical (e.g., species) a regression tree is called a classification tree. The classification tree is a set of if-then rules that results in a probability for each category. If instability is high and wind shear is strong, then predict a high chance of damaging tornadoes. If a patient’s resting heart rate exceeds 120 bpm, then predict a high chance that the patient will suffer a heart attack within the next 5 years. Consider again the penguins data frame from the {palmerpenguins} package. head(palmerpenguins::penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex year ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 male 2007 ## 2 Adelie Torgersen 39.5 17.4 186 3800 fema… 2007 ## 3 Adelie Torgersen 40.3 18 195 3250 fema… 2007 ## 4 Adelie Torgersen NA NA NA NA &lt;NA&gt; 2007 ## 5 Adelie Torgersen 36.7 19.3 193 3450 fema… 2007 ## 6 Adelie Torgersen 39.3 20.6 190 3650 male 2007 ## # … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g With species as the categorical response variable (there are three different penguin species) and the other columns as explanatory variables we can specify the model formula as species ~ .. The period after the tilde (~) indicating all other variables in the data frame. model &lt;- tree(species ~ ., data = palmerpenguins::penguins) plot(model) text(model) From the plot you see that flipper length split at 206.5 mm is the most important variable in distinguishing the species. The next two branches are bill length split at 43.45 mm and island. Since island is a categorical explanatory variable there is a :a next to the name island. This indicates that the spit is on the first island in alphabetical order. The island names are unique(palmerpenguins::penguins$island) ## [1] Torgersen Biscoe Dream ## Levels: Biscoe Dream Torgersen Biscoe Island is the first in alphabetical order so the split says that if the island is Biscoe (yes, means branch left) then predict the species is Gentoo and if the island is not Biscoe (either Dream or Torgersen) then predict the species to be Chinstrap. After these three splits resulting in four branches, the branches become very small (the vertical lines are short) indicating that these additional splits do not reduce the RSS by much at all. So you prune the tree by setting best = 4. That is, you ask the model to grow a tree with the four most important branches. model2 &lt;- prune.tree(model, best = 4) plot(model2) text(model2) You examine the model quantitatively by the misclassification error rate. This is given in the last line of the output from the summary() method. summary(model2) ## ## Classification tree: ## snip.tree(tree = model, nodes = 4:5) ## Variables actually used in tree construction: ## [1] &quot;flipper_length_mm&quot; &quot;bill_length_mm&quot; &quot;island&quot; ## Number of terminal nodes: 4 ## Residual mean deviance: 0.2791 = 91.81 / 329 ## Misclassification error rate: 0.03604 = 12 / 333 You see that the model misclassifies the species in 12 of the 333 penguins resulting in a misclassification error rate of 3.6%. Note that the full model has a misclassification error rate of only 1.8% (6 out of the 333 penguins). But this requires twice as many nodes so the model has greater bias. Suppose you are able to observe a penguin on Biscoe Island with a flipper length of 200 mm, a bill length of 44 mm, a bill depth 18 mm, and a body mass of 4000 g. What is the likely species? You use the predict() method with the newdata = argument specified as a data frame. You must include by name all the variables in the model specification. This includes island and sex as factors along with year, flipper_length_mm, bill_length_mm, bill_depth_mm, and body_mass_g as integers or numeric values. Note that since the final tree only includes the variables island, flipper_length_mm, and bill_length_mm it does not matter what values you use for the other variables. predict(model2, newdata = data.frame(island = factor(&quot;Biscoe&quot;), sex = factor(&quot;male&quot;), year = 2020, flipper_length_mm = 200, bill_length_mm = 44, bill_depth_mm = 18, body_mass_g = 4000)) ## Adelie Chinstrap Gentoo ## 1 0.06349206 0.9206349 0.01587302 The result says that there is a 92% chance that it is a Chinstrap penguin, a 6% chance that it is an Adelie penguin and a 2% chance that it is a Gentoo penguin. The ctree() function in {party} package can be used to model binary, nominal, ordinal and numeric variables. The {rpart} packages contains additional functions for modeling our data with decision trees. See also: http://rstatistics.net/decision-trees-with-r/ "],["thursday-november-3-2022.html", "Thursday, November 3, 2022 Binary outcomes and probabilities Logistic regression", " Thursday, November 3, 2022 Today Logistic regression Binary outcomes and probability The logistic regression model Linear regression is one of the most powerful tools in data science. But it’s utility is limited to response variables that can be approximated with a normal distribution. When the response variable is binary (only two outcomes) a linear regression model should not be used. Many scientific research questions involve binary outcomes, and there is often a need to predict the probability that something will happen. The probability you will get a virus. Fortunately, with only a small modification to the linear regression model you can get a logistic regression model that describes how a binary response variable is related to a set of explanatory variables. The mechanics of fitting a logistic regression model are the same as with fitting a linear regression. The model coefficients are determined using the method of maximum likelihood. The method of maximum likelihood answers the question, what is the probability of our data given the model. A model that assigns the highest probability to our data is the model that is chosen. Binary outcomes and probabilities A key understanding here is that the average value over a set of binary responses is a number between 0 and 1. To see this, here we generate a set of random binary responses using the rbinom() function. r for random and binom for binomial distribution with a long-run probability of .2. rbinom(n = 100, size = 1, prob = .2) ## [1] 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 ## [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 ## [75] 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 The n = 100 is the number of binary responses to generate, size = 1 indicates two choices (0 or 1), and prob = .2 is the long-run probability of getting a 1. If we say that an outcome of 1 is a success and that an outcome of 0 is a failure, then the average taken over all outcomes is the sample probability of success. Here we compute averages over a samples of binary outcomes where the probability of a 1 (success) is .2, .5, and .9. mean(rbinom(n = 100, size = 1, prob = .2)) ## [1] 0.16 mean(rbinom(n = 100, size = 1, prob = .5)) ## [1] 0.48 mean(rbinom(n = 100, size = 1, prob = .9)) ## [1] 0.84 Repeat. We see that the sample means are close to the long-run probabilities. Recall that a regression model predicts the conditional mean values of the response variable. When the response variable is binary, the mean values are probabilities, so a logistic regression predicts the conditional probability of something happening. For example, the probability of getting COVID-19 given your social distancing and mask-wearing habits. Probabilities are bounded between 0 and 1, but the linear regression line is not. Example: Consider data collected from the recovered solid booster rockets of the 23 previous shuttle flights (before the Challenger Space Shuttle disaster in 1986). The temperature in F at launch time and whether there was (1) or was not (0) damage to the O-rings on the booster. Temperature &lt;- c(66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 70, 78, 67, 53, 67, 75, 70, 81, 76, 79, 75, 76, 58) Damage &lt;- c(0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1) The variable Damage is the response variable and Temperature is the explanatory variable. We start with a scatter plot of these two variables along with the linear regression line. df &lt;- data.frame(Damage, Temperature) library(ggplot2) ggplot(data = df, mapping = aes(x = Temperature, y = Damage)) + geom_point() + geom_smooth(method = lm, se = FALSE) + scale_y_continuous(limits = c(0, 1)) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing missing values (geom_smooth). The plot shows more points at a value of 1 (at least some damage to the O-rings) when temperatures are lower and more more points at a value of 0 (no damage) when the temperatures are higher. The linear regression line indicates this inverse relationship. The line slopes from upper left to lower right. We conclude that there is less chance of O-ring damage when temperatures are higher. But we note that the straight line does not capture this relationship very well. We fit a linear regression model to these data. lrm &lt;- lm(Damage ~ Temperature, data = df) summary(lrm) ## ## Call: ## lm(formula = Damage ~ Temperature, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.43762 -0.30679 -0.06381 0.17452 0.89881 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.90476 0.84208 3.450 0.00240 ** ## Temperature -0.03738 0.01205 -3.103 0.00538 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3987 on 21 degrees of freedom ## Multiple R-squared: 0.3144, Adjusted R-squared: 0.2818 ## F-statistic: 9.63 on 1 and 21 DF, p-value: 0.005383 The model diagnostics indicate temperature is significant in explaining O-ring damage. Looking at the slope coefficient we can state that for every one degree F decrease in temperature the chance of damage increases by 3.7 percentage points. We interpret the predicted damage as the probability of damage for a given temperature (conditional mean). We use the predict() method to predict the probability of damage when the launch temperatures are 77 F, 65 F, and 51 F. We multiply the predicted mean values by 100 so that the probabilities are in percentages. predict(lrm, newdata = data.frame(Temperature = c(77, 65, 51))) * 100 ## 1 2 3 ## 2.642857 47.500000 99.833333 Predictions make sense. When the launch temperature is 77 F, the chance of O-ring damage is quite small (2.6%) but when the launch temperature is 51 F, the change of O-ring damage is quite large (99.8%). Although this appears to be a reasonable prediction equation, it leads to nonsensical forecasts. For example, what does the linear regression model predict when the temperature is 85 F and when it is 45 F? predict(lrm, newdata = data.frame(Temperature = c(85, 45))) * 100 ## 1 2 ## -27.2619 122.2619 These values do not make sense since probabilities are constrained to be between 0 and 100 (0 and 1). Key point: Linear regression can not capture the bounded nature of probabilities. Logistic regression A regression model has a response function and a regression structure. The response function is specified on the left side of the regression equation and the regression structure on the right side of the equation. With linear regression, the response function is the identity function. That is, for any value of \\(y\\) the function returns the value of \\(y\\). We write it as \\[ f(y) = y \\] With logistic regression, the response function is the logit function. Said another way, logistic regression is a generalization of the linear regression model where the response function is the logit function. We write the logit function as \\[ f(y) = \\hbox{logit}(y) \\] The logit is the logarithm of the odds ratio. The odds ratio is the probability of a thing happening to the probability it won’t happen. Using math \\[ \\hbox{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1 - \\pi}\\right). \\] We write odds as for:against. If there is a 2:1 odds of something happening, then the probability it will happen is 2/(1 + 2) or 2/3 (67%). The inverse of the logit function is the logistic function. With logit(\\(\\pi\\)) = \\(z\\), we have \\[ \\pi = \\frac{\\exp(z)}{1 + \\exp(z)} = \\frac{1}{1 + \\exp(-z)}. \\] To make a graph of the logistic function we use the curve() function. curve(1/(1 + exp(-x)), from = -6, to = 6, col = &quot;red&quot;, ylab = &quot;logistic function of z&quot;, xlab = &quot;z&quot;) abline(h = 1, lty = 2) abline(h = 0, lty = 2) The logistic function takes as input any value from negative infinity to positive infinity and returns a value between 0 and 1. The variable \\(z\\) can be thought of as the exposure to some set of risk factors, while the logistic function represents the chance of a particular outcome given the exposure. The variable \\(z\\) is a measure of the total contribution (weighted sum) of all the risk factors (explanatory variables). It is defined as \\[ z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p \\] where the regression coefficients are \\(\\beta_0\\),…, \\(\\beta_p\\) and the risk factors are \\(x_1\\),…, \\(x_p\\). The intercept \\(\\beta_0\\) is the value of \\(z\\) when there is no risk (base rate). The regression coefficient quantifies the amount of contribution the associated variable makes to the overall risk. For example, the magnitude of \\(\\beta_1\\) quantifies the amount of contribution variable \\(x_1\\) makes to the overall risk. A positive regression coefficient means the variable increases the probability of the outcome. If the outcome is getting COVID-19, then the variable indicating that you don’t wear a mask in public, increases the probability of that outcome. A negative regression coefficient means the variable decreases the probability of that outcome. A large regression coefficient means the variable strongly influences the probability of that outcome; while a near-zero regression coefficient means the variable has little influence on the probability of that outcome. Example: Who survives in the Donner Party? The Donner and Reed families from Springfield, Illinois headed to California in 1846. The group, which became know as the ‘Donner Party’, was stranded in the Sierra Nevada mountains by heavy late October snows. By the time the last survivor was rescued in April 1847, 40 of the 87 members had died from famine and exposure to the cold. The data file DonnerParty.txt contains a list of individuals in the Donner Party along with their age, sex, and whether or not they survived. DP.df &lt;- readr::read_table(file = &quot;http://myweb.fsu.edu/jelsner/temp/data/DonnerParty.txt&quot;) |&gt; dplyr::mutate(Survival = as.factor(Survival)) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## Last = col_character(), ## First = col_character(), ## Sex = col_character(), ## Age = col_double(), ## Survival = col_character() ## ) str(DP.df) ## tibble [45 × 5] (S3: tbl_df/tbl/data.frame) ## $ Last : chr [1:45] &quot;Antoine&quot; &quot;Breen&quot; &quot;Breen&quot; &quot;Burger&quot; ... ## $ First : chr [1:45] &quot;A&quot; &quot;Mary&quot; &quot;Patrick&quot; &quot;Charles&quot; ... ## $ Sex : chr [1:45] &quot;male&quot; &quot;female&quot; &quot;male&quot; &quot;male&quot; ... ## $ Age : num [1:45] 23 40 40 30 28 40 45 62 65 45 ... ## $ Survival: Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 1 1 1 1 1 1 1 ... The data frame contains the Name, Sex, Age, and Survival status of all individuals that were at least 15 years old. How many individuals are coded as female and how many are coded as male? table(DP.df$Sex) ## ## female male ## 15 30 How many survived? table(DP.df$Survival) ## ## no yes ## 25 20 What was the average age in the party? mean(DP.df$Age) ## [1] 31.8 What is the relationship between an individual’s age and whether or not they survived? ggplot(data = DP.df, mapping = aes(x = Survival, y = Age)) + geom_boxplot() Age appears to be a factor in explaining whether or not someone survived. Younger members of the party appear to have had a better chance of survival. What about sex as an explanatory variable? table(DP.df$Sex, DP.df$Survival) ## ## no yes ## female 5 10 ## male 20 10 The table shows that of the 15 women, 5 perished and 10 survived. This compares with 20 deaths and 10 survivors among the men. It appears that sex is also important. Relatively more women survived than men. Logistic regression quantifies these risk factors (sex and age) in terms of survival probability. The variable Survival is our dependent (response) variable coded as no and yes. The two risk factors are Sex coded as male and female and Age in years. So you fit a logistic model to these data with Survival as your response variable. You use the glm() function (generalized linear model) and specify family = binomial. logrm &lt;- glm(factor(Survival) ~ Age + Sex, data = DP.df, family = binomial) logrm ## ## Call: glm(formula = factor(Survival) ~ Age + Sex, family = binomial, ## data = DP.df) ## ## Coefficients: ## (Intercept) Age Sexmale ## 3.2304 -0.0782 -1.5973 ## ## Degrees of Freedom: 44 Total (i.e. Null); 42 Residual ## Null Deviance: 61.83 ## Residual Deviance: 51.26 AIC: 57.26 You write the model, where \\(\\pi\\) represents survival probability, as \\[ \\hbox{logit}(\\pi) = 3.23 - .0782 \\times \\hbox{Age} - 1.597 \\times \\hbox{Sex} \\] You interpret the value of 3.23 as the base rate when Age is zero and Sex is zero. Although Sex is coded as a factor variable female comes before male alphabetically so female is zero. You take the value of 3.23 as input to the logistic function and get exp(3.23)/(1 + exp(3.23)) ## [1] 0.9619478 The resulting value tells you that a newborn (Age = 0) girl would have a 96% chance of survival. You get this answer directly with the predict() method by typing predict(logrm, newdata = data.frame(Age = 0, Sex = &quot;female&quot;), type = &quot;response&quot;) ## 1 ## 0.9619629 Without the type = argument you get the logit value of 3.23. You can also interpret the model in terms of the odds ratio. For example, the model tells you that the odds ratio of a 20-year old surviving relative to a 50-year old is \\[ \\exp(-.0782 \\times (20 - 50)) \\] With code exp(-.0782 * (20 - 50)) ## [1] 10.44371 The odds of a 20-year old surviving is more than 10 times the odds of a 50-year old surviving regardless of whether the person is a man or woman. If an explanatory variable increases by one unit while the other explanatory variables remain constant, the odds will change by a multiplicative amount given by the exponent of the coefficient. The odds of a woman (Sex = 0) surviving relative to the odds of a man (Sex = 1) of the same age is exp(-1.597 * (0 - 1)) ## [1] 4.938196 Thus, the survival odds of a women are about 5 times the survival odds of a man both at the same age. You get uncertainty intervals on the coefficients using an extractor function. confint(logrm) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) 0.8514190 6.42669512 ## Age -0.1624377 -0.01406576 ## Sexmale -3.2286705 -0.19510198 You see that the 95% uncertainty intervals for both factors do not include the value of zero. This indicates that Age and Sex are significant in statistically explaining survivability. You can also check the importance of the variables with the summary() method on the model object. summary(logrm) ## ## Call: ## glm(formula = factor(Survival) ~ Age + Sex, family = binomial, ## data = DP.df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7445 -1.0441 -0.3029 0.8877 2.0472 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.23041 1.38686 2.329 0.0198 * ## Age -0.07820 0.03728 -2.097 0.0359 * ## Sexmale -1.59729 0.75547 -2.114 0.0345 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 61.827 on 44 degrees of freedom ## Residual deviance: 51.256 on 42 degrees of freedom ## AIC: 57.256 ## ## Number of Fisher Scoring iterations: 4 You see that Age and Sex are significant variables in explaining the probability of survival (\\(p\\) values less than .05). The smaller the residual deviance, the better the logistic model fits. Output shows a null deviance of 61.827 on 44 degrees of freedom. The null deviance is the deviance of a model with a constant (no explanatory variables) survival probability (44%; 20 out of 45 survived). The residual deviance for the model with Age and Sex as explanatory variables is 51.256 on 42 degrees of freedom. The difference in deviance between the null and the full model is: 61.827 - 51.256 = 10.571. The difference in degrees of freedom is 44 - 42 = 2. You compare this drop in deviance to a value from the chi-squared distribution with 2 degrees of freedom. pchisq(10.571, df = 2, lower.tail = FALSE) ## [1] 0.005064499 This is the \\(p\\) value for the model as a whole. Since it is less than .01 you state that there is convincing evidence that the model helps statistically explain survivability given these data. Here you predict the survival probability of a 50-year old man. predict(logrm, newdata = data.frame(Age = 50, Sex = &quot;male&quot;)) ## 1 ## -2.277083 By default the predicted value is the logarithm of the odds. To convert this to a probability we use the logistic function or we use the type = \"response\" argument in the predict() method. exp(-2.277)/(1 + exp(-2.277)) ## [1] 0.09304581 predict(logrm, newdata = data.frame(Age = 50, Sex = &quot;male&quot;), type = &quot;response&quot;) ## 1 ## 0.09303878 You see that, based on the data and the model, you would predict a survival probability of 9.3% for a 50-year old man. What is the survival probability for a 50-year old woman? A 25-year old woman? predict(logrm, newdata = data.frame(Age = c(50, 25), Sex = &quot;female&quot;), type = &quot;response&quot;, se.fit = TRUE) ## $fit ## 1 2 ## 0.3363082 0.7816507 ## ## $se.fit ## 1 2 ## 0.1904773 0.1153756 ## ## $residual.scale ## [1] 1 You include the argument se.fit = TRUE to get a standard error on the predicted survival probability. Assessing the adequacy of a linear regression model involves plots of the residuals. Similar diagnostics are available for assessing the adequacy of logistic regression, but they are often more difficult to interpret. For instance how do you examine the linearity assumption when the linearity is on the log of the odds? Residual deviance: Returning to the output from summary(logrm), you find that the residual deviance is 51.256 on 42 degrees of freedom. A \\(p\\) value from a \\(\\chi^2\\) distribution with a quantile value of 51.256 with this many degrees of freedom provides evidence against the null hypothesis. The null hypothesis is that the model is adequate. Thus a small \\(p\\) value in this case is evidence against the null hypothesis, which is that the model is adequate. pchisq(q = 51.256, df = 42, lower.tail = FALSE) ## [1] 0.1549141 Since the \\(p\\) value exceeds .05 you fail to reject the null hypothesis that the model is adequate and report that there is no compelling evidence that the model needs to be improved. Example: Probability of rocket damage on liftoff Returning to the data on the Space Shuttle O-ring damage and launch time temperature. The recovered booster rockets were checked for damage to the rings. A damaged rocket booster is coded as 1. Temperature &lt;- c(66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 70, 78, 67, 53, 67, 75, 70, 81, 76, 79, 75, 76, 58) Damage &lt;- c(0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1) ORings.df &lt;- data.frame(Temperature, Damage) A logistic regression model is used to predict the probability of damage given the temperature. Fit a logistic regression model to the data. logrm &lt;- glm(Damage ~ Temperature, data = ORings.df, family = binomial) summary(logrm) ## ## Call: ## glm(formula = Damage ~ Temperature, family = binomial, data = ORings.df) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0611 -0.7613 -0.3783 0.4524 2.2175 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 15.0429 7.3786 2.039 0.0415 * ## Temperature -0.2322 0.1082 -2.145 0.0320 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 28.267 on 22 degrees of freedom ## Residual deviance: 20.315 on 21 degrees of freedom ## AIC: 24.315 ## ## Number of Fisher Scoring iterations: 5 The negative sign on the Temperature coefficient indicates that the chance of O-ring damages decreases with increasing temperature. You interpret the magnitude of the coefficient by first applying the exponentiation function (exp()). exp(-.2322) ## [1] 0.7927875 The value of .79 tells you that for every 1 degree increase in temperature the chance of O-ring damage is only 79% of it’s previous value (given the model). For example, to determine how much more likely there is to be damage when the temperature is 65 F compared to when the temperature is 69 F you use round(exp(-.2322 * (65 - 69)), 1) ## [1] 2.5 Thus, on average, there is a 2.5 times increase in the chance of O-ring damage when launch temperatures are 65 F compared to when they are 69 F. predict(logrm, newdata = data.frame(Temperature = c(45, 95)), type = &quot;response&quot;) ## 1 2 ## 0.990004536 0.000899691 With generalized linear models, model adequacy is determined by the residual deviance. No plots are needed. The residual deviance is 20.315 on 21 degrees of freedom. A \\(p\\) value from a \\(\\chi^2\\) distribution with this quantile value and this many degrees of freedom is evidence in favor of the null hypothesis that the model is adequate. pchisq(q = 20.315, df = 21, lower.tail = FALSE) ## [1] 0.5013948 The large \\(p\\) value gives you no reason to question the adequacy of the model. "],["tuesday-november-8-2022.html", "Tuesday, November 8, 2022 Quantifying autocorrelation in spatial data Spatial data as polygon areas Nearest neighbors Spatial weights Moran’s I Spatial lag variable", " Tuesday, November 8, 2022 Today Spatial autocorrelation Quantifying autocorrelation in spatial data Spatial data as polygon areas Nearest neighbors Spatial weights Moran’s I Spatial lag variable When your data set contains observations made across space (spatial data), the assumption of independent values is usually not valid. This is because nearby locations tend to have similar values. Similarity in observations that are nearby arises from: Association: whatever is causing an observation to have a certain value in one area causes the observation to have a similar value in areas nearby. Crime rates in nearby neighborhoods might tend to cluster due to similar factors such as economic status and the amount of policing. Non-infectious diseases (e.g., lung cancer) might have similar rates in neighborhoods close to an oil refinery. Causality: something within an area directly influences outcomes within nearby areas. The broken window theory of crime suggests that poverty, lack of maintenance, and petty crime tends to breed more crime due to a perceived breakdown in civil order. Interaction: the movement of people, goods or information creates relationships between nearby areas. Infectious diseases might spread from a source region thus increasing the disease rates in surrounding areas as the direct result of contact or movement of people between regions. We quantify how similar nearby values tend to be through a metric called spatial autocorrelation. Statistical methods can quantify, and condition on, spatial autocorrelation but they are silent about it’s physical cause. Understanding the reason for spatial autocorrelation is important for causal inference because the causal mechanism might be statistically confounded by it. Divorce rates are high in states in the South but so to are the number of Waffle Houses. Understanding the potential causes of spatial autocorrelation requires domain specific knowledge. Quantifying autocorrelation in spatial data When values are aggregated into spatial regions, autocorrelation is quantified by how similar a value in region \\(i\\) is to the value in region \\(j\\) and weighting this similarity by how ‘close’ region \\(i\\) is to region \\(j\\). Closer regions are given more weight. High similarities with high weight (similar values close together) yield high values of spatial autocorrelation. Low similarities with high weight (dissimilar values close together) yield low values of spatial autocorrelation. Let \\(\\hbox{sim}_{ij}\\) be a measure of similarity between values \\(Y_i\\) and \\(Y_j\\), and let \\(w_{ij}\\) be a set of weights inversely proportional to the distances between regions \\(i\\) and \\(j\\), for all \\(i\\), \\(j\\). Then a spatial autocorrelation index (SAI) will have the form \\[ \\hbox{SAI} = \\frac{\\sum_{i,j=1}^N w_{ij}\\hbox{sim}_{ij}}{\\sum_{i,j=1}^N w_{ij}} \\] The set of weights is called a spatial weights matrix and \\(N\\) is the number of observations. The spatial weights matrix defines the neighbors and the strength of their relationship. The spatial unit of observation can be points or polygons. Temperatures measured by thermometers across the cities is an example of point observations. The number of people living within a census block is an example of polygon observations. Spatial data as polygon areas Consider crime data at the census tract level in the city of Columbus, Ohio. The data are available in a zipped folder containing ESRI shapefiles on my website. Here you use the download.file() function and then the unzip() function to get the data into our project folder. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/columbus.zip&quot; download.file(url, destfile = here::here(&quot;data&quot;, &quot;columbus.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;columbus.zip&quot;), exdir = here::here(&quot;data&quot;)) Then you use the read_sf() function from the {sf} package to get the data into your current session. You assign the spatial data frame the name CC.sf. CC.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;columbus&quot;), layer = &quot;columbus&quot;) The spatial data frame contains crime rates (residential burglaries and vehicle thefts per 1000 households) in the column labeled CRIME. There is one crime rate for each census tract polygon. The data frame also contains housing values and median household income in units of 1000 of dollars. The object CC.sf behaves as a regular data frame but it is also has a simple feature class (sf) that indicates a column containing the geographic information. class(CC.sf) ## [1] &quot;sf&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; Simple features is an open standard from the Open Geospatial Consortium (OGC) for representing geographic information as a spatial data frame. Simple features condense geographic forms into a single geometry class. The standard is implemented in spatial databases (e.g., PostGIS), commercial GIS (e.g., ESRI) and forms the vector data basis for libraries such as GDAL. Functions in the {sf} package can be used to manipulate vector geometry types including points, lines, polygons and their respective ‘multi’ versions (which group together features of the same type into a single feature). The geometry is a list column in the simple feature data frame with the label geometry. To print the column names type names(CC.sf) ## [1] &quot;AREA&quot; &quot;PERIMETER&quot; &quot;COLUMBUS_&quot; &quot;COLUMBUS_I&quot; &quot;POLYID&quot; ## [6] &quot;NEIG&quot; &quot;HOVAL&quot; &quot;INC&quot; &quot;CRIME&quot; &quot;OPEN&quot; ## [11] &quot;PLUMB&quot; &quot;DISCBD&quot; &quot;X&quot; &quot;Y&quot; &quot;NSA&quot; ## [16] &quot;NSB&quot; &quot;EW&quot; &quot;CP&quot; &quot;THOUS&quot; &quot;NEIGNO&quot; ## [21] &quot;geometry&quot; The first twenty columns in the spatial data frame contain observations (attributes) and the last column contains the geometry information associated with each observation (row). The geometry column contains the spatial coordinates in well-known text (WKT) format. The {sf} package has a plot() method for viewing the geographic data on a map. For example to plot the census tract borders in the city of Columbus we apply the method to the geometry column of the spatial data frame. plot(CC.sf$geometry) The sf spatial objects behave as regular data frames. For example, the summary() method, gives an overview of the variables and the spatial information. Here you summarize the data in the CRIME column. summary(CC.sf[&quot;CRIME&quot;]) ## CRIME geometry ## Min. : 0.1783 POLYGON:49 ## 1st Qu.:20.0485 epsg:NA: 0 ## Median :34.0008 ## Mean :35.1288 ## 3rd Qu.:48.5855 ## Max. :68.8920 You see that the median crime rate is about 34 cases per 1000 households. Although you selected the variable CRIME the output also includes a summary on the geometry column. Here the information tells you that there are 49 polygons (49 census tracts) and no coordinate information. You can create choropleth maps from sf objects with the base plot() method (plot.sf()). For example, to create a choropleth map of the crime variable you subset on the variable name. plot(CC.sf[&quot;CRIME&quot;]) You can also make maps with functions from the {tmap} package. library(tmap) tm_shape(CC.sf) + tm_fill(&quot;CRIME&quot;, title = &quot;&quot;) + tm_borders(col = &quot;gray70&quot;) + tm_layout(title = &quot;Burglary &amp; Vehicle Thefts\\n/1000 Households&quot;, legend.outside = TRUE) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is assumed. And with functions from the {ggplot2} package. library(ggplot2) ggplot(data = CC.sf) + geom_sf(mapping = aes(fill = CRIME), col = &quot;gray70&quot;) + ggtitle(&quot;Burglary &amp; Vehicle Thefts/1000 Households&quot;) The map shows that high crime areas tend to cluster. That is census tracts with high crime rates tend to be adjacent to other tracts with high rates. Spatial autocorrelation quantifies the amount of clustering. To compute the spatial autocorrelation you first need to define a list of neighbors for each tract (each polygon). Nearest neighbors You create a list of neighbors for each tract using the poly2nb() function from the {spdep} package. nbs &lt;- spdep::poly2nb(CC.sf) nbs ## Neighbour list object: ## Number of regions: 49 ## Number of nonzero links: 236 ## Percentage nonzero weights: 9.829238 ## Average number of links: 4.816327 See also the new package {sfdep}. https://sfdep.josiahparry.com/ The function builds a list of neighbors for each polygon based on contiguity. Neighbors must share one or more boundary points. The default contiguity is defined as having at least one point in common (called queen contiguity). This is changed with the argument queen = FALSE. There are 49 polygons (census tracts). Each polygon is bordered by at least one other polygon (each polygon is bordered by at least one polygon). The average number of neighboring polygons is 4.8. The total number of neighboring polygons over all tracts is 236. This represents 9.8% of all possible neighbors (if every polygon is a neighbor of itself and every other polygon 49 * 49). You make a graph the neighbors with the plot() method. The arguments include the neighbor list object (nbs) and the location of the polygon centers, which are extracted from the simple feature data frame using the st_centroid(). plot(nbs, sf::st_centroid(CC.sf$geometry)) The graph is a network of nodes and links showing the contiguity (topology). Each node is a tract and each link indicates spatial contiguity. The graph makes it clear that tracts in the center of the city have more neighbors than tracts away from the center. The number of links per node [link number distribution] is obtained with the summary() method applied to the neighborhood list object. summary(nbs) ## Neighbour list object: ## Number of regions: 49 ## Number of nonzero links: 236 ## Percentage nonzero weights: 9.829238 ## Average number of links: 4.816327 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 ## 5 9 12 5 9 3 4 1 1 ## 5 least connected regions: ## 1 6 42 46 47 with 2 links ## 1 most connected region: ## 20 with 10 links You print the neighboring tracts for the first two tracts using the double bracket notation. nbs[[1]] ## [1] 2 3 nbs[[2]] ## [1] 1 3 4 The first tract has two neighbors that include tracts 2 and 3. The neighbor numbers are stored as an integer vector. Tract 2 has three neighbors that include tracts 1, 3, and 4. Tract 5 has 8 neighbors and so on. The function card() tallies the number of neighbors by tract. spdep::card(nbs) ## [1] 2 3 4 4 8 2 4 6 8 4 5 6 4 6 6 8 3 4 3 10 3 6 3 7 8 ## [26] 6 4 9 7 5 3 4 4 4 7 5 6 6 3 5 3 2 6 5 4 2 2 4 3 Tract 5 has 8 neighbors. Spatial weights Next you add weights to the neighbor list object. The weights specifying how ‘close’ each neighbor is. You do this with the nb2listw() function from the {spdep} package. The function turns the neighbor list object into a spatial weights object. wts &lt;- spdep::nb2listw(nbs) class(wts) ## [1] &quot;listw&quot; &quot;nb&quot; This new wts object is a list with two elements. The first element (listw) is the weights matrix and the second element (nb) is the neighbor list object. summary(wts) ## Characteristics of weights list object: ## Neighbour list object: ## Number of regions: 49 ## Number of nonzero links: 236 ## Percentage nonzero weights: 9.829238 ## Average number of links: 4.816327 ## Link number distribution: ## ## 2 3 4 5 6 7 8 9 10 ## 5 9 12 5 9 3 4 1 1 ## 5 least connected regions: ## 1 6 42 46 47 with 2 links ## 1 most connected region: ## 20 with 10 links ## ## Weights style: W ## Weights constants summary: ## n nn S0 S1 S2 ## W 49 2401 49 22.75119 203.7091 By default all neighboring tracts are assigned a weight equal to the inverse of the number of neighbors (style = \"W\"). For a tract with 5 neighbors each neighbor gets a weight of 1/5. The sum over all weights (S0) is the number of tracts. To see the weights for the first two tracts type wts$weights[1:2] ## [[1]] ## [1] 0.5 0.5 ## ## [[2]] ## [1] 0.3333333 0.3333333 0.3333333 The $weights list is the spatial weights matrix. To see the neighbors of the first two tracts type wts$neighbours[1:2] ## [[1]] ## [1] 2 3 ## ## [[2]] ## [1] 1 3 4 Tract 1 has two neighbors (tract 2 &amp; 3) so each are given a weight of 1/2. Tract 2 has three neighbors (tract 1, 3, &amp; 4) so each are given a weight of 1/3. With the weights matrix specified and saved as an object you are ready to quantify the amount of spatial autocorrelation. Caution: Using contiguity as a indicator of closeness can result in areas having no neighbors; islands for example. By default the nb2listw() function assumes each area has at least one neighbor. If this is not the case you need to specify how areas without neighbors are handled using the argument zero.policy = TRUE. This allows the weights list to be formed with zero-length vectors. For example, consider the districts in Scotland. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/scotlip.zip&quot; download.file(url, destfile = here::here(&quot;data&quot;, &quot;scotlip.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;scotlip.zip&quot;), exdir = here::here(&quot;data&quot;)) SL.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;scotlip&quot;), layer = &quot;scotlip&quot;) plot(SL.sf$geometry) Here there are three island districts. Create a list of neighbors. ( nbs2 &lt;- spdep::poly2nb(SL.sf) ) ## Neighbour list object: ## Number of regions: 56 ## Number of nonzero links: 234 ## Percentage nonzero weights: 7.461735 ## Average number of links: 4.178571 ## 3 regions with no links: ## 6 8 11 Three regions with no links. Use the nb2listw() function with the argument zero.policy = TRUE. Otherwise you will get an error saying the empty neighbor sets are found. wts2 &lt;- spdep::nb2listw(nbs2, zero.policy = TRUE) head(wts2$weights) ## [[1]] ## [1] 0.3333333 0.3333333 0.3333333 ## ## [[2]] ## [1] 0.5 0.5 ## ## [[3]] ## [1] 1 ## ## [[4]] ## [1] 0.3333333 0.3333333 0.3333333 ## ## [[5]] ## [1] 0.3333333 0.3333333 0.3333333 ## ## [[6]] ## NULL Here we see that district 6 in Scotland has no neighbors. Moran’s I Moran’s I follows the basic form of spatial autocorrelation indexes where the similarity between regions \\(i\\) and \\(j\\) is proportional to the product of the deviations from the mean such that \\[ \\hbox{sim}_{ij} \\propto (Y_i - \\bar Y) (Y_j - \\bar Y) \\] where \\(i\\) indexes the region (areal unit) and \\(j\\) indexes the neighbors of \\(i\\). The value of \\(\\hbox{sim}_{ij}\\) is large when the \\(Y\\) values in the product are on the same side of their respective means and small when they are on opposite sides of their respective means. The formula for I is \\[ \\hbox{I} = \\frac{N} {W} \\frac {\\sum_{i,j} w_{ij}(Y_i-\\bar Y) (Y_j-\\bar Y)} {\\sum_{i} (Y_i-\\bar Y)^2} \\] where \\(N\\) is the number regions, \\(w_{ij}\\) is the matrix of spatial weights, and \\(W\\) is the sum over all weights. Consider the following spatial grid of cells containing random values. set.seed(6750) Y &lt;- spatstat.geom::ppp(runif(200, 0, 1), runif(200, 0, 1)) plot(spatstat.geom::quadratcount(Y), main = &quot;&quot;) The formula for I results in one value for the entire grid. First consider a single cell on the grid (\\(N\\) = 1). Here the middle cell (row 3, column 3). Let \\(i\\) = 3 in the above formula and let \\(j\\) index the cells touching the center cell in reading order starting with cell (2, 2), then cell (2, 3), etc. Assume each neighbor is given a weight of 1/8 so \\(W\\) = 1. Then the value of I for the single center cell is I_{3, 3} = (6 - mean(y)) * ((8 - mean(y)) + (3 - mean(y)) + (9 - mean(y)) + (12 - mean(y)) + (10 - mean(y)) + (10 - mean(y)) + (9 - mean(y))) / (6 - mean(y))^2) y &lt;- c(3, 10, 7, 12, 5, 11, 8, 3, 9, 12, 6, 12, 6, 10, 3, 8, 10, 10, 9, 7, 5, 10, 8, 5, 11) yb &lt;- mean(y) Inum_i &lt;- (6 - yb) * ((8 - yb) + (3 - yb) + (9 - yb) + (12 - yb) + (10 - yb) + (10 - yb) + (10 - yb) + (9 - yb)) Iden_i &lt;- (6 - yb)^2 Inum_i/Iden_i ## [1] -3.5 The I value of -3.5 indicates that the center cell which has a value less than the average over all 25 cells is surrounded by cells with values above the average. Now repeat this calculation for every cell and then add all the results. This is done with the moran() function from the {spdep} package. The first argument is the vector containing the values for which we are interested in determining the magnitude of the spatial autocorrelation and the second argument is the listw object. We also need to specify the number of regions and the global sum of the weights S0. The latter is obtained from the Szero() function applied to the listw object. Returning to the Columbus crime data here we let m be the number of census tracts and s be the sum of the weights. m &lt;- length(CC.sf$CRIME) s &lt;- spdep::Szero(wts) spdep::moran(CC.sf$CRIME, listw = wts, n = m, S0 = s) ## $I ## [1] 0.5001886 ## ## $K ## [1] 2.225946 The function returns the Moran’s I value and the kurtosis (K) of the distribution of crime values. Values for Moran’s I range between -1 to +1. Positive values of Moran’s I indicate clustering. The value of .5 for the crime rates indicates fairly high spatial autocorrelation. This is expected based on what you saw in the maps showing a clustering of crime. Kurtosis is a statistic that indicates how ‘peaked’ the distribution of values is relative to a normal (bell-shaped) distribution. A normal distribution has a kurtosis of 3. If the kurtosis is too large or too small relative to a normal distribution (greater than about 4 or less than about 2) then any inference you make about clustering might be incorrect. Another measure of the amount of spatial autocorrelation is the Geary’s C statistic. Geary’s C is computed as \\[ \\hbox{C} = \\frac{(N-1) \\sum_{i,j} w_{ij} (Y_i-Y_j)^2}{2 W \\sum_{i}(Y_i-\\bar Y)^2} \\] where \\(W\\) is the sum over all weights (\\(w_{ij}\\)) and \\(N\\) is the number of areas. The syntax of the geary() function is similar to the syntax of the moran() function except you also specify n1 to be one minus the number of polygons. spdep::geary(CC.sf$CRIME, listw = wts, n = m, S0 = s, n1 = m - 1) ## $C ## [1] 0.5405282 ## ## $K ## [1] 2.225946 Values for Geary’s C range from 0 to 2 with 1 indicating no spatial autocorrelation. Values less than 1 indicate positive autocorrelation. Both I and C are global measures of autocorrelation, but C is more sensitive to local variations in autocorrelation. Rule of thumb: If the interpretation of Geary’s C is much different than the interpretation of Moran’s I then consider computing local measures of spatial autocorrelation. Spatial lag variable Moran’s I is the slope coefficient from a regression of the weighted average of the neighborhood values onto the observed values. The weighted average of neighborhood values is called the spatial lag. Let crime be the set of crime values in each region. You create a spatial lag variable using the lag.listw() function. The first argument is the listw object and the second is the vector of crime values. crime &lt;- CC.sf$CRIME Wcrime &lt;- spdep::lag.listw(wts, crime) For each value in the vector crime there is a corresponding value in the vector Wcrime representing the average crime over the neighboring regions. Recall tract 1 had tract 2 and 3 as its only neighbors. So the following should return a TRUE. Wcrime[1] == (crime[2] + crime[3])/2 ## [1] TRUE A scatter plot of the neighborhood average crime rates versus the individual polygon crime rates in each shows there is a relationship. library(ggplot2) data.frame(crime, Wcrime) |&gt; ggplot(aes(x = crime, y = Wcrime)) + geom_point() + geom_smooth(method = lm) + scale_x_continuous(limits = c(0, 70)) + scale_y_continuous(limits = c(0, 70)) + xlab(&quot;Crime&quot;) + ylab(&quot;Average Crime in the Neighborhood&quot;) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; The vertical axis contains the neighborhood average crime rate. The range of neighborhood averages is smaller than the range of individual polygon crime rates. Tracts with low values of crime tend to be surrounded by tracts with low values of crime on average and tracts with high values of crime tend be surrounded by tracts with high values of crime. The slope is upward (positive). The magnitude of the slope is the Moran’s I value. To check this type lm(Wcrime ~ crime) ## ## Call: ## lm(formula = Wcrime ~ crime) ## ## Coefficients: ## (Intercept) crime ## 17.4797 0.5002 The coefficient on the crime variable in the simple linear regression is .5. The scatter plot is a Moran’s scatter plot. "],["thursday-november-10-2022.html", "Thursday, November 10, 2022 Statistical significance of spatial autocorrelation A Monte Carlo approach to inference about spatial autocorrelation Spatial autocorrelation in model residuals", " Thursday, November 10, 2022 Today Statistical significance of spatial autocorrelation A Monte Carlo approach to inference Spatial autocorrelation in model residuals As a review let us consider another data set where we start by compute spatial autocorrelation. The data are sudden infant deaths (SIDS) rates in the 100 counties of North Carolina for the period July 1, 1979 to June 30, 1984. Download and unzip the data in your working directory. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/sids2.zip&quot; download.file(url, destfile = here::here(&quot;data&quot;, &quot;sids2.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;sids2.zip&quot;), exdir = here::here(&quot;data&quot;)) Then import the data with the read_sf() function from the {sf} package. Set the coordinate reference system to WGS 1984 with latitude/longitude coordinates using the ESPG code of 4326. SIDS.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;sids2&quot;)) sf::st_crs(SIDS.sf) &lt;- 4326 head(SIDS.sf) ## Simple feature collection with 6 features and 18 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -81.74107 ymin: 36.07282 xmax: -75.77316 ymax: 36.58965 ## Geodetic CRS: WGS 84 ## # A tibble: 6 × 19 ## AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS…¹ BIR74 SID74 NWBIR74 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.114 1.44 1825 1825 Ashe 37009 37009 5 1091 1 10 ## 2 0.061 1.23 1827 1827 Allegh… 37005 37005 3 487 0 10 ## 3 0.143 1.63 1828 1828 Surry 37171 37171 86 3188 5 208 ## 4 0.07 2.97 1831 1831 Currit… 37053 37053 27 508 1 123 ## 5 0.153 2.21 1832 1832 Northa… 37131 37131 66 1421 9 1066 ## 6 0.097 1.67 1833 1833 Hertfo… 37091 37091 46 1452 7 954 ## # … with 8 more variables: BIR79 &lt;dbl&gt;, SID79 &lt;dbl&gt;, NWBIR79 &lt;dbl&gt;, ## # SIDR74 &lt;dbl&gt;, SIDR79 &lt;dbl&gt;, NWR74 &lt;dbl&gt;, NWR79 &lt;dbl&gt;, ## # geometry &lt;MULTIPOLYGON [°]&gt;, and abbreviated variable name ¹​CRESS_ID ## # ℹ Use `colnames()` to see all variable names The column SIDR79 contains the death rate per 1000 live births (1979-84) from sudden infant death syndrome. Create a choropleth map of the SIDS rates. library(tmap) tm_shape(SIDS.sf) + tm_fill(&quot;SIDR79&quot;, title = &quot;&quot;) + tm_borders(col = &quot;gray70&quot;) + tm_layout(title = &quot;SIDS Rates 1979-84 [per 1000]&quot;, legend.outside = TRUE) Counties with the highest rates are noted in the southern and western parts of the state. There appears to be some clustering. Counties with high rates tend to be surrounded by counties also with high rates and counties with low rates tend to be surrounded by counties also with low rates. To quantify the amount of clustering you first create a neighborhood list (nb) and a listw object (wts). To visualize the connectivity you graph the neighborhood network. nbs &lt;- spdep::poly2nb(SIDS.sf) wts &lt;- spdep::nb2listw(nbs) plot(nbs, sf::st_centroid(SIDS.sf)$geometry) ## Warning in st_centroid.sf(SIDS.sf): st_centroid assumes attributes are constant ## over geometries of x plot(SIDS.sf$geometry, add = TRUE) Next you compute Moran’s I with the moran() function from the {spdep} package. m &lt;- length(SIDS.sf$SIDR79) s &lt;- spdep::Szero(wts) spdep::moran(SIDS.sf$SIDR79, listw = wts, n = m, S0 = s) ## $I ## [1] 0.1427504 ## ## $K ## [1] 4.44434 I is .14 and K is 4.4. A normal distribution has a kurtosis of 3. Values less than about 2 or greater than about 4 indicate that inferences about autocorrelation based on the assumption of normality are suspect. Weights are specified using the style = argument in the nb2listw() function. The default “W” is row standardized (sum of the weights over all links equals the number of polygons). “B” is binary (each neighbor gets a weight of one). “S” is a variance stabilizing scheme. Each style gives a slightly different value for I. x &lt;- SIDS.sf$SIDR79 spdep::moran.test(x, spdep::nb2listw(nbs, style = &quot;W&quot;))$estimate[1] ## Moran I statistic ## 0.1427504 spdep::moran.test(x, spdep::nb2listw(nbs, style = &quot;B&quot;))$estimate[1] # binary ## Moran I statistic ## 0.1105207 spdep::moran.test(x, spdep::nb2listw(nbs, style = &quot;S&quot;))$estimate[1] # variance-stabilizing ## Moran I statistic ## 0.1260686 Thus when reporting Moran’s I you need to indicate what weighting scheme you used. Moran’s I is the slope of a regression of the spatial lag variable onto the variable itself. The spatial lag variable consists of the average of the neighboring values. To compute the spatial lag variable you use the lag.listw() function from the {spdep} package. Let sids be the SIDS rate in each county then the first argument in the lab.listw() function is the listw object and the second is the vector of sids values. sids &lt;- SIDS.sf$SIDR79 Wsids &lt;- spdep::lag.listw(wts, var = sids) For each value in the vector sids there is a corresponding value in the object Wsids representing the neighborhood average SIDS rate. Wsids[1] ## [1] 2.65921 j &lt;- wts$neighbours[[1]] j ## [1] 2 18 19 sum(SIDS.sf$SIDR79[j])/length(j) ## [1] 2.65921 The weight for county one is Wsids[1] = 2.659. The neighbor indexes for this county are in the vector wts$neighbours[[1]] of length 3. Add the SIDS rates from those counties and divide by the length. A scatter plot of the neighborhood average SIDS rate versus the actual SIDS rate in each region. library(ggplot2) data.frame(sids, Wsids) |&gt; ggplot(mapping = aes(x = sids, y = Wsids)) + geom_point() + geom_smooth(method = lm) + xlab(&quot;SIDS&quot;) + ylab(&quot;Spatial Lag of SIDS&quot;) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; The slope of the regression line is upward indicating positive autocorrelation. The value of the slope is I. To check this type lm(Wsids ~ sids) ## ## Call: ## lm(formula = Wsids ~ sids) ## ## Coefficients: ## (Intercept) sids ## 1.7622 0.1428 Statistical significance of spatial autocorrelation Even if the county SIDS rates are randomly distributed across the state, the value of Moran’s I will not necessarily be close to zero (-1/(n-1)). If we want to use I to establish statistically significant clustering, you need a way to guard against this randomness. Said another way: Is the value of Moran’s I significant with respect to the null hypothesis of no spatial autocorrelation? One way to answer this question is to draw an uncertainty band on the regression line and see if a horizontal line can be placed within the band. If not, then I is statistically different than what you would expect if the null hypothesis of no spatial autocorrelation were true. More formally the question is answered by comparing the standard deviate (\\(z\\) value) of the I statistic with a standard normal deviate. This is done using the moran.test() function, where the \\(z\\) value is the difference between I and the expected value of I divided by the square root of the variance of I. The function takes a variable name or numeric vector and a spatial weights list object in that order. The argument randomisation = FALSE means the variance of I is computed under the assumption of normally distributed SIDS rates. ( mt &lt;- spdep::moran.test(sids, listw = wts, randomisation = FALSE) ) ## ## Moran I test under normality ## ## data: sids ## weights: wts ## ## Moran I statistic standard deviate = 2.3438, p-value = 0.009544 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.142750392 -0.010101010 0.004252954 I is .143 with a variance of .0043. The \\(z\\) value for the I statistic is 2.3438 giving a \\(p\\) value slightly less than .01 under the null hypothesis of no spatial autocorrelation. Thus we reject the null hypothesis and conclude there is weak (I is small) but significant spatial autocorrelation in SIDS rates across North Carolina at the county level. Look at the output in the object mt with the str() function. str(mt) ## List of 6 ## $ statistic : Named num 2.34 ## ..- attr(*, &quot;names&quot;)= chr &quot;Moran I statistic standard deviate&quot; ## $ p.value : num 0.00954 ## $ estimate : Named num [1:3] 0.14275 -0.0101 0.00425 ## ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Moran I statistic&quot; &quot;Expectation&quot; &quot;Variance&quot; ## $ alternative: chr &quot;greater&quot; ## $ method : chr &quot;Moran I test under normality&quot; ## $ data.name : chr &quot;sids \\nweights: wts \\n&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; The element called estimate is a vector of length three containing Moran’s I statistic, the expected value of Moran’s I under the assumption of uncorrelated normally distributed SIDS rates, and the variance of Moran’s I. The \\(z\\) value is the difference between the statistic and it’s expected value divided by the square root of the variance. ( mt$estimate[1] - mt$estimate[2] ) / sqrt(mt$estimate[3]) ## Moran I statistic ## 2.343819 The \\(p\\) value is the area under a standard normal distribution curve to the right (lower.tail = FALSE) of 2.3438 (mt$statistic). pnorm(mt$statistic, lower.tail = FALSE) ## Moran I statistic standard deviate ## 0.009543713 curve(dnorm(x), from = -4, to = 4, lwd = 2) abline(v = mt$statistic, col = &#39;red&#39;) So slightly less than 1% (\\(p\\) value of .0095) of the area lies to the right of the red line. Recall the \\(p\\) value summarizes the evidence in support of the null hypothesis. The smaller the \\(p\\) value, the less evidence there is to support the null hypothesis. In this case it is the probability that the county SIDS rates could have been arranged at random across the state. The small \\(p\\) value tells us that the spatial arrangement of our data is unusual with respect to the null hypothesis. The interpretation of the \\(p\\) value is stated as evidence AGAINST the null hypothesis. This is because your interest lies in the null hypothesis being unsupported. \\(p\\) value Statement of evidence against the null less than .01 convincing .01 - .05 moderate .05 - .15 suggestive, but inconclusive greater than .15 no Under the assumption of normal distributed and uncorrelated data, the expected value for Moran’s I is -1/(n-1) where n is the number of counties. A check on the distribution of SIDS rates indicates that normality is suspect. Recall a good way to check the normality assumption is to use the sm.density() function from the {sm} package. sm::sm.density(sids, model = &quot;Normal&quot;, xlab = &quot;SIDS Rates 1979-84 [per 1000]&quot;) The SIDS rates are more “peaked” (higher kurtosis) than a normal distribution. In this case it is better to use the default randomisation = TRUE argument. Further, the assumptions underlying the test are sensitive to the form of the graph of neighbor relationships and other factors so results should be checked against a test that involves permutations. A Monte Carlo approach to inference about spatial autocorrelation A permutation test to examine the statistical significance of spatial autocorrelation is performed with the moran.mc() function. MC stands for Monte Carlo which refers to the city of Monte Carlo in Monaco famous for its gambling casinos. The MC procedure refers to random sampling. The name of the data vector and the weights list object (listw) are required arguments as is the number of permutations (nsim). Each permutation is a rearrangement of the SIDS rates across the counties. This rearrangement removes the spatial autocorrelation but keeps the non-spatial distribution of the SIDS rates. The neighbor topology and weights remain the same. For each permutation (shuffle or simulation), I is computed and saved. The \\(p\\) value is obtained as the ratio of the number of permuted I values greater or exceeding the observed I over the number of permutation plus one. In the case where there are 5 permuted I values greater or equal to the observed value based on 99 simulations, the \\(p\\) value is 5/(99 + 1) = .05. For example, if you want draw inference on your value for I using 99 simulations you type set.seed(4102) ( mP &lt;- spdep::moran.mc(sids, listw = wts, nsim = 99) ) ## ## Monte-Carlo simulation of Moran I ## ## data: sids ## weights: wts ## number of simulations + 1: 100 ## ## statistic = 0.14275, observed rank = 96, p-value = 0.04 ## alternative hypothesis: greater You see that only two of the 99 simulations give a Moran’s I value exceeding .1428. Thus the \\(p\\) value, as evidence in support of the null hypothesis, (the true value for is close to zero) is .02. Note: here you initiate the random number generator with a seed value (4102) with the set.seed() function so that the set of random permutations of the values across the domain will be the same each time you run the simulations. This is not needed but it is important for reproducibility. The default random number generator seed value is determined from the current time (internal clock) and so no two sets of permutations will be identical. The values of I computed for each permutation are saved in the vector mP$res. head(mP$res) ## [1] 0.072218451 0.011785265 -0.032330997 0.046546755 0.008730068 ## [6] 0.080639514 tail(mP$res) ## [1] 0.03479774 -0.01222684 0.03392046 -0.03833658 -0.03410486 0.14275039 The last value in the vector is I computed using the data in the correct counties. So the \\(p\\) value as evidence in support of the null hypothesis that I is close to zero is given as sum(mP$res &gt; mP$res[100])/99 ## [1] 0.04040404 A density graph shows the distribution of permuted I values. To see this create 999 simulations and then plot a density curve adding a vertical line at the value of Moran’s I computed from the data at the correct counties. mP &lt;- spdep::moran.mc(sids, listw = wts, nsim = 999) df &lt;- data.frame(mp = mP$res[-1000]) ggplot(data = df, mapping = aes(mp)) + geom_density() + geom_rug() + geom_vline(xintercept = mP$res[1000], color = &quot;red&quot;, size = 2) + theme_minimal() Only a handful simulations give a Moran’s I value exceeding .143 (tic marks to the right of the red vertical line). The density curve is centered just to the left of zero which is consistent with the true mean being slightly less than zero (-.01). Note that the right tail of the distribution is fatter than the left tail. This is due to the skewness of the rates used in the calculation of I. What do you do with the knowledge that the SIDS rates have significant spatial autocorrelation? By itself not much, but it can be a warning that something is going on a larger scale (larger than the scale of the counties). More typically the knowledge is useful after other known factors are considered in the analysis. In the language of statistics, knowledge of significant spatial autocorrelation in the residuals from a model can help you better specify the relationships defined by the model. Spatial autocorrelation in model residuals A spatial regression model might be needed when the residuals resulting from an aspatial (non-spatial) regression model exhibit significant spatial autocorrelation. A common way to proceed is to regress the response variable onto the explanatory variables and check for spatial autocorrelation in the residuals. If the residuals are spatially uncorrelated then a spatial regression model is not needed. Let’s return to the Columbus crime data and fit a linear regression model with CRIME as the response variable and INC and HOVAL as the explanatory variables. CC.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;columbus&quot;), layer = &quot;columbus&quot;) Start with a choropleth map displaying the crime rates across the city at the tract level. tm_shape(CC.sf) + tm_fill(&quot;CRIME&quot;, title = &quot;&quot;) + tm_borders(col = &quot;gray70&quot;) + tm_layout(title = &quot;Burglary &amp; Vehicle Thefts\\n/1000 Households&quot;, legend.outside = TRUE) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is assumed. Next compute Moran’s I on the CRIME variable using row-standardized weights (default option in the nb2listw() function). nbs &lt;- spdep::poly2nb(CC.sf) wts &lt;- spdep::nb2listw(nbs) spdep::moran(CC.sf$CRIME, listw = wts, n = length(nbs), S0 = spdep::Szero(wts)) ## $I ## [1] 0.5001886 ## ## $K ## [1] 2.225946 Is the spatial autocorrelation you detect due to a clustering of income and housing values across the city? Said another way, after accounting for income and housing values, do crime rates still cluster? This is answered by fitting a regression model to the crime rates with income and housing values as explanatory variables. model.lm &lt;- lm(CRIME ~ INC + HOVAL, data = CC.sf) summary(model.lm) ## ## Call: ## lm(formula = CRIME ~ INC + HOVAL, data = CC.sf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -34.418 -6.388 -1.580 9.052 28.649 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 68.6190 4.7355 14.490 &lt; 2e-16 *** ## INC -1.5973 0.3341 -4.780 1.83e-05 *** ## HOVAL -0.2739 0.1032 -2.654 0.0109 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.43 on 46 degrees of freedom ## Multiple R-squared: 0.5524, Adjusted R-squared: 0.5329 ## F-statistic: 28.39 on 2 and 46 DF, p-value: 9.341e-09 Income and housing values vary indirectly with values of crime. Higher income tracts and tracts with higher housing values have less crime. The model statistically explains 55% of the variation in crime. You use the residuals() method to extract the vector of residuals from the model. res &lt;- residuals(model.lm) There are 49 residuals, one residual for every tract. A residual is defined as the difference between the observed value and the model predicted value. A tract with a residual that is greater than zero indicates that there is more crime than the model predicts for that location. You next check on the distribution of the residuals relative to a normal distribution. sm::sm.density(res, model = &quot;Normal&quot;) Since the black lines falls completely within the blue envelope (defined by a normal distribution) you find no evidence to reject the assumption of normally distributed model residuals. Next you plot a choropleth map of the model residuals. Are there clusters of high and low residuals? First add the vector of residuals as a column in the simple feature data frame. Call this new column residuals then use functions from the {tmap} package. CC.sf$residuals &lt;- res tm_shape(CC.sf) + tm_fill(&quot;residuals&quot;) + tm_borders(col = &quot;gray70&quot;) + tm_layout(title = &quot;Regression Residuals&quot;) ## Warning: Currect projection of shape CC.sf unknown. Long-lat (WGS84) is assumed. ## Variable(s) &quot;residuals&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. There appears to be distinct regions where the model over predicts crime (yellow to green tracts) conditional on household income and housing values and where it under predicts crime (yellow to red tracts). The amount of clustering appears to be is less than the crime rates themselves. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced. To determine how much less clustering you compute I on the residuals with the lm.morantest() function. You need to include the model object and the spatial weights object. spdep::lm.morantest(model = model.lm, listw = wts) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = CRIME ~ INC + HOVAL, data = CC.sf) ## weights: wts ## ## Moran I statistic standard deviate = 2.8393, p-value = 0.00226 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.222109407 -0.033418335 0.008099305 Moran’s I on the model residuals is .222. This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors. The output gives a \\(p\\) value on I of .002, thus you reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the model. The \\(z\\)-value takes into account the fact that these are residuals so the variance is adjusted accordingly. The next step is to choose the type of spatial regression model. Before that, let’s look at one more example. Example: Percentage of folks identifying as white in Mississippi counties Download the county-level police data. Import the data as a simple feature data frame and assign the geometry a geographic CRS. The file police.zip contains shapefiles in a folder on my website. url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/police.zip&quot; download.file(url, destfile = here::here(&quot;data&quot;, &quot;police.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;police.zip&quot;), exdir = here::here(&quot;data&quot;)) PE.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;police&quot;), layer = &quot;police&quot;) sf::st_crs(PE.sf) &lt;- 4326 names(PE.sf) ## [1] &quot;AREA&quot; &quot;PERIMETER&quot; &quot;CNTY_&quot; &quot;CNTY_ID&quot; &quot;NAME&quot; ## [6] &quot;STATE_NAME&quot; &quot;STATE_FIPS&quot; &quot;CNTY_FIPS&quot; &quot;FIPS&quot; &quot;FIPSNO&quot; ## [11] &quot;POLICE&quot; &quot;POP&quot; &quot;TAX&quot; &quot;TRANSFER&quot; &quot;INC&quot; ## [16] &quot;CRIME&quot; &quot;UNEMP&quot; &quot;OWN&quot; &quot;COLLEGE&quot; &quot;WHITE&quot; ## [21] &quot;COMMUTE&quot; &quot;geometry&quot; Variables include police expenditures (POLICE), crime (CRIME), income (INC), unemployment (UNEMP) and other socio-economic characteristics across Mississippi at the county level. The police expenditures are per capita 1982 (dollars per person). The personal income per county resident, 1982 (dollars per person). The crime is the number of serious crimes per 100,000 residents, 1981. Unemployment is percent unemployed in 1980. The geometries are polygons defining the county borders. plot(PE.sf$geometry) Again, you need to assign the neighborhoods and the associated weights between the each region and each neighbor of that region. nbs &lt;- spdep::poly2nb(PE.sf) wts &lt;- spdep::nb2listw(nbs) Another way to do this is to specify the number of neighbors and define who the neighbors are based on distance. You do this with the knearneigh() function. First extract the coordinates of the polygon centroids. coords &lt;- PE.sf |&gt; sf::st_geometry() |&gt; sf::st_centroid() |&gt; sf::st_coordinates() head(coords) ## X Y ## 1 -88.56938 34.88746 ## 2 -88.23073 34.74665 ## 3 -88.89928 34.77600 ## 4 -89.17942 34.82323 ## 5 -89.98973 34.88099 ## 6 -89.49699 34.76984 You can specify that each county for example has six neighbors where the neighbors are based on proximity. Since the CRS is geographic we include the longlat = TRUE argument so the distances are based on great circles. knn &lt;- spdep::knearneigh(coords, k = 6, longlat = TRUE) head(knn$nn) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 9 3 2 4 10 14 ## [2,] 9 1 16 3 14 10 ## [3,] 4 10 1 9 6 14 ## [4,] 3 6 10 1 11 9 ## [5,] 8 7 6 12 11 4 ## [6,] 4 8 11 5 3 10 The output is a list of five elements with the first being a matrix of dimension number of counties by the number of neighbors. You see here that the neighborhoods are non-symmetric. County 3 is a neighbor of county 2, but county 2 is not a neighbor of county 3. This is important since some spatial regression models require symmetric neighborhood definitions. You turn this list into a neighborhood object (class nb) with the knn2nb() function. nbs2 &lt;- spdep::knn2nb(knn) summary(nbs2) ## Neighbour list object: ## Number of regions: 82 ## Number of nonzero links: 492 ## Percentage nonzero weights: 7.317073 ## Average number of links: 6 ## Non-symmetric neighbours list ## Link number distribution: ## ## 6 ## 82 ## 82 least connected regions: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 with 6 links ## 82 most connected regions: ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 with 6 links The argument sym = TRUE forces the output neighbors list to be symmetric. Create another neighborhood object. nbs3 &lt;- spdep::knn2nb(knn, sym = TRUE) summary(nbs3) ## Neighbour list object: ## Number of regions: 82 ## Number of nonzero links: 568 ## Percentage nonzero weights: 8.447353 ## Average number of links: 6.926829 ## Link number distribution: ## ## 6 7 8 9 10 ## 37 25 13 3 4 ## 37 least connected regions: ## 1 2 5 6 7 8 13 16 18 21 22 28 31 33 34 40 42 44 45 46 48 49 50 54 57 59 62 63 65 73 74 76 77 78 80 81 82 with 6 links ## 4 most connected regions: ## 10 19 66 70 with 10 links Compare the neighborhood topologies. par(mfrow = c(1, 2)) plot(sf::st_geometry(PE.sf), border = &quot;grey&quot;) plot(nbs, coords, add = TRUE) plot(sf::st_geometry(PE.sf), border = &quot;grey&quot;) plot(nbs2, coords, add = TRUE) Create weight matrices for these alternative neighborhood definitions using the same nb2listw() function. wts2 &lt;- spdep::nb2listw(nbs2) wts3 &lt;- spdep::nb2listw(nbs3) You then compute Moran I for the variable percentage white (WHITE) with the moran() function. And you do it separately for the three different weight matrices. spdep::moran(PE.sf$WHITE, listw = wts, n = length(nbs), S0 = spdep::Szero(wts)) ## $I ## [1] 0.5634778 ## ## $K ## [1] 2.300738 spdep::moran(PE.sf$WHITE, listw = wts2, n = length(nbs2), S0 = spdep::Szero(wts2)) ## $I ## [1] 0.5506132 ## ## $K ## [1] 2.300738 spdep::moran(PE.sf$WHITE, listw = wts3, n = length(nbs3), S0 = spdep::Szero(wts3)) ## $I ## [1] 0.5592557 ## ## $K ## [1] 2.300738 Values of Moran I are constrained between -1 and +1. In this case the neighborhood definition has little impact on inferences made about spatial autocorrelation. The kurtosis is between 2 and 4 consistent with a set of values from a normal distribution. In a similar way you compute Geary’s c statistic. spdep::geary(PE.sf$WHITE, listw = wts, n = length(nbs), S0 = spdep::Szero(wts), n1 = length(nbs) - 1) ## $C ## [1] 0.4123818 ## ## $K ## [1] 2.300738 Values of Geary’s c are between 0 and 2 with values less than one indicating positive autocorrelation. If the interpretation on the amount of spatial autocorrelation based on Geary’s c is different than the interpretation on the amount of autocorrelation based on Moran I then it is a good idea to examine local variations in autocorrelation. Finally, recall that to create a Moran scatter plot. You first assign to white the percentage of whites in each county (variable WHITE) and to Wwhite the spatial lagged value of percentage of whites. You then use ggplot() as follows. white &lt;- PE.sf$WHITE Wwhite &lt;- spdep::lag.listw(wts, white) library(ggplot2) data.frame(white, Wwhite) |&gt; ggplot(mapping = aes(x = white, y = Wwhite)) + geom_point() + geom_smooth(method = lm) + scale_x_continuous(limits = c(0, 100)) + scale_y_continuous(limits = c(0, 100)) + xlab(&quot;% White&quot;) + ylab(&quot;Avg of % White in the Neighborhood\\n(Spatial Lag)&quot;) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; The line is the best-fit linear regression of Wwhite onto white and it’s slope is equal to the value of Moran I. This is true regardless of the neighborhood definition. lm(Wwhite ~ white) ## ## Call: ## lm(formula = Wwhite ~ white) ## ## Coefficients: ## (Intercept) white ## 27.2333 0.5635 You test for significant spatial autocorrelation with the moran.test() function. spdep::moran.test(white, listw = wts) ## ## Moran I test under randomisation ## ## data: white ## weights: wts ## ## Moran I statistic standard deviate = 8.5089, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.563477779 -0.012345679 0.004579648 You see that the value of .56 is much larger than the expected value under the null hypothesis of no autocorrelation (-.012 = -1/(n-1)). "],["tuesday-november-15-2022.html", "Tuesday, November 15, 2022 Bi-variate spatial autocorrelation Local autocorrelation 0.2 Local regression Spatial weights", " Tuesday, November 15, 2022 Today Bivariate spatial autocorrelation Local spatial autocorrelation Local regression Spatial weights Recall the demographic data from the state of Mississippi at the county level. The data are available as ESRI shape files in the folder police. First we import the data to create a simple feature data frame called PE.sf. PE.sf &lt;- sf::read_sf(dsn = here::here(&quot;data&quot;, &quot;police&quot;), layer = &quot;police&quot;) names(PE.sf) ## [1] &quot;AREA&quot; &quot;PERIMETER&quot; &quot;CNTY_&quot; &quot;CNTY_ID&quot; &quot;NAME&quot; ## [6] &quot;STATE_NAME&quot; &quot;STATE_FIPS&quot; &quot;CNTY_FIPS&quot; &quot;FIPS&quot; &quot;FIPSNO&quot; ## [11] &quot;POLICE&quot; &quot;POP&quot; &quot;TAX&quot; &quot;TRANSFER&quot; &quot;INC&quot; ## [16] &quot;CRIME&quot; &quot;UNEMP&quot; &quot;OWN&quot; &quot;COLLEGE&quot; &quot;WHITE&quot; ## [21] &quot;COMMUTE&quot; &quot;geometry&quot; The simple feature data frame includes police expenditures (POLICE), crime (CRIME), income (INC), unemployment (UNEMP), percentage of people who identify as white (WHITE) among other socioeconomic characteristics in Mississippi by county. Police expenditures are dollars per person in 1982. The crime variable is the number of serious crimes per 100,000 residents in 1981. The income variable is personal income per county resident in 1982 (dollars per person). The unemployment variable is percent unemployed in 1980. Because PE.sf is a simple feature data frame it has a column labeled geometry. Here the column contains polygons defining the longitude and latitude coordinates in WKT format that define the county borders. head(PE.sf$geometry) ## Geometry set for 6 features ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -90.30535 ymin: 34.46518 xmax: -88.09043 ymax: 35.00496 ## CRS: NA ## First 5 geometries: ## POLYGON ((-88.35416 34.76262, -88.71059 34.7596... ## POLYGON ((-88.32171 34.4693, -88.3175 34.74454,... ## POLYGON ((-88.72614 34.60488, -89.07788 34.6071... ## POLYGON ((-89.23874 34.59352, -89.29233 34.5927... ## POLYGON ((-90.20186 34.72979, -90.19975 34.8669... A simple way to display the county borders is to apply the plot() method to the geometry column. plot(PE.sf$geometry) Recall that to compute spatial autocorrelation you need a spatial weights matrix. The spatial weights matrix includes the set of neighbors and their weights. nbs &lt;- spdep::poly2nb(PE.sf) wts &lt;- spdep::nb2listw(nbs) You then apply the moran.test() function. Here we apply the function to the variable labeled WHITE. spdep::moran.test(PE.sf$WHITE, listw = wts) ## ## Moran I test under randomisation ## ## data: PE.sf$WHITE ## weights: wts ## ## Moran I statistic standard deviate = 8.5089, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.563477779 -0.012345679 0.004579648 The Moran I statistic is .56 and it is statistically significant indicating counties that are predominantly white tend to be surrounded by counties that are also predominately white. Bi-variate spatial autocorrelation Spatial autocorrelation can be extended to two variables. The extension is motivated by the fact that association measures, like Pearson’s correlation coefficient, do not recognize the spatial arrangement of the data values. Consider for example the correlation between police expenditure (POLICE) and the amount of crime (CRIME) in the police expenditure data set. police &lt;- PE.sf$POLICE crime &lt;- PE.sf$CRIME cor.test(police, crime) ## ## Pearson&#39;s product-moment correlation ## ## data: police and crime ## t = 6.2916, df = 80, p-value = 1.569e-08 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.4094516 0.7043990 ## sample estimates: ## cor ## 0.5753377 You find a significant correlation (\\(p\\) value &lt; .01) between these two variables under the null hypothesis that they are uncorrelated. The significance level assumes that the values of police expenditure and crime in one county is independent of the values of expenditure and crime in a neighboring county. You note, however, that these variables have significant spatial autocorrelation. spdep::moran.test(police, listw = wts) ## ## Moran I test under randomisation ## ## data: police ## weights: wts ## ## Moran I statistic standard deviate = 1.7899, p-value = 0.03674 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.087185424 -0.012345679 0.003092257 spdep::moran.test(crime, listw = wts) ## ## Moran I test under randomisation ## ## data: crime ## weights: wts ## ## Moran I statistic standard deviate = 2.2072, p-value = 0.01365 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.103588680 -0.012345679 0.002758842 The spatial autocorrelation indicates that the counties cannot be treated as independent. In addition there can be indirect correlation, where the amount of police spending in a particular county is correlated with the level of crime in neighboring counties. The Lee statistic brings together Pearson’s r statistic and Moran’s I. The formula is \\[ L(x,y) = \\frac{n}{\\sum_{i=1}^{n}(\\sum_{j=1}^{n}w_{ij})^2} \\frac{\\sum_{i=1}^{n}(\\sum_{j=1}^{n}w_{ij}(x_i-\\bar{x})) ((\\sum_{j=1}^{n}w_{ij}(y_j-\\bar{y}))}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} \\] For the crime example, you let \\(x\\) be crime rates and \\(y\\) be police spending. The formula is applied with the lee() function where the first two arguments are the variables and you need to include the weights matrix with the listw = argument and the number of regions with the n = argument. The output from this function is a list of two with the first being the value of Lee’s statistic (L). spdep::lee(x = crime, y = police, listw = wts, n = length(nbs))$L ## [1] 0.1306991 Values of the Lee statistic can range between -1 and +1 with the value of .13 here indicating weak bi-variate spatial autocorrelation between crime and police expenditures. You state that police spending has some weak correlation with crime in neighboring counties, but not much. You note that neither values in crime nor values in police are adequately described by a normal distribution. sm::sm.density(crime, model = &quot;normal&quot;) sm::sm.density(police, model = &quot;normal&quot;) So you use a non-parametric (Monte Carlo) test for significance on the bivariate spatial autocorrelation with the lee.mc() function. The crime and police expenditure values are randomly permuted across the counties and values of L are computed for each permutation. Here we set the number of permutations to 999 with the nsim = argument. spdep::lee.mc(x = crime, y = police, listw = wts, nsim = 999) ## ## Monte-Carlo simulation of Lee&#39;s L ## ## data: crime , police ## weights: wts ## number of simulations + 1: 1000 ## ## statistic = 0.1307, observed rank = 758, p-value = 0.242 ## alternative hypothesis: greater From the test results you conclude that there is no significant bi-variate spatial autocorrelation between crime and police expenditure. Police expenditure in a county is not significantly influenced by crime in neighboring counties. Local autocorrelation The Moran I statistic was first used in the 1950s. A local version of the statistic was introduced in the 1990s. Local I is a deconstruction of the ‘global’ Moran’s I, where geographic proximity is used in two ways. (1) to define and weight neighbors and (2) to determine the spatial scale over which Moran’s I is computed. For example, consider the tenth county in the police expenditure data frame. PE.sf[10, ] ## Simple feature collection with 1 feature and 21 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: -89.24072 ymin: 34.37103 xmax: -88.72366 ymax: 34.60714 ## CRS: NA ## # A tibble: 1 × 22 ## AREA PERIME…¹ CNTY_ CNTY_ID NAME STATE…² STATE…³ CNTY_…⁴ FIPS FIPSNO POLICE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 0.109 1.49 2199 2199 Union Missis… 28 145 28145 28145 364 ## # … with 11 more variables: POP &lt;int&gt;, TAX &lt;int&gt;, TRANSFER &lt;int&gt;, INC &lt;int&gt;, ## # CRIME &lt;int&gt;, UNEMP &lt;int&gt;, OWN &lt;int&gt;, COLLEGE &lt;int&gt;, WHITE &lt;int&gt;, ## # COMMUTE &lt;int&gt;, geometry &lt;POLYGON&gt;, and abbreviated variable names ## # ¹​PERIMETER, ²​STATE_NAME, ³​STATE_FIPS, ⁴​CNTY_FIPS ## # ℹ Use `colnames()` to see all variable names It is Union County with a population of 21,400 residents of which 86% identify as white. Union County has 7 neighbors as defined by contiguity. The neighbors are PE.sf$NAME[unlist(wts$neighbours[10])] ## [1] &quot;Tippah&quot; &quot;Benton&quot; &quot;Marshall&quot; &quot;Prentiss&quot; &quot;Lafayette&quot; &quot;Lee&quot; ## [7] &quot;Pontotoc&quot; On a map you color Union County red and its neighboring counties pink. plot(PE.sf$geometry) plot(PE.sf$geometry[10], add = TRUE, col = &quot;red&quot;) plot(PE.sf$geometry[unlist(wts$neighbours[10])], add = TRUE, col = &quot;pink&quot;) The neighborhood average percent white is computed as mean(PE.sf$WHITE[unlist(wts$neighbours[10])]) ## [1] 74 The neighbor average percentage of whites is lower than the percentage of whites in Union County. This two dimensional point (86, 74) contributes to the global Moran’s I, but it also contributes to the local Moran’s I if we consider only these 8 counties. For example, consider the first neighbor of Union County. PE.sf$NAME[unlist(wts$neighbours[10])[1]] ## [1] &quot;Tippah&quot; PE.sf$WHITE[unlist(wts$neighbours[10])[1]] ## [1] 84 Tippah County, MS where according to the data set 84% of its residents identifying as white. Tippah County has 4 neighbors. They are PE.sf$NAME[unlist(wts$neighbours[unlist(wts$neighbours[10])[1]])] ## [1] &quot;Alcorn&quot; &quot;Benton&quot; &quot;Prentiss&quot; &quot;Union&quot; On a map plot(PE.sf$geometry) plot(PE.sf$geometry[unlist(wts$neighbours[10])[1]], add = TRUE, col = &quot;pink&quot;) plot(PE.sf$geometry[unlist(wts$neighbours[unlist(wts$neighbours[10])[1]])], add = TRUE, col = &quot;gray70&quot;) The neighborhood average percent of whites in Tippah County is computed as mean(PE.sf$WHITE[unlist(wts$neighbours[unlist(wts$neighbours[10])[1]])]) ## [1] 81.5 This gives you a new point (84, 81.5). Continuing this for all of the 7 contiguous neighbors of Union County plus Union County itself gives you 8 points on a local Moran’s scatter plot. W &lt;- PE.sf$WHITE Wlag &lt;- spdep::lag.listw(wts, var = W) library(ggplot2) data.frame(W = c(W[10], W[unlist(wts$neighbours[10])]), Wlag = c(Wlag[10], Wlag[unlist(wts$neighbours[10])]), Name = c(PE.sf$NAME[10], PE.sf$NAME[unlist(wts$neighbours[10])])) |&gt; ggplot(mapping = aes(x = W, y = Wlag, label = Name)) + geom_smooth(method = lm, se = FALSE) + geom_point(col = &quot;pink3&quot;) + geom_text(nudge_y = .5, col = &quot;pink3&quot;) + geom_point(aes(x = W[1], y = Wlag[1]), col = &quot;red&quot;) + geom_text(aes(x = W[1], y = Wlag[1], label = Name[1]), nudge_y = .5, col = &quot;red&quot;) + ylab(&quot;Average percent white in neighboring counties&quot;) + xlab(&quot;Percent white&quot;) + theme_minimal() + ggtitle(label = &quot;Union County&quot;, subtitle = &quot;Moran&#39;s scatter plot of percent white&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; This is the local Moran’s scatter plot for Union County. In the same way, you can draw a local Moran scatter plot for each county. Here for Clarke County. county &lt;- 54 PE.sf$NAME[county] ## [1] &quot;Clarke&quot; data.frame(W = c(W[county], W[unlist(wts$neighbours[county])]), Wlag = c(Wlag[county], Wlag[unlist(wts$neighbours[county])]), Name = c(PE.sf$NAME[county], PE.sf$NAME[unlist(wts$neighbours[county])])) |&gt; ggplot(mapping = aes(x = W, y = Wlag, label = Name)) + geom_smooth(method = lm, se = FALSE) + geom_point(col = &quot;pink3&quot;) + geom_text(nudge_y = .5, col = &quot;pink3&quot;) + geom_point(aes(x = W[1], y = Wlag[1]), col = &quot;red&quot;) + geom_text(aes(x = W[1], y = Wlag[1], label = Name[1]), nudge_y = .5, col = &quot;red&quot;) + ylab(&quot;Average percent white in neighboring counties&quot;) + xlab(&quot;Percent white&quot;) + theme_minimal() + ggtitle(label = &quot;Clarke County&quot;, subtitle = &quot;Moran&#39;s scatter plot of percent white&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Clark County’s four neighbors are Jasper, Wayne, Newton, and Lauderdale and the relationship is opposite to that of Union County. Here you see that counties with a relatively high percentage of whites neighbors to counties with a relatively low percentage of whites. The slope of the regression line is the local Moran’s I value. It can be computed for Union and Clarke counties as coef(lm(c(Wlag[10], Wlag[unlist(wts$neighbours[10])]) ~ c(W[10], W[unlist(wts$neighbours[10])])))[2] ## c(W[10], W[unlist(wts$neighbours[10])]) ## 0.2508013 coef(lm(c(Wlag[54], Wlag[unlist(wts$neighbours[54])]) ~ c(W[54], W[unlist(wts$neighbours[54])])))[2] ## c(W[54], W[unlist(wts$neighbours[54])]) ## -0.3683312 In practice you compute local Moran I with the localmoran() function from the {spdep} package. The two required arguments are the variable for which you want to compute local correlation and the weights matrix as a list object. Ii_stats &lt;- spdep::localmoran(PE.sf$WHITE, listw = wts) str(Ii_stats) ## &#39;localmoran&#39; num [1:82, 1:5] 2.28138 2.97475 1.31244 0.00231 -1.03216 ... ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:82] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## ..$ : chr [1:5] &quot;Ii&quot; &quot;E.Ii&quot; &quot;Var.Ii&quot; &quot;Z.Ii&quot; ... ## - attr(*, &quot;call&quot;)= language spdep::localmoran(x = PE.sf$WHITE, listw = wts) ## - attr(*, &quot;quadr&quot;)=&#39;data.frame&#39;: 82 obs. of 3 variables: ## ..$ mean : Factor w/ 4 levels &quot;Low-Low&quot;,&quot;High-Low&quot;,..: 4 4 4 4 2 3 1 1 4 4 ... ## ..$ median: Factor w/ 4 levels &quot;Low-Low&quot;,&quot;High-Low&quot;,..: 4 4 4 3 2 3 1 1 4 4 ... ## ..$ pysal : Factor w/ 4 levels &quot;Low-Low&quot;,&quot;High-Low&quot;,..: 4 4 4 4 2 3 1 1 4 4 ... The local I is given in the first column of a matrix where the rows are the counties. The other columns are the expected value of I, the variance of I, the \\(z\\)-value and the \\(p\\) value. For example, the local I statistics from the first six counties are given by typing head(Ii_stats) ## Ii E.Ii Var.Ii Z.Ii Pr(z != E(Ii)) ## 1 2.281375143 -2.748824e-02 7.124247e-01 2.735450 0.006229509 ## 2 2.974750377 -4.354053e-02 1.109833e+00 2.865051 0.004169423 ## 3 1.312440365 -1.827251e-02 3.539514e-01 2.236725 0.025304339 ## 4 0.002313108 -2.007906e-07 5.351069e-06 1.000031 0.317295645 ## 5 -1.032155817 -1.511126e-02 3.966295e-01 -1.614907 0.106330864 ## 6 -0.493034653 -8.356103e-03 1.291002e-01 -1.348933 0.177358557 Because these local values must average to the global value (when using row standardized weights), they can take on values outside the range between -1 and 1. A summary() method on the first column of the Li object gives statistics from the distribution of I’s. summary(Ii_stats[, 1]) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.03216 0.01733 0.26984 0.56348 1.05945 2.97475 The mean value over all counties is the global Moran I (.56). To map the values you first attach the matrix columns of interest to the simple feature data frame. Here you attach Ii, Var, and Pi. PE.sf$Ii &lt;- Ii_stats[, 1] PE.sf$Vi &lt;- Ii_stats[, 3] PE.sf$Pi &lt;- Ii_stats[, 5] Then you map the local spatial autocorrelation. ( g1 &lt;- ggplot(data = PE.sf) + geom_sf(mapping = aes(fill = Ii)) + scale_fill_gradient2(low = &quot;green&quot;, high = &quot;blue&quot;) + theme_minimal() ) The map shows three areas (clusters) of relatively high positive local spatial autocorrelation across the state; the northeast, the west, and the south. A few counties in the north have negative local spatial autocorrelation. To make greater sense of these so-called ‘clusters’ you plot the map of local autocorrelation alongside a map of the percentage of whites. ( g2 &lt;- ggplot(data = PE.sf) + geom_sf(mapping = aes(fill = WHITE)) + scale_fill_gradient(low = &quot;brown&quot;, high = &quot;white&quot;) + theme_minimal() ) Plot them together using syntax from the {patchwork} package. library(patchwork) g1 + g2 Areas of the state where percent white values are high, like over the northeast, are areas where there is high spatial autocorrelation. Also areas of the Mississippi delta region in the west, where percent white values are low, are areas where there is high spatial autocorrelation. The \\(p\\) values on the local autocorrelation values highlight the three clusters. ggplot(data = PE.sf) + geom_sf(mapping = aes(fill = Pi)) + scale_fill_gradient() + theme_minimal() Finally, variances are larger for counties near the boundaries as the sample sizes are smaller. ggplot(data = PE.sf) + geom_sf(mapping = aes(fill = Vi)) + scale_fill_gradient() + theme_minimal() Local values of Lee’s bi-variate spatial autocorrelation are available from the lee() function. lee_stat &lt;- spdep::lee(crime, police, listw = wts, n = length(nbs)) PE.sf$localL &lt;- lee_stat$localL library(tmap) tm_shape(PE.sf) + tm_fill(&quot;localL&quot;, title = &quot;&quot;) + tm_borders(col = &quot;gray70&quot;) + tm_layout(title = &quot;Local Bi-variate Spatial Autocorrelation&quot;, legend.outside = TRUE) ## Warning: Currect projection of shape PE.sf unknown. Long-lat (WGS84) is assumed. ## Variable(s) &quot;localL&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. Areas in dark green indicate where the correlation between crime and policing is most influenced by neighboring crime and policing. 0.2 Local regression When you fit a regression model to your data you get a single set of coefficients that quantifies the relationship between the response variable and the explanatory variables. When you fit a geographic regression model to your data yet get an entire set of coefficients, one for every location. Geographic regression shows where the response variable is most strongly related to the explanatory variables. Geographic regression is similar to local regression. For example, compare a regression of ozone on temperature (‘global’ regression) with a ‘local’ regression of ozone on temperature. ggplot(data = airquality, mapping = aes(x = Temp, y = Ozone)) + geom_point() + geom_smooth(se = FALSE) + geom_smooth(method = lm, se = FALSE, color = &quot;red&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; ## Warning: Removed 37 rows containing non-finite values (stat_smooth). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 37 rows containing non-finite values (stat_smooth). ## Warning: Removed 37 rows containing missing values (geom_point). Global regression is represented by the red line. Local regression is represented by the blue curve. Global regression over estimates the relationship at low temperature (the red line has a steeper slope compared to the blue line for temperatures less than 75F) and it under estimates the relationship at high temperature (the red line has a shallower slope compared to the blue line for temperatures above 75F). Note: It is the relationship we are describing not the observations. The red line has a single value for the slope and a single value for the intercept across all values of temperature. You can get these two values with the lm() function. The local regression has a slope and intercept at each observation. This is done by weighting the observations based on ‘distance’ to Temp = 70. Observations with temperature near 70F are given more weight than those much warmer and those much colder. The change in ozone per degree change in temperature is small for temperatures less than 75F, but this change increases as temperatures rise through the 80’s. Spatial weights This ‘inverse-distance’ weighting (more weight to nearby observations) is applied to each observation and the weights are stored in matrix (W). Elements of the weights matrix W are labeled \\(w_{ij}\\), where \\(i\\) indexes the location and \\(j\\) indexes all other values. The precise values in W are determined by a function called a kernel. The kernel describes how much weight to give each observation based on how far the observation is from the location of interest. The kernel is symmetric about the location of interest and often takes the form of a gaussian curve. p &lt;- ggplot(data.frame(x = c(-3, 3)), aes(x)) + stat_function(fun = function(x) 1/sqrt(2 * pi) * exp(-.5 * x^2), lwd = 2) + xlab(&quot;spatial distance&quot;) + ylab(&quot;weight&quot;) + ggtitle(label = &quot;kernel centered at location 0&quot;) p Local feature distance is the horizontal axis and the amount of weight is the vertical axis. The kernel is centered on the location of interest. Here at 0 distance. p &lt;- p + geom_point(aes(x = 0, y = 0), col = &quot;red&quot;, size = 3) + geom_text(aes(x = 0, y = -.02, label = &quot;x1&quot;)) p Data values at that location get the most weight indicated by the height of the kernel above that location. p &lt;- p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * 0^2), yend = 0, x = 0, xend = 0), linetype = 2) + geom_text(aes(x = .3, y = .2, label = &quot;w11&quot;)) p Data values at neighboring locations get less weight. For example, the second data value at location x2 we have a weight w12. The first index indicates the kernel centered on observation 1, and the second index is location of observation 2. p &lt;- p + geom_point(aes(x = 1, y = 0), col = &quot;black&quot;, size = 3) + geom_text(aes(x = 1, y = -.02, label = &quot;x2&quot;)) p &lt;- p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * 1^2), yend = 0, x = 1, xend = 1), linetype = 2) + geom_text(aes(x = .7, y = .1, label = &quot;w12&quot;)) p Similarly, at the third data value at location x3 we have a weight w13. p &lt;- p + geom_point(aes(x = -2, y = 0), col = &quot;black&quot;, size = 3) + geom_text(aes(x = -2, y = -.02, label = &quot;x3&quot;)) p &lt;- p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * (-2)^2), yend = 0, x = -2, xend = -2), linetype = 2) + geom_text(aes(x = -1.7, y = .025, label = &quot;w13&quot;)) p Other kernel shapes are possible, but they all tend to be symmetric reflecting the type of dependency found in most spatial processes. At each location \\(i\\) for which the local regression model is estimated, the kernel is given by \\[ w_{ij} = \\left[-(d_{ij}/h)^2\\right], \\] where \\(d_{ij}\\) is the distance between locations \\(i\\) and \\(j\\), and \\(h\\) is the bandwidth. As \\(h\\) increases, the gradient of the kernel becomes less steep and more data points are included in the local estimation. p + geom_segment(aes(y = .23, yend = .23, x = -1, xend = 1), col = &quot;blue&quot;) + geom_text(x = 0, y = .25, label = &quot;bandwidth&quot;) The choice of bandwidth is a trade-off between variance and bias. A bandwidth that is narrow results in large variation in model coefficients across the domain. A bandwidth that is wide leads to a large bias locally as the model coefficients are influenced by processes not representative of the local conditions. "],["thursday-november-17-2022.html", "Thursday, November 17, 2022 Geographic regression", " Thursday, November 17, 2022 Today Geographic regression Geographic regression Geographic regression (geographically weighted regression or GWR) fits separate regression models for each location (e.g., polygon) using only values in neighboring locations defined by distance (bandwidth) or contiguity. This is useful for showing where the response variable is most strongly related to the explanatory variables. GWR is not a single model but a procedure for fitting many models, one at each location. The bandwidth is determined by a cross-validation procedure. GWR is used in epidemiology, particularly for research on infectious diseases and for evaluating health policies or health programs. Example: Southern homicides {-} url &lt;- &quot;http://myweb.fsu.edu/jelsner/temp/data/south.zip&quot; download.file(url, destfile = here::here(&quot;data&quot;, &quot;south.zip&quot;)) unzip(zipfile = here::here(&quot;data&quot;, &quot;south.zip&quot;), exdir = here::here(&quot;data&quot;)) The folder south contains shapefiles with homicide rates and explanatory variables for counties in the southern United States. Import the data using the st_read() from the {sf} package. The data have latitude/longitude coordinates but there is no projection information so you set the CRS to long-lat with the st_crs() function. SH.sf &lt;- sf::st_read(dsn = here::here(&quot;data&quot;, &quot;south&quot;), layer = &quot;south&quot;) ## Reading layer `south&#39; from data source ## `/Users/jameselsner/Desktop/ClassNotes/QG-2022/data/south&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 1412 features and 69 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -106.6495 ymin: 24.95597 xmax: -75.046 ymax: 40.63714 ## CRS: NA sf::st_crs(SH.sf) &lt;- 4326 names(SH.sf) ## [1] &quot;NAME&quot; &quot;STATE_NAME&quot; &quot;STATE_FIPS&quot; &quot;CNTY_FIPS&quot; &quot;FIPS&quot; ## [6] &quot;STFIPS&quot; &quot;COFIPS&quot; &quot;FIPSNO&quot; &quot;SOUTH&quot; &quot;HR60&quot; ## [11] &quot;HR70&quot; &quot;HR80&quot; &quot;HR90&quot; &quot;HC60&quot; &quot;HC70&quot; ## [16] &quot;HC80&quot; &quot;HC90&quot; &quot;PO60&quot; &quot;PO70&quot; &quot;PO80&quot; ## [21] &quot;PO90&quot; &quot;RD60&quot; &quot;RD70&quot; &quot;RD80&quot; &quot;RD90&quot; ## [26] &quot;PS60&quot; &quot;PS70&quot; &quot;PS80&quot; &quot;PS90&quot; &quot;UE60&quot; ## [31] &quot;UE70&quot; &quot;UE80&quot; &quot;UE90&quot; &quot;DV60&quot; &quot;DV70&quot; ## [36] &quot;DV80&quot; &quot;DV90&quot; &quot;MA60&quot; &quot;MA70&quot; &quot;MA80&quot; ## [41] &quot;MA90&quot; &quot;POL60&quot; &quot;POL70&quot; &quot;POL80&quot; &quot;POL90&quot; ## [46] &quot;DNL60&quot; &quot;DNL70&quot; &quot;DNL80&quot; &quot;DNL90&quot; &quot;MFIL59&quot; ## [51] &quot;MFIL69&quot; &quot;MFIL79&quot; &quot;MFIL89&quot; &quot;FP59&quot; &quot;FP69&quot; ## [56] &quot;FP79&quot; &quot;FP89&quot; &quot;BLK60&quot; &quot;BLK70&quot; &quot;BLK80&quot; ## [61] &quot;BLK90&quot; &quot;GI59&quot; &quot;GI69&quot; &quot;GI79&quot; &quot;GI89&quot; ## [66] &quot;FH60&quot; &quot;FH70&quot; &quot;FH80&quot; &quot;FH90&quot; &quot;geometry&quot; Each row is a separate county in the southeast U.S. There are 1412 counties. You want a model to predict homicide rates (HR). The values are given as the number of homicides per 100,000 people. You consider five explanatory variables for your model including RD: resource deprivation index, PS: population structure index, MA: marriage age, DV: divorce rate, and UE: unemployment rate. The two digit number in the column names is the census year from the 20th century. First use the plot() method on the geometry column to see the extent of the data and the spatial geometries. plot(SH.sf$geometry, col = &quot;gray70&quot;) Next you reduce the number of variables in the data frame keeping only the variables of interest using the select() function from the {dplyr} package. SH.sf &lt;- SH.sf |&gt; dplyr::select(HR90, RD90, PS90, MA90, DV90, UE90) You then create a thematic map of the homicide rates from the 1990 census (HR90) using the functions from the {tmap} package. library(tmap) tm_shape(SH.sf) + tm_fill(&quot;HR90&quot;, title = &quot;1990\\nHomicide Rates\\n[/100,000]&quot;) + tm_layout(legend.outside = TRUE) You start with a regression model regressing homicide rate onto resource deprivation, population structure, marriage age, divorce rate, and unemployment rate in 1990. model.lm &lt;- lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, data = SH.sf) summary(model.lm) ## ## Call: ## lm(formula = HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, data = SH.sf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.583 -3.514 -0.747 2.591 41.833 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.96250 1.78133 5.031 5.50e-07 *** ## RD90 4.58778 0.21457 21.381 &lt; 2e-16 *** ## PS90 1.95589 0.20540 9.522 &lt; 2e-16 *** ## MA90 -0.04948 0.04890 -1.012 0.312 ## DV90 0.46159 0.11517 4.008 6.45e-05 *** ## UE90 -0.52440 0.07003 -7.489 1.22e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.861 on 1406 degrees of freedom ## Multiple R-squared: 0.3092, Adjusted R-squared: 0.3067 ## F-statistic: 125.8 on 5 and 1406 DF, p-value: &lt; 2.2e-16 You see that RD90, PS90, and DV90 all have a direct relationship to HR90 (positive coefficient) while both MA90 and UE90 have an indirect relationship to HR90 (negative coefficient). Based on the \\(p\\) values listed in the table of coefficients you suggest that the model might be simplified by removing marriage age (MA90). You check this suggestion with the drop1() function. drop1(model.lm) ## Single term deletions ## ## Model: ## HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90 ## Df Sum of Sq RSS AIC ## &lt;none&gt; 48296 4999.7 ## RD90 1 15703.4 63999 5395.2 ## PS90 1 3114.7 51410 5085.9 ## MA90 1 35.2 48331 4998.7 ## DV90 1 551.8 48848 5013.7 ## UE90 1 1926.3 50222 5052.9 You see that when marriage age (MA90) is removed from the model the RSS (residual sum of squares) value increases by 35.2 units. This increase is not enough to justify the loss in the degrees of freedom. Thus the AIC value is lower (4998.7) than the AIC when all terms are retained (4999.7) (see the row labeled &lt;none&gt;). The AIC is a way to balance the trade-off between bias and variance. Choose a model that has the lowest AIC. A model may have too much bias (toward the particular data set) if it has too many coefficients and a model may have too much residual variance if there are too few coefficients. You then remove the marriage age variable and refit the model. model.lm2 &lt;- lm(HR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sf) Based on the AIC you find that the new model (model.lm2) should not be simplified further. drop1(model.lm2) ## Single term deletions ## ## Model: ## HR90 ~ RD90 + PS90 + DV90 + UE90 ## Df Sum of Sq RSS AIC ## &lt;none&gt; 48331 4998.7 ## RD90 1 16830.1 65161 5418.6 ## PS90 1 3750.2 52081 5102.2 ## DV90 1 516.7 48848 5011.7 ## UE90 1 1902.9 50234 5051.2 All the AIC values exceed those in the first row so you are content with this model. Next you map the predicted values. You first add the predicted values to the simple features data frame as a column with name predLM2. The predicted values from the model object are extracted with the predict() method. SH.sf$predLM2 &lt;- predict(model.lm2) head(cbind(SH.sf$HR90, SH.sf$predLM2)) ## [,1] [,2] ## [1,] 0.9460827 6.965969 ## [2,] 1.2349338 4.719254 ## [3,] 2.6210087 9.876765 ## [4,] 4.4615769 6.786793 ## [5,] 6.7127356 8.695625 ## [6,] 1.6475415 7.549207 The first column is the actual homicide rates in the first six counties and the second column printed is the predicted homicide rate from the linear regression model. The predictions do not appear to be very good. A scatter plot of the observed versus the predicted shows this clearly. library(ggplot2) ggplot(data = SH.sf, mapping = aes(x = HR90, y = predLM2)) + geom_point() + geom_abline(slope = 1) + geom_smooth(method = lm, se = FALSE) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; Since the homicide rates are non-negative, you create a new column in the SH.sf data frame called logHR90, which is the logarithm of HR90. Since there are some counties with no homicides [log(0) = -Inf] you change values in those counties to the minimum observed value before taking logarithms. Here you first create a logical vector x corresponding to the rows with non-zero homicide rates. You then find the minimum non-zero rate and assign it to e. Next you subset on this value for all rates equal to zero and finally you create a new column as the logarithm of the non-zero rates. x &lt;- SH.sf$HR90 != 0 e &lt;- min(SH.sf$HR90[x]) SH.sf$HR90[!x] &lt;- e SH.sf$logHR90 &lt;- log(SH.sf$HR90) You then fit a model with logHR90 as our response variable. model.lm3 &lt;- lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sf) summary(model.lm3) ## ## Call: ## lm(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sf) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.54480 -0.34707 0.09298 0.46404 3.11601 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.490522 0.118854 12.541 &lt; 2e-16 *** ## RD90 0.506974 0.027083 18.720 &lt; 2e-16 *** ## PS90 0.378972 0.025063 15.121 &lt; 2e-16 *** ## DV90 0.068451 0.014464 4.733 2.44e-06 *** ## UE90 -0.048356 0.009042 -5.348 1.04e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.758 on 1407 degrees of freedom ## Multiple R-squared: 0.2929, Adjusted R-squared: 0.2909 ## F-statistic: 145.7 on 4 and 1407 DF, p-value: &lt; 2.2e-16 You again compute the predicted values and include them in the data frame as predLM3. The predictions are on the logarithm scale so you use the exponential function exp() to transform the output to rates. You then create a scatter plot of the observed versus predicted as before. SH.sf$predLM3 &lt;- exp(predict(model.lm3)) ggplot(data = SH.sf, mapping = aes(x = HR90, y = predLM3)) + geom_point() + geom_abline(slope = 1) + geom_smooth(method = lm, se = FALSE) + theme_minimal() ## `geom_smooth()` using formula &#39;y ~ x&#39; The range of predicted values is better. It is likely that homicide rates are similar in neighboring counties. It also might be the case that the similarity is statistically explained by the variables in the model. So your next step it to test for significant autocorrelation in the model residuals. You create a weights matrix using the functions from the {spdep} package and then use the lm.morantest() function. nbs &lt;- spdep::poly2nb(SH.sf) wts &lt;- spdep::nb2listw(nbs) spdep::lm.morantest(model.lm3, listw = wts) ## ## Global Moran I for regression residuals ## ## data: ## model: lm(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sf) ## weights: wts ## ## Moran I statistic standard deviate = 7.1847, p-value = 3.367e-13 ## alternative hypothesis: greater ## sample estimates: ## Observed Moran I Expectation Variance ## 0.1139749888 -0.0021045133 0.0002610288 Moran I is only .11 but it is statistically significant (\\(p\\) value &lt; .01) because of the large number of counties. Next you map the residuals. First add the residuals as a column named res3 in the simple feature data frame. SH.sf$res3 &lt;- residuals(model.lm3) library(tmap) tm_shape(SH.sf) + tm_fill(&quot;res3&quot;, title = &quot;Model\\nResiduals&quot;) ## Variable(s) &quot;res3&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. There are small clusters of counties with positive residuals and other small clusters of negative residuals. Interestingly the pattern of these clusters appears to be different over western and northern areas compared to over the deep south. This suggests that the relationships between homicide rates and the socioeconomic factors might vary across the domain. GWR is a procedure to fit local regression models. Linear regression is a model for the conditional mean. The mean of the response variable depends on the explanatory variable(s). Geographic regression might show how this dependency varies by location. It is an exploratory technique intended to indicate where local regression coefficients are different from the global values. A model is fit at each location. All observations contribute to the fit but they are weighted inversely by their distance to the location. At the shortest distances observations are given the largest weights based on a Gaussian function. The process results in a set of regression coefficients for each observation. You do this with functions from the {spgwr} package. The geometry information in simple feature data frames is NOT accessible by functions in this package so we need to create another type of spatial data frame. SH.sp &lt;- as(SH.sf, &quot;Spatial&quot;) The spatial information in the SH.sp is separated from the data frame but accessible by the functions gwr.sel() and gwr(). The variables remain the same. You obtain the optimal bandwidth with the gwr.sel() function from the {spgwr} package. You include the model formula, the data, and since the CRS is geographic you use the argument longlat = TRUE to get the distances in kilometers. library(spgwr) ## Loading required package: sp ## Loading required package: spData ## ## Attaching package: &#39;spData&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## coords ## NOTE: This package does not constitute approval of GWR ## as a method of spatial analysis; see example(gwr) bw &lt;- gwr.sel(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sp, longlat = TRUE) ## Bandwidth: 1264.192 CV score: 803.4638 ## Bandwidth: 2043.463 CV score: 809.8007 ## Bandwidth: 782.5756 CV score: 792.1951 ## Bandwidth: 484.9205 CV score: 776.8019 ## Bandwidth: 300.9595 CV score: 759.9205 ## Bandwidth: 187.2653 CV score: 748.7083 ## Bandwidth: 116.9985 CV score: 757.0343 ## Bandwidth: 202.3323 CV score: 749.7891 ## Bandwidth: 178.7092 CV score: 748.2676 ## Bandwidth: 155.1378 CV score: 748.2124 ## Bandwidth: 166.1585 CV score: 747.9707 ## Bandwidth: 166.318 CV score: 747.9712 ## Bandwidth: 165.6051 CV score: 747.97 ## Bandwidth: 161.6069 CV score: 748.0019 ## Bandwidth: 165.5209 CV score: 747.9699 ## Bandwidth: 165.5242 CV score: 747.9699 ## Bandwidth: 165.5245 CV score: 747.9699 ## Bandwidth: 165.5244 CV score: 747.9699 ## Bandwidth: 165.5245 CV score: 747.9699 ## Bandwidth: 165.5245 CV score: 747.9699 bw ## [1] 165.5245 The automatic selection procedure makes an initial guess at the bandwidth distance then fits local regression models in each county using neighbors defined by that distance. A cross-validated (CV) skill score is the root mean square prediction error. The cross-validation procedures successively removes one county from the modeling and that county’s homicide rate is predicted. Each county takes turn getting removed. The selection procedure continues by changing the initial guess at the bandwidth and computing the CV score. If the CV score is higher than with the initial guess the bandwidth is changed in the other direction. If it is lower than the bandwidth is changed in the same direction. The entire procedure continues until no additional improvement is made to the CV score. This results in an optimal bandwidth distance. In this case it is 165.5 km. The bandwidth is assigned to the object bw as a single value. To get a feel for what a bandwidth distance of 165 km means in terms of the average number of neighbors per county you note that one-half the distance squared times \\(\\pi\\) is the area captured by the bandwidth. ( bwA &lt;- pi * (bw * 1000 /2)^2 ) ## [1] 21518612143 Or 21,519 square kilometers. County areas are computed using the st_area() function. The average size of the counties and the ratio of the bandwidth area to the average county area is also computed. areas &lt;- sf::st_area(SH.sf) ctyA &lt;- mean(areas) bwA/ctyA ## 13.2473 [1/m^2] The ratio indicates that, on average, a neighborhood consists of 13 counties. For comparison, on a raster there are 8 first-order neighboring cells (queen contiguity) and 16 second-order neighboring cells (neighbors of neighbors) or a total of 24 neighbors. You then use the gwr() function from the {spgwr} package that includes the formula, data, and the bandwith = argument. model.gwr &lt;- gwr(formula = logHR90 ~ RD90 + PS90 + DV90 + UE90, data = SH.sp, bandwidth = bw) ## Warning in proj4string(data): CRS object has comment, which is lost in output; in tests, see ## https://cran.r-project.org/web/packages/sp/vignettes/CRS_warnings.html The warning message can be safely ignored. The model and observed data are assigned to list object with element names extracted with the names() function. names(model.gwr) ## [1] &quot;SDF&quot; &quot;lhat&quot; &quot;lm&quot; &quot;results&quot; &quot;bandwidth&quot; &quot;adapt&quot; ## [7] &quot;hatmatrix&quot; &quot;gweight&quot; &quot;gTSS&quot; &quot;this.call&quot; &quot;fp.given&quot; &quot;timings&quot; The first element of the list named SDF contains the model output as a S4 spatial class data frame. The geometry of the spatial data frame is inherited from the type of data frame specified in the data = argument. The structure of the S4 spatial class is obtained with the str() function and by setting the max.level argument to 2. str(model.gwr$SDF, max.level = 2) ## Formal class &#39;SpatialPolygonsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 1412 obs. of 9 variables: ## ..@ polygons :List of 1412 ## ..@ plotOrder : int [1:1412] 1247 1172 1091 1088 1253 1368 1286 1201 1089 292 ... ## ..@ bbox : num [1:2, 1:2] -106.6 25 -75 40.6 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## ..$ comment: chr &quot;TRUE&quot; Here there are 5 slots with the first slot being the attribute table labeled @data. The dimension of the attribute table is retrieved with the dim() function. dim(model.gwr$SDF) ## [1] 1412 9 There are 1412 rows and 9 columns. Each row corresponds to a county and information about the regression localized to the county is given in the columns. The attribute names are extracted with the names() function. names(model.gwr$SDF) ## [1] &quot;sum.w&quot; &quot;X.Intercept.&quot; &quot;RD90&quot; &quot;PS90&quot; &quot;DV90&quot; ## [6] &quot;UE90&quot; &quot;gwr.e&quot; &quot;pred&quot; &quot;localR2&quot; They include the sum of the weights sum.w (the larger the sum the more often the county was included in the local regressions–favoring smaller counties and ones farther from the borders of the spatial domain), the five regression coefficients (one for each of the 4 explanatory variables and an intercept term), the residual (gwr.e), the predicted value (pred) and the local goodness-of-fit (localR2). You put the predictions into the SH.sf simple feature data frame with the column name predGWR. SH.sf$predGWR &lt;- exp(model.gwr$SDF$pred) tm_shape(SH.sf) + tm_fill(&quot;predGWR&quot;, title = &quot;Predicted\\nHomicide Rates\\n[/100,000]&quot;) The geographic regressions similarly capture the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the regression model. The pattern is also a smoother. With many more model parameters metrics of predictive skill will favor the geographic regression. For example, the root mean-square-error is lower for GWR. sqrt(sum(residuals(model.lm3)^2)) ## [1] 28.43259 sqrt(sum(model.gwr$SDF$gwr.e^2)) ## [1] 25.68342 Geographic regression is useful for generating hypotheses. From the linear regression model we saw that homicide rates increased with resource deprivation. How does this relationship between homicide rates and resource deprivation vary across the South? coef(model.lm3)[2] ## RD90 ## 0.5069745 range(model.gwr$SDF$RD90) ## [1] 0.08433603 0.98150259 The global regression coefficient is .51 but locally the coefficients range from 0.08 to .98. Importantly you can map where resource deprivation has the most (and least) influence on homicide rates. SH.sf$RDcoef &lt;- model.gwr$SDF$RD90 tm_shape(SH.sf) + tm_fill(&quot;RDcoef&quot;, title = &quot;Resource\\nDeprivation\\nCoefficient&quot;, palette = &#39;Blues&#39;) All values are above zero indicating the importance of resource deprivation as a predictor of homicides, but areas in darker blue indicate where resource deprivation plays a bigger role in explaining homicides. Places like western Texas and southern Florida. What about the influence of unemployment on homicide rates? SH.sf$UEcoef &lt;- model.gwr$SDF$UE90 tm_shape(SH.sf) + tm_fill(&quot;UEcoef&quot;, title = &quot;Unemployment\\nCoefficient&quot;, palette = &#39;PiYG&#39;) ## Variable(s) &quot;UEcoef&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. While the global coefficient is negative indicating homicide rates tend to be lower in areas with more unemployment, the opposite is the case over much of Texas into Oklahoma. Finally, where does the model provide the best fit to the data? This is answered with a map of local R squared values (localR2). SH.sf$localR2 &lt;- model.gwr$SDF$localR2 tm_shape(SH.sf) + tm_fill(&quot;localR2&quot;, title = &quot;Local\\nR Squared&quot;, palette = &#39;Purples&#39;) You see that the models are best at statistical explaining homicides in places like western Texas and southern Florida. Key point: When you fit a regression model to data that vary spatially you are assuming an underlying stationary process. This means you believe the explanatory variables ‘provoke’ the same statistical response across the entire domain. If this is not the case then it shows up in a map of correlated residuals. One approach to investigate this assumption is to use geographic regression. Another approach is to use a single spatial regression model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
