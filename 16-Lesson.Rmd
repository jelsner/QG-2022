# Tuesday, October 25, 2022 {.unnumbered}

Today

Topics in linear regression

-   Simpson's paradox
-   Multiple variable regression
-   Fitting a multiple variable regression model
-   Should you remove a variable from your model?
-   Checking model assumptions
-   Using the model to make predictions
-   Collinearity and what to do about it
-   How to include and interpret interaction terms

## Simpson's paradox {.unnumbered}

The four assumptions that under gird a linear regression model include (1) linearity, (2) equal variance, (3) normality, and (4) independence.

Even if those assumptions are valid, another issue is that of scale. Scale can be spatial regions or other groupings.

Simpson's paradox occurs when a relationship appears in different groups of data but disappears or reverses when the groups are combined.

It is encountered in social-science and medical-science statistics and is problematic when data are aggregated at various scales (spatial or otherwise).

As an example, consider again the relationship between bill length and bill depth in the Palmer penguins data without regards to species.

You start with a scatter plot and a linear regression model.

```{r}
library(palmerpenguins)
library(ggplot2)

( p <- ggplot(data = penguins,
            mapping = aes(y = bill_depth_mm, 
                          x = bill_length_mm)) +
       geom_point() +
       geom_smooth(method = lm) )
```

The plot shows an inverse relationship between bill depth and bill length. 

The inverse relationship is confirmed with a regression model.

```{r}
( model.lr <- lm(bill_depth_mm ~ bill_length_mm,
                 data = penguins) )
```

The model shows a significant relationship with bill depth decreasing by .85 mm for every one cm increase in bill length.

Checking on the model residuals

```{r}
sm::sm.density(residuals(model.lr), model = "Normal")
```

You find no strong evidence to reject the assumption of normality.

However the relationship is based on using data across all species of penguins. If you group by species you see that for each species the relationship is the opposite.

```{r}
p + 
  geom_point(mapping = aes(y = bill_depth_mm, 
                           x = bill_length_mm, 
                           color = species)) +
  geom_smooth(method = lm, 
              mapping = aes(color = species)) +
  scale_color_manual(values = c("darkorange", 
                                "purple", 
                                "cyan4"))
```

Bill depth *increases* with increasing bill length. So you conclude that the negative relationship is an artifact of grouping the penguins with different body masses together.

The paradox is resolved when the relations are addressed in the statistical modeling.

See also Berkson's paradox <https://en.wikipedia.org/wiki/Berkson%27s_paradox>
The correlation between talent and attractiveness should be close to zero, but if you ignore folks with low talent and low attractiveness (example, non-celebrities) you might find a negative correlation.

## Multiple variable regression {.unnumbered}

Multiple variable (multi-variable) linear regression is used to model a dependent variable when there is more than one explanatory variable.

Everything from simple (single explanatory variable) regression carries over to multiple variable linear regression. There is a slope coefficient for each explanatory variable.

Note: You regress the response variable onto the explanatory variables. You do not perform a regression of the variables or regress y AND x or regress y with respect to x. These are ambiguous at best and misleading at worst.

With one explanatory variable the regression model is described with a straight line. With two explanatory variables the regression model is described with a flat surface.

To see this consider a three dimensional scatter plot created with the `scatterplot3d()` from the {scatterplot3d} package and add the regression plane.

The data comes from the `trees` data frame. Start with a single-variable linear regression. You regress timber volume (`Volume`) on tree girth (`Girth`) and add the model as a line on the scatter plot.

Here I use the `plot()` method and `abline()` are base R graphics functions.

```{r}
model1 <- lm(Volume ~ Girth, 
             data = trees)
plot(trees$Volume ~ trees$Girth, pch = 16, 
     xlab = "Tree diameter (in)", 
     ylab = "Timber volume (cubic ft)")
abline(model1)
```

Next you regress timber volume (`Volume`) on girth (`Girth`) and height (`Height`) and use the `scatterplot3d()` function to plot the scatter of points. The resulting `plane3d()` function takes the regression model and adds the surface.

```{r}
model2 <- lm(Volume ~ Girth + 
                      Height, 
             data = trees)
s3d <- scatterplot3d::scatterplot3d(trees, angle = 55, 
                    scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The graph shows that timber volume increases with tree diameter and tree height.

By changing the view angle (`angle =`) you can see that some observations are above the surface and some are below.

```{r, eval=FALSE}
s3d <- scatterplot3d::scatterplot3d(trees, angle = 32, 
                    scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The distance along the vertical axis from the observation to the model surface is the residual. So even though there are two explanatory variables there is only one set of residuals as with simple regression.

The multiple linear regression model is given by: 

$$
\hat y_{i} = \hat \beta_0 + \hat \beta_1 x_{i1} + \hat \beta_2 x_{i2} + \cdots + \hat \beta_p x_{ip}
$$

The explanatory variables are written with a double subscript (there are $p$ of them) to indicate the observation number (1st subscript) and the variable number (2nd subscript).

Each explanatory variable gets a coefficient, so there are $p + 1$ of them (including $\hat \beta_0$, the y-intercept).

Now for each observation $i$ there is an observed response ($y_i$) and a predicted response ($\hat y_i$) and the difference is called the residual.

The residuals are given by the equation 

$$
y_i - \hat y_i = \varepsilon_i
$$ 
and are assumed to be described by a set of normal distributions each centered on zero and having the same variance ($\sigma^2$).

A grid of scatter plots

As with simple linear regression, before you model the data you should make a scatter plot. With more than one explanatory variable you rely on a series of scatter plots.

Consider the data set `PetrolConsumption.txt` that has gasoline consumption by state for a given year.

```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt"
PC.df <- readr::read_table(url)
head(PC.df)
```

Columns in the data frame include petrol (gas) tax (`Petrol.Tax`) [cents per gallon], per capita income (`Avg.Inc`) [\$/10], miles of paved highway (`Pavement`), proportion of drivers (`Prop.DL`), and consumption of petrol (`Petrol.Consumption`) [millions of gallons].

First create a panel of scatter plots using all the variables in the data frame with the `GGally::ggpairs()` function from the {GGally} package.

```{r}
GGally::ggpairs(PC.df)
```

The plots are arranged as a matrix. The rows correspond to the variables as given from left to right in the data frame. The first column in the data set is `Petrol.Tax` then `Avg.Inc`, etc.

The plot in row 2, column 1 is shows `Avg.Inc` on the vertical axis and `Petrol.Tax` on the horizontal axis. The value in the diagonal panel (row 1, column 2) is the Pearson correlation coefficient between `Avg.Inc` and `Petrol.Tax`.

The panel in row 3 column 1 shows `Pavement` on the vertical axis and `Petrol.Tax` on the horizontal axis.

You want to quantify how much of the variation in gas consumption at the state level is statistically explained by the variables taxes, income, amount of pavement, and proportional of drivers.

So gas consumption (the variable `Petrol.Consumption`) is your response variable and you focus on the subset of scatter plots (in row 5) where `Petrol.Consumption` is on the vertical axis in each plot.

By eye, which of the variables appears to you to be the most (least) related to gas consumption?

The variable with the highest correlation with gas consumption is `Prop.DL`, the proportion of people with a drivers license. The correlation between these two variables is .7. With more people driving, gas consumption increases.

Gas taxes have an inverse relationship with gas consumption. This is seen in the scatter plot in the lower left corner of the grid and by the negative correlation (-.45) in the upper right corner.

Plots along the diagonal are density plots of each of the variables individually.

Correlations arranged in a matrix like the grid of scatter plots is printed using the `cor()` function.

```{r}
cor(PC.df)
```

The correlations between the explanatory variables and gas consumption are located in the last column of the matrix under `Petrol.Consumption`.

From the plots and the correlation matrix you anticipate the proportion of people with drivers licenses to be the most important variable in a regression model and the coefficient on this term will be a positive number.

Further you anticipate that the gas tax will be an important variable but the coefficient on this term will be a negative number.

Another nice visualization is to use the `corrplot::corrplot()` function applied to the output of the `cor()` function.

```{r}
PC.df |>
  cor() |>
  corrplot::corrplot()
```

## Fitting a multiple variable regression model {.unnumbered}

You fit a multiple variable regression model to the gas data using the `lm()` function. You put the name of the response variable (`Petrol.Consumption`) to the left of the `~` and then the names of the explanatory variables to the right. Between each variable you include a `+` sign.

Here you assign the model to an object called `model1`.

```{r}
model1 <- lm(Petrol.Consumption ~ Prop.DL + 
                                  Pavement + 
                                  Avg.Inc + 
                                  Petrol.Tax,
             data = PC.df)
model1
```

You have four explanatory variables so the model has five coefficients. There is a coefficient for the y-intercept (`Intercept`).

Looking only at the signs on the numbers under the variable names you see that gas consumption *increases* with the proportion of drivers and *decreases* with the amount of pavement, average income and gas tax. Why might gas consumption decrease with increasing income?

The model is interpreted as follows:

Average gas consumption [millions of gallons] = 377.3 + 1336 \* Prop.DL -- 0.0024 \* Pavement -- 0.06659 \* Avg.Inc -- 34.79 \* Petrol.Tax

The model says that for every 1 percentage point increase in the proportion of drivers (to overall population), the mean gas consumption *increases* by 1336 million gallons (1.336 billion gallons) assuming `Pavement`, `Avg.Inc` and `Petrol.Tax` are constant.

For every 1 cent/gallon increase in taxes, mean gas consumption *decreases* by 34.79 million gallons assuming the three other variables are held constant.

The statistically significant variables in the model for explaining gas consumption are identified by looking at the table of coefficients from the `summary()` function.

```{r}
model1 |>
  summary()
```

You ignore the `Intercept` term as it represents the value of the response variable where the model line intersects the $y$-axis.

You see that three of the four coefficients have a $p$-value at or less than .01. Only the coefficient on the variable `Pavement` has a $p$-value greater than .15. Therefore this variable is not significant in explaining the variance in the response variable (the coefficient is not statistically different from zero).

The null hypothesis is that the variable is NOT important to the model. The $p$-value on the variable's coefficient is evidence in support of the null hypothesis. The larger the $p$-value the more evidence you have that the variable is not important to the model.

In determining which variables are most important in the regression model, the coefficient estimates (effect sizes) cannot be compared directly because they do not all have the same units.

*Key idea*: A regression coefficient has units of the response variable divided by the units of the explanatory variable. But the explanatory variables have different units. For example, `Petrol.Tax` has units of cents per gallon and `Pavement` has units of miles.

So instead of comparing the values of the coefficients directly (i.e., instead of looking in the column labeled `Estimate`) you look at the column labeled `t value`.

The magnitude of the $t$ values are comparable. They are coefficients that have been *standardized* by dividing by the corresponding standard error. Accordingly, `Avg.Inc` is more more important than `Petrol.Tax` even though .0666 is much smaller than 34.79.

*Key point*: The order of the explanatory variables does not change the magnitude or the sign of the corresponding coefficients.

```{r}
summary(lm(Petrol.Consumption ~ Pavement +
                                Petrol.Tax + 
                                Avg.Inc + 
                                Prop.DL, 
           data = PC.df))
```

*Key point*: The model does not change with a linear transformation of the variables. For example, if you multiply `Pavement` by 20, the coefficient and standard error change accordingly, but not the $t$ value or the corresponding $p$-value.

```{r}
summary(lm(Petrol.Consumption ~ I(Pavement * 20) + 
                                Petrol.Tax + 
                                Avg.Inc + 
                                Prop.DL,
           data = PC.df))
```

Note: Here the multiplication is done inside the function `I()` to restrict the interpretation of the operator `*` to multiplication rather than interaction.

The multiple R-squared value is .68 which means the model explains 68% of the variation in gas consumption.

Note that the $p$-value on the model (given on the last line of the output) is small. The null hypothesis in this case is that none of the variables are important in explaining petrol consumption. You reject this null hypothesis.

## Should you remove a variable from your model? {.unnumbered}

A model is said to be simpler if it has fewer explanatory variables.

You try another model without the `Pavement` variable (the variable that was not significant in the previous model).

```{r}
model2 <- lm(Petrol.Consumption ~ Prop.DL + 
                                  Avg.Inc + 
                                  Petrol.Tax, 
             data = PC.df)
summary(model2)
```

The remaining explanatory variables are significant. The values are slightly different as the variance in the gas consumption attributed to `Pavement` is now spread across the remaining variables.

*Key point*: Removing an explanatory variable changes the coefficient values on the remain variables in the model.

The proportion of the population with licenses is the most important variable. It has the largest $t$ value (in absolute value).

By removing a variable the R-squared statistic DECREASES to .675. This is always the case with a simpler model (fewer explanatory variables). 

Because of this, the R-squared statistic should not be used to compare models unless the models have the same number of explanatory variables.

An alternative for comparing models is the adjusted R squared. It is a modification of the R squared that accounts for the number of explanatory variables in the model.

*Key point*: The adjusted R squared increases only if the new variable _improves_ the model more than would be expected by chance. The adjusted R squared can be negative, and will always be less than or equal to R squared.

To see the importance of the adjusted R squared, first note that `Petrol.Tax` has the smallest $t$ value. Suppose you remove it. What happens?

```{r}
model3 <- lm(Petrol.Consumption ~ Prop.DL + 
                                  Avg.Inc, 
             data = PC.df)
summary(model3)
```

The adjusted R squared is smaller, so you conclude that this variable should be kept in the model.

Thus you settle on a final model: Average gas consumption [millions of gallons] = 307.3 + 1375 x Prop.DL - .06802 x Avg.Inc - 29.48 x Petrol.Tax

## Checking the model assumptions {.unnumbered}

Next you need to check the model assumptions. Equal variance: You saw how to check this assumption by using the `cut()` function and creating side-by-side box plots.

Another way is to plot the standardized residuals on the vertical axis and the fitted values along the horizontal axis. Adding a smoothed curve and the y-equal-zero line makes it easy to see whether there is a pattern to the residuals.

```{r}
model2 |> 
  fortify() |>
ggplot(mapping = aes(x = .fitted, y = .stdresid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```

The standardized residuals (`.stdresid`) are located on either side of the 0 line independent of the predicted values (`.fitted`). So in this case you conclude there is no pattern in the residuals.

To check on normality you again use the `sm::sm.density()` function.

```{r}
res <- model2 |>
  residuals()
sm::sm.density(res, xlab = "Model Residuals", model = "Normal")
```

You also examine a quantile-normal plot of the residuals.

```{r}
qqnorm(model2$residuals)
qqline(model2$residuals)
```

These plots indicate some weak evidence against normality (longer right tail). This evidence decreases the confidence you can place on your inferences (e.g., stating a particular variable is significant in explaining variations in the response variable).

To improve model adequacy you might transform the response variable or use a weighted regression model. You will return to this shortly.

## Using the model to make predictions {.unnumbered}

Posterior predictive checks mean simulating replicated data under the fitted model and then comparing these to the observed data. Posterior predictive checks are used to look for systematic discrepancies between real and simulated data.

The {performance} package gives posterior predictive check methods using the `check_predictions()` function. The function works for a variety of frequentist and Bayesian models. 

Note: To get a plot make sure you have the package {see} available.

```{r}
performance::check_predictions(model2)
```

The model predicted lines (blued) should resemble the observed data line (smoothed density of response variable). Here you see that it looks pretty good, but improvements could be made by removing the right skew.

Example: Predicting house prices.

A Realtor can use multiple variable regression to justify a selling price for a house based on a list of features the house has. Here you consider a data file (`houseprice.txt`) containing a random sample of 107 home sales in Albuquerque, New Mexico during the period February 15 through April 30, 1993 (Albuquerque Board of Realtors, 1993).

Get the data.

```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/houseprice.txt"
hp.df <- readr::read_table(url)
head(hp.df)
```

The data include:

-   `price`: Selling price in \$100s
-   `sqft`: Square feet of living space
-   `custom`: Whether the house was built with custom features (1) or not (0)
-   `corner`: Whether the house sits on a corner lot (1) or not (0)
-   `taxes`: Annual taxes in \$

Here you assume that taxes determine price. In some (many?) real estate contexts the causality would work in the opposite direction: selling price affects appraisals and hence taxes.

Use the `ggpairs()` function from the {GGally} package to get a look at your data.

```{r}
GGally::ggpairs(hp.df)
```

As expected selling prices increase with size of the living space and with taxes. Scatter plots are not very informative for binary variables (variables with only two values).

The response variable is selling price (`price`). You begin with living space (`sqft`) and whether the house was `custom` built as the two explanatory variables.

```{r}
model1 <- lm(price ~ sqft + 
                     custom, 
             data = hp.df)
summary(model1)
```

The model indicates that for a one square foot increase in living space, selling prices increase by \$54 controlling for whether or not it has custom features. Note: you move the decimal place on the coefficient to the right two places because selling price is in 100s of dollars.

The model also indicates that custom-featured houses (indicated with a value of 1 in the variable `custom`) sell for more by \$17,221 on average controlling for living space.

To predict the selling price of a house that has custom features and has 3500 square feet of living space type

```{r}
predict(model1, 
        newdata = data.frame(sqft = 3500, custom = 1), 
        interval = "confidence")
```

The model predicts a selling price of \$219.5K with a 95% uncertainty interval between \$205.7K and \$233.3K.

## Collinearity and what to do about it {.unnumbered}

With more than one explanatory variable in a regression model you need to consider the possibility that the model coefficients are imprecise (small changes in input can lead to large changes in the model) due to collinearity (multicollinearity). Collinearity is when explanatory variables are highly correlated.

If collinearity exists then your ability to properly interpret the model is compromised.

As an example, consider the data set called `fat.txt` containing measurements related to body fat from 47 individuals.

```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/fat.txt"
bf.df <- readr::read_table(url)
head(bf.df)
```

The response variable `bodyfat` is a percentage measured using special equipment. The four explanatory variables are circumferences of different body parts and are easy to measure. So you are interested in a model to predict `bodyfat` from these easier to measure variables.

Again you start with a grid of scatter plots.

```{r}
GGally::ggpairs(bf.df)
```

Here `abdomen` circumference has the strongest linear relationship with `bodyfat`, but other variables have high direct (positive) correlation. Note the large correlation between the explanatory variables.

Next you regress `bodyfat` onto the four explanatory variables and summarize the model.

```{r}
model1 <- lm(bodyfat ~ abdomen + 
                       biceps + 
                       forearm +
                       wrist, 
             data = bf.df)
summary(model1)
```

What is wrong here?

The negative coefficient on the variable `biceps` circumference is opposite of the sign on the correlation between `bodyfat` and `biceps`.

```{r}
cor(bf.df$bodyfat, bf.df$biceps)
```

This is an indication of collinearity. The problem is that there is a large correlation between `abdomen` and `biceps` leading to a model that may not make physical sense.

_Key idea_: A rule of thumb is that when the correlation between two explanatory variables exceeds .6, collinearity can be a problem.

When explanatory variables have large correlation estimates of the model parameters are not precise. It is risky to make inferences using a model with imprecise parameter estimates. The best way to proceed in this situation is to reduce the set of explanatory variables.

Remove the explanatory variable that makes the least sense for inclusion in the model from physical arguments. For these data, it is probably best to remove all variables except `abdomen`. That is, abdomen circumference is the best indicator of body fat and the other variables do not add any additional new information in understanding body fat.

## How to include and interpret interactions terms {.unnumbered}

Sometimes the *relationship* between an explanatory variable and the response variable *depends on* another explanatory variable. In this case you say there is an *interaction* between the two explanatory variables.

There are two questions you should ask before including interaction in your model 

(1) Does this interaction make sense conceptually?
(2) Is the interaction term statistically significant? Or, whether or not we believe the slopes of the regression lines are significantly different.

For instance, returning to the housing prices data frame, the relationship between selling price and living space might depend on whether the house was custom built. To see if the interaction might need to be included in the regression model first make a plot.

```{r}
ggplot(data = hp.df, 
       mapping = aes(y = price, 
                     x = sqft, 
                     color = factor(custom))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Living Space (sq ft)") +
  ylab("Selling Price ($100)")
```

Here the conditioning variable must be a factor. You see both regression lines have about the same slope. This implies the relationship between selling price and living space does NOT depend on whether the house has custom features.

In contrast, whether or not the house is on the corner influences the relationship between selling price and living space.

```{r}
ggplot(data = hp.df, 
       mapping = aes(y = price, 
                     x = sqft, 
                     color = factor(corner))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Living Space (sq ft)") +
  ylab("Selling Price ($100)")
```

The slope of the regression line between selling price and house living space is different for corner and non-corner houses. The regression lines have different slopes and cross. This is indicates an interaction between the continuous variable of living space and the categorical variable of corner.

To add an interaction term to the model, you use the `:` notation. You need to know how the categorical variable is coded to correctly interpret the model [On a corner lot (1), or not (0)]

```{r}
lm(price ~ sqft + 
           corner + 
           sqft:corner, 
   data = hp.df) |>
summary()
```

Or, you can use the notation `*` to add both the main effects and the interaction effect to the model. Models with interaction effects should also include the main effects, even if these main effects are not significant.

```{r}
lm(price ~ sqft * corner, 
   data = hp.df) |>
summary()
```

The model indicates that for a one square foot increase in living space, selling prices increase by \$75 for the non-corner houses. The coefficient for the interaction is -.453 which is the difference in slope between the non-corner and corner houses, i.e., the slope for the corner houses is .752 - .453 = .299 (\~\$30).

Lastly the coefficient on the corner term of 629 is the difference in the intercepts between non-corner and corner houses. Or the difference in the y-intercept when living space has a value of zero. The coefficient is significant when living space is zero but this is not realistic. 

You transform the square footage variable by centering it about the mean value and refit the model.

```{r}
lm(price ~ I(sqft - mean(sqft)) + 
           corner + 
           I(sqft - mean(sqft)):corner, 
   data = hp.df) |>
summary()
```

Now the coefficient on the corner term is negative and still significant.

If you look back at the plot above showing the two slope lines for corner versus non-corner houses you see that the lines intersect about a value of 1500 square feet. So you center on 1500 and refit the model.

```{r}
lm(price ~ I(sqft - 1500) + 
           corner + 
           I(sqft - 1500):corner,
   data = hp.df) |>
summary()
```
Now the intercept difference is close to zero and insignificant.

Centering explanatory variables has no influence on the main effects in a regression model with only main effects. In contrast, centering has an influence on the main effects in a regression model that includes interaction terms.  

*Key idea*: When interpreting the results from models that contain interaction terms, the rule is to NOT interpret the coefficients on the main effects. The presence of interactions implies that the meaning of coefficients for terms vary depending on the values of the other variables and thus cannot be interpreted directly.

Thursday: Assignment #5 in class.