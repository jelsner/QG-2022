# Tuesday, October 25, 2022 {-}

Today

- Simpson's paradox

## Simpson's paradox {-}

The four assumptions that under gird a linear regression model include (1) linearity, (2) equal variance, (3) normality of residuals, and (4) independence of observations. 

But even if those assumptions are valid, another issue is that of scale.

Simpson's paradox occurs when a trend appears in different groups of data but disappears or reverses when the groups are combined.

It is encountered in social-science and medical-science statistics and is problematic when data are aggregated at various scales (spatial or otherwise). 

The paradox can be resolved when causal relations are appropriately addressed in the statistical modeling.

As an example, consider again the relationship between bill length and bill depth in the Palmer penguins data without regards to species.

You start with a scatter plot and a linear regression model.
```{r}
library(palmerpenguins)

( p <- ggplot(data = penguins,
            mapping = aes(y = bill_depth_mm, x = bill_length_mm)) +
       geom_point() +
       geom_smooth(method = lm) )

lm(bill_depth_mm ~ bill_length_mm,
   data = penguins)
```

The plot shows an inverse relationship between bill depth and bill length. And the regression model shows a statistically significant relationship with bill depth decreasing by .85 cm for every one mm increase in bill length.

However the relationship is across all species of penguins. If you group by species you see that for each species the relationship is the opposite.
```{r}
p + 
  geom_point(mapping = aes(y = bill_depth_mm, x = bill_length_mm, color = species)) +
  geom_smooth(method = lm, 
              mapping = aes(color = species)) +
  scale_color_manual(values = c("darkorange", "purple", "cyan4"))
```

That is bill depth _increases_ with increasing bill length. So you conclude that the negative relationship is an artifact of grouping the penguins with different body masses together.

The paradox is resolved when the relations are appropriately addressed in the statistical modeling.

See also Berkson's paradox https://en.wikipedia.org/wiki/Berkson%27s_paradox
## A regression plane {-}

Multiple variable (multi-variable) linear regression is used to model a dependent variable when there is more than one explanatory variable.

Everything from simple (single explanatory variable) regression carries over to multiple variable linear regression. There is a slope coefficient for each explanatory variable. 

Note: You regress the response variable onto the explanatory variables. You do not perform a regression of the variables or regress y AND x or regress y with respect to x. These are ambiguous at best and misleading at worse.

With one explanatory variable the regression model is described with a straight line. With two explanatory variables the regression model is described with a flat surface.

To see this consider a three dimensional scatter plot created with the `scatterplot3d()` from the {scatterplot3d} package and add the regression plane.

The data comes from the `trees` data frame. You start with a (simple) linear regression. You regress timber volume (`Volume`) on tree girth (`Girth`) and add the model as a line on the scatter plot. 

Here I use the `plot()` method and `abline()` are base R graphics functions.
```{r}
model1 <- lm(Volume ~ Girth, 
             data = trees)
plot(trees$Volume ~ trees$Girth, pch = 16, 
     xlab = "Tree diameter (in)", 
     ylab = "Timber volume (cubic ft)")
abline(model1)
```

Next you regress timber volume (`Volume`) on girth (`Girth`) and height (`Height`) and use the `scatterplot3d()` function to plot the scatter of points. The resulting `plane3d()` function takes the regression model and adds the surface.
```{r}
model2 <- lm(Volume ~ Girth + Height, data = trees)
s3d <- scatterplot3d::scatterplot3d(trees, angle = 55, scale.y = .7, pch = 16, 
                                    xlab = "Tree diameter (in)", 
                                    zlab = "Timber volume (cubic ft)",
                                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The graph shows that timber volume increases with tree diameter and tree height.

By changing the view angle (`angle =`) you can see that some observations are above the surface and some are below.
```{r, eval=FALSE}
s3d <- scatterplot3d(trees, angle = 32, scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The distance along the vertical axis from the observation to the model surface is the residual. So even though there are two explanatory variables there is only one set of residuals as with simple regression.

The multiple linear regression model is given by:
$$
\hat y = \hat \beta_0 + \hat \beta_1 x_{i1} + \hat \beta_2 x_{i2} + \cdots + \hat \beta_p x_{ip}
$$

The explanatory variables are written with a double subscript (there are $p$ of them) to indicate the observation number (1st subscript) and the variable number (2nd subscript).

Each explanatory variable gets a coefficient, so there are $p + 1$ of them (including $\hat \beta_0$, the y-intercept).

Now for each observation $i$ there is an observed response ($y_i$) and a predicted response ($\hat y_i$) and the difference is called the residual.

The residuals are given by the equation
$$
y_i - \hat y_i = \varepsilon_i
$$
and are assumed to be described by a set of normal distributions each centered on zero and having the same variance ($\sigma^2$).

A grid of scatter plots

As with simple linear regression, before you model the data you should make a scatter plot. With more than one explanatory variable you rely on a series of scatter plots.

Consider the data set `PetrolConsumption.txt` that has gasoline consumption by state for a given year.
```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt"
PC.df <- readr::read_table(url)
head(PC.df)
```

Columns in the data frame include petrol (gas) tax (`Petrol.Tax`) [cents per gallon], per capita income (`Avg.Inc`) [$/10], miles of paved highway (`Pavement`), proportion of drivers (`Prop.DL`), and consumption of petrol (`Petrol.Consumption`) [millions of gallons].

First create a panel of scatter plots using all the variables in the data frame with the `GGally::ggpairs()` function from the {GGally} package.
```{r}
GGally::ggpairs(PC.df)
```

The plots are arranged in a grid. The rows correspond to the variables as given from left to right in the data frame. The first column in the data set is `Petrol.Tax` then `Avg.Inc`, etc.

The plot in row 2, column 1 is shows `Avg.Inc` on the vertical axis and `Petrol.Tax` on the horizontal axis. The value in the diagonal panel (row 1, column 2) is the Pearson correlation coefficient between `Avg.Inc` and `Petrol.Tax`. 

The panel in row 3 column 1 shows `Pavement` on the vertical axis and `Petrol.Tax` on the horizontal axis.

You want to quantify how much of the variation in gas consumption at the state level is statistically explained by the variables taxes, income, amount of pavement, and proportional of drivers.

So gas consumption (the variable `Petrol.Consumption`) is your response variable and you focus on the subset of scatter plots (in row 5) where `Petrol.Consumption` is on the vertical axis in each plot.

By eye, which of the variables appears to you to be the most (least) related to gas consumption?

The variable with the highest correlation with gas consumption is `Prop.DL`, the proportion of people with a drivers license. The correlation between these two variables is .7. With more people driving, gas consumption increases.

Gas taxes have an inverse relationship with gas consumption. This is seen in the scatter plot in the lower left corner of the grid and by the negative correlation (-.45) in the upper right corner.

Plots along the diagonal are density plots of each of the variables individually. 

Correlations arranged in a matrix like the grid of scatter plots is printed using the `cor()` function.
```{r}
cor(PC.df)
```

The correlations between the explanatory variables and gas consumption are located in the last column of the matrix under `Petrol.Consumption`.

From the plots and the correlation matrix you anticipate the proportion of people with drivers licenses to be the most important variable in a regression model and the coefficient on this term will be a positive number. 

Further you anticipate that the gas tax will be an important variable but the coefficient on this term will be a negative number.

## Fitting a multiple variable regression model {-}

You fit a multiple variable regression model to the gas data using the `lm()` function. You put the name of the response variable (`Petrol.Consumption`) to the left of the `~` and then the names of the explanatory variables to the right. After each variable you include a `+` sign (except for the last variable). 

Here you assign the model to an object called `model1`.
```{r}
model1 <- lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax,
             data = PC.df)
model1
```

You have four explanatory variables so the model has five coefficients. There is a coefficient for the y-intercept (`Intercept`).

Looking only at the signs on the numbers under the variable names you see that gas consumption _increases_ with the proportion of drivers and _decreases_ with the amount of pavement, average income and gas tax. Why might gas consumption decrease with increasing income?

The model is interpreted as follows:

Average gas consumption [millions of gallons] = 377.3 + 1336 * Prop.DL – 0.0024 * Pavement – 0.06659 * Avg.Inc – 34.79 * Petrol.Tax

The model says that for every 1 percentage point increase in the proportion of drivers (to overall population), the mean gas consumption _increases_ by 1336 million gallons (1.336 billion gallons) assuming `Pavement`, `Avg.Inc` and `Petrol.Tax` are constant. 

For every 1 cent/gallon increase in taxes, mean gas consumption _decreases_ by 34.79 million gallons assuming the three other variables are held constant.

The statistically significant variables in the model for explaining gas consumption are identified by looking at the table of coefficients from the `summary()` function.
```{r}
summary(model1)
```

You ignore the `Intercept` term. It represents the value of the response variable where the model line intersects the $y$-axis.

You see that three of the four coefficients have a $p$-value at or less than .01. Only the coefficient on the variable `Pavement` has a $p$-value greater than .15. Therefore this variable is not significant in explaining the variance in the response variable (the coefficient is not statistically significant from a value of zero).

The null hypothesis is that the variable is NOT important to the model. The $p$-value on the variable's coefficient is evidence in support of the null hypothesis. The larger the $p$-value the more evidence you have that the variable is not important to the model. 

In determining which variables are most important in the regression model, the coefficient estimates (effect sizes) cannot be compared directly because they do not have the same units. 

Key idea: A regression coefficient has units of the response variable divided by the units of the explanatory variable. But the explanatory variables have different units. For example, `Petrol.Tax` has units of cents per gallon and `Pavement` has units of miles.

So instead of comparing the values of the coefficients directly (i.e., instead of looking in the column labeled `Estimate`) you look at the column labeled `t value`.

The magnitude of the $t$ values are inter comparable. They are coefficients that have been _standardized_ by dividing by the corresponding standard error. Accordingly, `Avg.Inc` is more more important than `Petrol.Tax` even though .0666 is much smaller than 34.79.

Key point: The order of the explanatory variables does not change the magnitude or the sign of the corresponding coefficients.
```{r}
summary(lm(Petrol.Consumption ~ Pavement + Petrol.Tax + Avg.Inc + Prop.DL, 
           data = PC.df))
```

Key point: The model does not change with a linear transformation of the variables. For example, if you multiply `Pavement` by 20, the coefficient and standard error change accordingly, but not the $t$ value or the corresponding $p$-value.
```{r}
summary(lm(Petrol.Consumption ~ I(Pavement * 20) + Petrol.Tax + Avg.Inc + Prop.DL,
           data = PC.df))
```

Side note: Here the multiplication is done inside the function `I()` to restrict the interpretation of the operator `*` to multiplication rather than interaction.

The multiple R-squared value is .68 which means the model explains 68% of the variation in gas consumption.  

Note that the $p$-value on the model (given on the last line of the output) is small. The null hypothesis in this case is that none of the variables are important in explaining petrol consumption. You reject this null hypothesis.

## Should you remove a variable from the model? {-}

A model is said to be simpler if it has fewer explanatory variables.

You try another model without the `Pavement` variable (the variable that was not significant in the previous model).
```{r}
model2 <- lm(Petrol.Consumption ~ Prop.DL + Avg.Inc + Petrol.Tax, 
             data = PC.df)
summary(model2)
```

The remaining explanatory variables are significant. The values are slightly different as the variance in the gas consumption attributed to `Pavement` is now spread across the remaining variables. 

Key point: Removing an explanatory variable changes the coefficient values on the remain variables in the model.

The proportion of the population with licenses is the most important variable. It has the largest $t$ value (in absolute value).

By removing a variable the R-squared statistic DECREASES to .675. This is always the case with a simpler model (fewer explanatory variables). Thus the R-squared statistic should not be used to compare models unless the models have the same number of explanatory variables.  

One alternative for comparing models is the adjusted R squared. It is a modification of the R squared that accounts for the number of explanatory variables in the model.

Key point: The adjusted R squared increases only if the new variable improves the model more than would be expected by chance. The adjusted R squared can be negative, and will always be less than or equal to R squared.

To see the importance of the adjusted R squared, first note that `Petrol.Tax` has the smallest $t$ value. Suppose you remove it. What happens?
```{r}
model3 <- lm(Petrol.Consumption ~ Prop.DL + Avg.Inc, 
             data = PC.df)
summary(model3)
```

The adjusted R squared is smaller, so you conclude that this variable should be kept in the model.

Thus you settle on a final model: 
Average gas consumption [millions of gallons] = 307.3 + 1375 x Prop.DL - .06802 x Avg.Inc - 29.48 x Petrol.Tax

## Checking on the model assumptions {-}

Next you need to check the model assumptions. Equal variance: You saw how to check this assumption by using the `cut()` function and creating side-by-side box plots. 

Another way is to plot the standardized residuals on the vertical axis and the fitted values along the horizontal axis. Adding a smoothed curve and the y equal zero line makes it easy to see whether there is a pattern to the residuals.
```{r}
library(ggplot2)
model2.df <- fortify(model2)
ggplot(data = model2.df, 
       mapping = aes(x = .fitted, y = .stdresid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```

In this case: no pattern in the residuals.

Normality. First you use the `sm.density()` function.
```{r}
res <- residuals(model2)
sm::sm.density(res, xlab = "Model Residuals", model = "Normal")
```

You also examine a quantile-normal plot of the residuals.
```{r}
qqnorm(model2$residuals)
qqline(model2$residuals)
```

These plots indicate some evidence against normality and constant variance. The fact that the residuals do not exactly follow a normal distribution decreases the confidence you can place on your inferences (e.g., stating a particular variable is significant in explaining variations in the response variable).

To improve model adequacy you might transform the response variable or use a weighted regression model. You will return to this shortly.

## Using the model to make predictions {-}

https://easystats.github.io/performance/reference/check_predictions.html

Posterior predictive checks mean simulating replicated data under the fitted model and then comparing these to the observed data. Posterior predictive checks are used to look for systematic discrepancies between real and simulated data.

The {performance} package gives posterior predictive check methods for a variety of frequentist and Bayesian models.
```{r}
performance::check_predictions(model2)
```

The model predicted lines should resemble the observed data line (smoothed density of response variable). Here you see that it looks pretty good, but improvements could be made.

Example: Predicting house prices

A Realtor can use multiple variable regression to justify a selling price for a house based on a list of features the house has. Here you consider a data file (`houseprice.txt`) containing a random sample of 107 home sales in Albuquerque, New Mexico during the period February 15 through April 30, 1993 (Albuquerque Board of Realtors, 1993).

Get the data.
```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/houseprice.txt"
hp.df <- readr::read_table(url)
head(hp.df)
```

The data include:

* `price`: Selling price in \$100s
* `sqft`: Square feet of living space
* `custom`: Whether the house was built with custom features (1) or not (0)
* `corner`: Whether the house sits on a corner lot (1) or not (0)
* `taxes`: Annual taxes in \$

Here you assume that taxes determine price. In some (many?) real estate contexts the causality would work in the opposite direction: selling price affects appraisals and hence taxes.

Use the `ggpairs()` function from the {GGally} package to get a look at your data.
```{r}
GGally::ggpairs(hp.df)
```

As expected selling prices increase with size of the living space and with taxes. Scatter plots are not very informative for binary variables (variables with only two values).

The response variable is selling price (`price`). You begin with living space (`sqft`) and whether the house was `custom` built as the two explanatory variables.
```{r}
model1 <- lm(price ~ sqft + custom, 
             data = hp.df)
summary(model1)
```

The model indicates that for a one square foot increase in living space, selling prices increase by \$54 controlling for whether or not it has custom features. You move the decimal place on the coefficient to the right two places because selling price is in 100s of dollars.

The model also indicates that custom-featured houses (indicated with a value of 1 in the variable `custom`) sell for more by \$17,211 on average controlling for living space.

To predict the selling price of a house that has custom features and has 3500 square feet of living space type
```{r}
predict(model1, 
        newdata = data.frame(sqft = 3500, custom = 1), 
        interval = "confidence")
```

The model predicts a selling price of \$219.5K with a 95% uncertainty interval between \$205.7K and \$233.3K.

## Collinearity {-}

With more than one explanatory variable in a regression model you need to consider the possibility that the model coefficients are imprecise (small changes in input can lead to large changes in the model) due to collinearity (multicollinearity). Collinearity is when explanatory variables are highly correlated.

If collinearity exists then your ability to properly interpret the model is compromised.

As an example, consider the data set called `fat.txt` containing measurements related to body fat from 47 individuals. 
```{r}
url <- "http://myweb.fsu.edu/jelsner/temp/data/fat.txt"
bf.df <- readr::read_table(url)
head(bf.df)
```

The response variable `bodyfat` is a percentage measured using special equipment. The four explanatory variables are circumferences of different body parts and are easy to measure. So you are interested in a model to predict `bodyfat` from these easier to measure variables.

As always, you start with a grid of scatter plots.
```{r}
GGally::ggpairs(bf.df)
```

Here `abdomen` has the strongest linear relationship with `bodyfat`, but other variables have high direct (positive) correlation. Note the large correlation between the explanatory variables.

Next you regress `bodyfat` onto the four explanatory variables and summarize the model.
```{r}
model1 <- lm(bodyfat ~ abdomen + biceps + forearm + wrist, 
            data = bf.df)
summary(model1)
```

What is wrong here?

The negative coefficient on the variable `biceps` is opposite of the sign on the correlation between `bodyfat` and `biceps`.
```{r}
cor(bf.df$bodyfat, bf.df$biceps)
```

This is an indication of collinearity. The problem is that there is a large correlation between `abdomen` and `biceps` leading to a model that may not make physical sense.

Key idea: The rule is that when the correlation between two explanatory variables exceeds .6, collinearity can be a problem.


When explanatory variables have large correlation then estimates of the model parameters are not precise. A model with imprecise parameter estimates is not useful. The best way to proceed in this situation is to reduce the set of explanatory variables. 

Remove the explanatory variable that makes the least sense from physical arguments. For these data, it is probably best to remove all variables except `abdomen`.

## Interactions {-}

Sometimes the _relationship_ between an explanatory variable and the response variable _depends on_ another explanatory variable. In this case you say there is an _interaction_ between the two explanatory variables.

For instance, the relationship between selling price and living space might depend on whether the house was custom built. To see if the interaction might need to be included in the regression model first make a plot.
```{r}
ggplot(data = hp.df, 
       mapping = aes(y = price, x = sqft, color = factor(custom))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Living Space (sq ft)") +
  ylab("Selling Price ($100)")
```

Here the conditioning variable must be a factor. You see both regression lines have about the same slope. This implies the relationship between selling price and living space does not depend on whether the house has custom features.

In contrast, whether or not the house is on the corner influences the relationship between selling price and living space.
```{r}
ggplot(data = hp.df, 
       mapping = aes(y = price, x = sqft, color = factor(corner))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Living Space (sq ft)") +
  ylab("Selling Price ($100)")
```

The slope of the regression line between selling price and house living space is different for corner and non-corner houses. The regression lines have different slopes and cross. This is indicates an interaction between the continuous variable of living space and the categorical variable of corner.

To add an interaction term to the model, you use the `:` notation. You need to know how the categorical variable is coded to correctly interpret the model [On a corner lot (1), or not (0)]
```{r}
model1 <- lm(price ~ sqft + corner + sqft:corner, 
             data = hp.df)
summary(model1)
```

Or, you can use the notation `*` to add both the main effects and the interaction effect to the model. Models with interaction effects should also include the main effects, even if these main effects are not significant.
```{r}
model1b <- lm(price ~ sqft * corner, 
              data = hp.df)
summary(model1b)
```

The model indicates that for a one square foot increase in living space, selling prices increase by \$75 for the non-corner houses. The coefficient for the interaction is -.453 which is the difference in slope between the non-corner and corner houses, i.e., the slope for the corner houses is .752 - .453 = .299 (~\$30).

Lastly the coefficient on the corner term of 629 is the difference in the intercepts between non-corner and corner. Or the difference in the y-intercept when living space has a value of zero. The coefficient is significant when living space is zero but this is not realistic. You transform the variables and refit the model.
```{r}
model1c <- lm(price ~ I(sqft - mean(sqft)) + corner + 
               I(sqft - mean(sqft)):corner, 
             data = hp.df)
summary(model1c)

model1d <- lm(price ~ I(sqft - 1500) + corner + I(sqft - 1500):corner,
             data = hp.df)
summary(model1d)
```

In both cases the intercept difference is negative. In the second case it is insignificant. 

When interpreting the results from models that contain interaction terms, the rule is to not interpret the coefficients on the main effects (marginal coefficients). As demonstrated the presence of interactions implies that the meaning of coefficients for terms vary depending on the values of the other variables and thus are not easily interpreted.