# Thursday, October 13, 2022 {-}

Today

- More examples of Bayesian data analyses

Example: Percentage of survey signups

Let's consider another example. You are studying attitudes toward race in America since the election of Trump and you want to determine the likely response rate before you send out a large survey on attitudes. I use the pronoun 'you' because I really want you to imagine yourself in this situation.

Your adviser suggests that you will get the best response rate using an email listserv (your advisor is obviously older than you). So you ask 16 people on the listserv and get 6 positive responses (6 are willing to take your survey): 6/16 = 38%

What is a good guess for the positive response rate when you start sending out the survey to everyone on the listserv? How uncertain is this guess? Perhaps 38% is good guess, but the uncertainty is large.

Recall: A Bayesian data analysis requires three things: 1) Data, 2) Generative Model, and 3) Prior: information you have before examining the data.

A generative model for people agreeing to take your survey

A. Assume there is single underlying rate at which each person agrees to take your survey. Say 55%.
B. "Ask" a number of people, where the chance of each person agreeing to take the survey is 55%.

"Ask" is in quotes because you are not going to actual ask. Rather you will use a random number function where the probability of getting a 'yes' is 55% and the probability of getting 'no' is 45%.
```{r}
set.seed(3045)
sample(c("yes", "no"), 
       size = 16,
       prob = c(.55, .45),
       replace = TRUE)
```

C. Count how many people agreed to be surveyed. 

In this case it is 9/16.

This is your generative model. A model where you can plug in the rate (parameter of interest) and generate data. Parameter values (e.g., 55%) --> Ask 16 people --> Data (positive response rate).

The problem is these simulated data are not really what you want. In fact you want just the opposite. You have data. You know that when you asked 16 actual people 6 of them said 'yes'.

You want to go backwards. You have P(D|$\theta$) but you want P($\theta$|D). From the data you want to figure out what the response rate could be.  Data --> Parameter values (theta?). What are reasonable values for theta that would give you the data you have (6 out 16).

The good news is you're almost there. You just need one more thing; a prior.

Your prior is the information you have _before_ seeing the data?

You need to use probability to represent uncertainty in all parts of your model. You have already included uncertainty in the generative model (e.g., through the `sample` function). But you need to include it on the prior.

Here you represent the uncertainty about the response rate as a uniform distribution between 0 and 1. Prior to seeing any data you assume that the response rate is equally likely to be any value between 0 and 100%.

Generate values from the prior and plot the distribution. Always a good idea to look at the distribution of your prior. Does it look reasonable?

```{r}
library(ggplot2)

M <- 100000
priorRate <- runif(n = M, min = 0, max = 1)
ggplot(data = as.data.frame(priorRate), 
       mapping = aes(x = priorRate)) +
  geom_histogram(binwidth = .1, boundary = 1, 
                 color = "white", fill = "gray") +
  xlab("Response Rate") +
  ggtitle(label = "Your prior distribution",
          subtitle = "Survey response rate") +
  theme_minimal()
```

Define the generative model and simulate the data

You could use the `sample()` function as before and then added up all the `yes` responses but here the `rbinom()` function does that for you. It returns the number of 1's in a sample of `size =` (here 16) when the probability of agreeing equals the `rate`.

```{r}
genModel <- function(rate){
  nYes <- rbinom(1, size = 16, prob = rate)
  nYes
}

nYes <- NULL
for(i in 1:M) {
  nYes[i] <- genModel(priorRate[i])
}
```

Keep only those parameter values that resulted in the data that you actually observed (6 "yes"'s out of 16). Check to see if there are enough samples remaining (at least 1000). If not, generate more samples.

```{r}
postRate <- priorRate[nYes == 6]
length(postRate)
```

Plot a histogram from the posterior values.

```{r}
ggplot(data = as.data.frame(postRate), 
       mapping = aes(x = postRate)) +
  geom_histogram(binwidth = .05, boundary = 1, 
                 color = "white", fill = "gray") +
  scale_x_continuous(limits = c(0, 1)) +
  xlab("Response Rate") +
  ggtitle(label = "Your posterior distribution",
          subtitle = "Survey response rate") +
  theme_minimal()
```

Use sample statistic functions to summarize information about the posterior distribution.

```{r}
median(postRate)
quantile(postRate, probs = c(.25, .75))
```

A good estimate of the response rate is 39% with there being a 50% (75% - 25%) chance that the response rate will be between 31% and 46%. 

There is slightly more than a 1% chance that you will do better than a 65% positive response rate.
```{r}
sum(postRate > .65)/length(postRate) * 100
```

Plot the prior and posterior distributions together.

```{r}
df <- data.frame(Distribution = rep(c("(A) Prior Distribution", 
                                     "(B) Posterior Distribution"), 
                                    c(M, length(postRate))), 
                Rate = c(priorRate, postRate))
ggplot(data = df, 
       mapping = aes(x = Rate)) +
  geom_histogram(aes(y = (..density..) * .1), binwidth = .05, boundary = 1,
                 color = "white", fill = "gray") +
  scale_x_continuous(limits = c(0, 1)) +
  facet_wrap(~ Distribution, ncol = 1) +
  ylab("Probability") + 
  xlab("Survey response rate") +
  theme_minimal()
```

So how did you go from the prior to the posterior? Let's take a response rate of 35% as an example. First a response rate of 35% had to be drawn from your prior and it did it with some probability you denote P(35%). That is, the probability of drawing a response rate of 35% is P(35%).

Now in order to keep this 35% it had to simulate data that matched the data you actually observed. And it did that with some probability that you denote P(6 "yes" | 35%). Probability of 6 positive responses (out of 16) given a response rate of 35%. Recall: the vertical line in P(X | Y) is read: "X given Y".

By multiplying these two probabilities you get the probability that 35% is the best estimate for the response rate given 6 yes responses, proportionally. That is,
$$
P(35\% | 6 ) \propto P(35\%) \times P(6 | 35\%)
$$

To turn that proportionality ($\propto$) into an equality you need to divide by the sum of all parameter value probabilities given the data.
$$
P(35\% | 6 ) = \frac{P(35\%) \times P(6 | 35\%)}{\sum P(\theta\%) \times P(6 | \theta\%)}
$$

To review: You specified prior information $P(\theta)$, a generative model $P(D|\theta)$, and you calculated probabilities for the different parameter values given the data. In this case there was a single parameter value indicating the response rate.

But the cool thing is this general method works for any generative model and any number of parameter values.
$$
P(\theta | D) = \frac{P(\theta) \times P(D|\theta)}{\sum P(\theta) \times P(D|\theta)}
$$
Bayes Theorem.

Prediction

Suppose the listserv you used had 100 people. What is the likely number of surveys that will get returned?

1. The answer is not a single number but a distribution over probable number of survey takers. 
2. As before, the binomial distribution is a good candidate for how many people are willing to take the survey out of the 100 possible.
3. Donâ€™t 'throw away' uncertainty, for example by using a summary of the posterior distribution. Instead use the full original posterior sample (all the values in the posterior).
4. The general pattern when calculating 'derivatives' of posterior values is to go through the values one-by-one, and perform a transformation (say, plugging in the value in a binomial distribution), and collect the new values in a vector.

For example the first six values in the vector of posterior values are:
```{r}
postRate[1:6]
```

You can multiply any of these by 100 (and round) to get a count. But what value do you choose? Also, given a rate the count is a random value.
```{r}
nSurveysX <- round(100 * postRate, 0)
```

To get a prediction about how many responses you need to take both these uncertainties into account.

This can be done with a `for` loop.

```{r}
nSurveys <- NULL
for(i in 1:length(postRate)){
  nSurveys[i] <- rbinom(n = 1, size = 100, 
                       prob = postRate[i])
}
```

But since `rbinom()` is vectorized (calculations are done in tandem) you can simply write it like this:

```{r}
nSurveys <- rbinom(n = length(postRate), 
                  size = 100, 
                  prob = postRate)

ggplot(data = as.data.frame(nSurveys), 
       mapping = aes(x = nSurveys)) +
  geom_histogram(binwidth = 5, color = "white") +
  xlab("Number of responses") +
  ggtitle(label = "Posterior predicted number of responses", 
          subtitle = "Given that 100 people were asked") +
  theme_minimal()
```

So you are most likely to get about 40 surveys returned but it could be anywhere between 20 and 60.

Compare with `nSurveysX` where only one source of uncertainty is included.
```{r}
df <- data.frame(Uncertainty = c(rep("Both", length(nSurveys)), rep("One", length(nSurveysX))), 
                                 nSurveys = c(nSurveys, nSurveysX))
ggplot(data = df, 
       mapping = aes(x = nSurveys)) +
  geom_histogram(binwidth = 5, color = "white") +
  facet_wrap(~ Uncertainty) +
  xlab("Number of responses") +
  ggtitle(lab = "Posterior predicted number of responses", 
          subtitle = "Given that 100 people were asked") +
  theme_minimal()
```

The histogram is slightly wider when both sources of uncertainty are included. Quantitatively you can see this with
```{r}
quantile(nSurveys, probs = c(.05, .95))
quantile(nSurveysX, probs = c(.05, .95))
```
  
Approximate Bayesian computation

The approximate Bayesian computation only works in rare cases. Conceptually easy, but very slow. Other much faster methods are available. All with the same end result (posterior distribution).

You don't really need to worry about how they work precisely. They are just different ways of applying Bayes Theorem, summarized as combining the prior distribution with the generative model (likelihood function) to get the posterior distribution.

What Bayesian data analysis is not

A category of models (e.g., regression models, decision trees). Rather it is a way of thinking about and constructing models. You can do regression or machine learning with a Bayesian framework (e.g., Bayesian decision analysis).

Not subjective. All statistics require you make assumptions. Results have to be interpreted in light of those assumptions.

Nothing new. Thomas Bayes, Simon LaPlace, Ronald Fischer (first to use the term Bayesian statistics).

Comparing two hypotheses

Recall that your adviser suggested that you will get the best response rate using a listserv. So you asked 16 people on the listserv and get 6 to respond positively (they would be willing to take your survey).

However, you feel that using social media might be better. So you direct message 16 randomly selected people from your twitter followers and get 10 people to respond positively.

Which seems to be the better method? Yes, there is some evidence that using social media is better. But how certain or uncertain should you be that this is the case? 

You want to specify and fit a Bayesian model to answer these questions. The cool thing is that all you need to do is copy and paste the single-rate model.

Define and draw from the prior distributions.

```{r}
priorRate1 <- runif(n = M, min = 0, max = 1)
priorRate2 <- runif(n = M, min = 0, max = 1)
```

Generate data.
```{r}
nYes1 = NULL
nYes2 = NULL
for(i in 1:M) {
  nYes1[i] <- genModel(priorRate1[i])
  nYes2[i] <- genModel(priorRate2[i])
}
```

Keep only those parameter values that result in the data that you actually observed (6 "yes"'s with rate 1 and 10 "yes"'s with rate 2).
```{r}
postRate1 <- priorRate1[nYes1 == 6]
postRate2 <- priorRate2[nYes2 == 10]
difference <- postRate2 - postRate1

ggplot(data = as.data.frame(difference), 
       mapping = aes(x = difference)) +
  geom_histogram(binwidth = .05, boundary = 0, color = "white") +
  geom_vline(xintercept = 0, color = "red") +
  xlab("Difference in response rates") +
  ggtitle(lab = "Posterior distribution of difference in response rates") +
  theme_minimal()
```

Positive differences indicate you are correct and that using social media will give you a better response rate.

What is the probability you are correct? That is answered from the posterior values as the relative number of positive differences.

```{r}
sum(difference > 0)/length(difference) * 100
```

Example: Reaction times while driving

Cell phone use while driving is believed to increase the chance of an accident due to slowed reaction time. Recall that the data set `reaction.time` {UsingR} gives the time it takes to react to an external event while driving by various groups.

```{r}
df <- UsingR::reaction.time
str(df)
```

The numeric variable 'time' is the reaction time in seconds. This is the response variable. The factor variable 'control' has two groups 'C' control (not using cell phone) and 'T' (using cell). The groups are not paired. The data set is in the long format.

Suppose you're asked to see if the reaction time is shorter for those not using a cell phone. Start with a box plot of reaction times for the two groups (`C`: control and `T`: cell phone users).
```{r}
ggplot(data = df, 
       mapping = aes(x = control, y = time)) +
  geom_boxplot()
```

In the classical setting you use a $t$-test to examine whether the reaction time is slower for those not using a phone. In this setting the null hypothesis is that there is no difference in reaction times.

You use the model syntax: response ~ explanatory as the first argument in the `t.test()` function. You include an argument that names the data frame where the columns `time` and `control` are located. Here the alternative hypothesis is the reaction time is shorter in the control group so you include the argument `alternative = 'less'`.
```{r}
t.test(time ~ control, 
       data = df, 
       alternative = 'less')
```

The evidence in support of the null hypothesis that the reaction times are the same in the population is summarized by the $p$-value. The $p$-value is .0176, so you conclude there is moderate evidence indicating cell phone use slows reaction times.

In a Bayesian setting, you can answer the question directly: what is the probability, given the data and your prior, that mean reaction time is longer when using a phone.

The package {bayesAB} provides functions to analyze these kinds of comparison problems in a Bayesian framework. It is intended as a replacement for common hypothesis tests like the t-test and the chi-squared test 

Recall you can use the `t.test()` function in the format that accepts the two samples are in separate vectors. Here you use `control` and `cellUse`.

```{r}
control <- df$time[df$control == "C"]
cellUse <- df$time[df$control == "T"]
t.test(control, cellUse, alternative = 'less')
```

The `bayesAB::bayesTest()` function only works with separate vectors (it does not work with the formula specification).

You need to specify values for the priors (`mu`, `lambda`, `alpha`, and `beta`) which come from a normal-inverse-gamma distribution.

```{r}
test <- bayesAB::bayesTest(cellUse, control, 
                           priors = c('mu' = 0, 
                                      'lambda' = 1, 
                                      'alpha' = 1, 
                                      'beta' = 1),
                         distribution = "normal")
```

You examine the output with the `str()` function.

```{r}
str(test)
```
The output includes the `inputs` (data, priors, number of samples, etc), the `prior` (normal inverse gamma), and the `posteriors` (`Mu` and `Sig_Sq`).

You summarize the posterior samples with the `summary()` method.

```{r}
test.summary <- test |>
  summary()
```

The answer to the question what is the probability, given the data and your prior, that mean reaction time is longer when using a phone is given in `test.summary$probability`.

```{r}
test.summary$probability
```
You can use the `plot` method applied to the output generates a series of graphs including the prior and posterior distributions.

```{r, eval=FALSE}
plot(test)
```

Repeat using different priors.
```{r}
test2 <- bayesAB::bayesTest(cellUse, control, 
                            priors = c('mu' = 0, 
                                       'lambda' = 1, 
                                        'alpha' = 2, 
                                        'beta' = 3),
                            distribution = "normal") 
summary(test2)$probability

```

Example: Repeat the survey example

Here you repeat the survey example using functions from the {bayesAB} package. This simplifies things considerably as you don't need to write your own functions.

```{r}
yourAdvisor <- c(1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
you <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0)
test <- bayesAB::bayesTest(you, yourAdvisor, 
                           priors = c('alpha' = 1, 
                                      'beta' = 1), 
                           distribution = "bernoulli")
summary(test)
```

More information:

[Bayes Theorem explained](https://www.youtube.com/watch?v=HZGCoVF3YvM).

Now that you've seen how to use functions from R to do a Bayesian analysis, this video explains the logic behind why these functions work.

[Online Bayesian book using R](https://www.bayesrulesbook.com/)

Many more examples are given in this excellent online source.
